{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API pcsk_4bLR9o_3crxHE9zjHW76VdRnBPi2Xo794pQnKSifnRfQ9iQc6U3iqeqeyVEZ3RjBPYtoD4\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "from pinecone import (\n",
    "    Pinecone,\n",
    "    ServerlessSpec,\n",
    "    CloudProvider,\n",
    "    AwsRegion,\n",
    "    VectorType\n",
    ")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import tkinter as tk\n",
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "import concurrent.futures\n",
    "# Important: Import pinecone-client properly\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"PINECONE_API\", PINECONE_API)\n",
    "\n",
    "\n",
    "# Groq API settings\n",
    "GROQ_EMBED_URL = \"https://api.groq.com/openai/v1/embeddings\"\n",
    "GROQ_CHAT_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "LLM_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "\n",
    "# documents = pdf_load_documents()\n",
    "# documents\n",
    "\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "#     \"\"\"Extract text from a PDF file.\"\"\"\n",
    "#     with open(pdf_path, 'r') as file:\n",
    "#         pdf_reader = PyPDF2.PdfReader(file)\n",
    "#         text = \"\"\n",
    "#         for page_num in range(len(pdf_reader.pages)):\n",
    "#             page = pdf_reader.pages[page_num]\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "#     return text\n",
    "# extract_text_from_pdf(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitting \\ Chunking using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False  # considers separators like '\\n\\n'if true\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "\n",
    "# chunks = split_documents(documents)\n",
    "# chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "print(PINECONE_API)\n",
    "\n",
    "\n",
    "#  --------------- initialize pinecone -----------------------------\n",
    "# pc.create_index_for_model(\n",
    "#     name=\"test-index\",\n",
    "#     cloud=\"aws\",\n",
    "#     region=\"us-east-1\",\n",
    "#     embed={\n",
    "#         \"model\":\"llama-text-embed-v2\",\n",
    "#         \"field_map\":{\"text\": \"page_content\"}\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What:\n",
    "**Use Upsert:**\n",
    "\n",
    "When you're adding new vectors or want to replace existing vectors with new data (including changing the vector values).\n",
    "When you need to add a completely new document or vector.\n",
    "When you want to update both the vector values and metadata.\n",
    "\n",
    "**Use Update:**\n",
    "\n",
    "When you're only modifying the metadata of an existing vector.\n",
    "When the vector values (embeddings) themselves are correct and only extra information like text, author, or document-related metadata needs to be updated.\n",
    "Summary:\n",
    "Upsert: Adds or replaces both the vector values and metadata. Use when inserting or completely replacing data.\n",
    "Update: Modifies the metadata without changing the vector values. Use when the vectors are correct, but metadata needs an update.\n",
    "For your case, if you just want to add or update the page_content or any other metadata for existing vectors, use update. If you want to re-upload vectors with new embeddings or metadata, use upsert.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings Via AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en'  and Upsert each to Pinecone one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(\"ai-coach\")\n",
    "\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained(\n",
    "    'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "# user_query = \"user query\"\n",
    "# Function to generate embeddings without tokenization\n",
    "\n",
    "\n",
    "def get_embedding(data):\n",
    "    embeddings = embedding_model.encode(data).tolist()\n",
    "    return embeddings\n",
    "\n",
    "# def upsert_chunks_to_pinecone(index, chunks):\n",
    "#   count = 1\n",
    "#   for chunk in chunks:\n",
    "#     #embedding = embedding_model.encode(chunk.page_content).tolist()\n",
    "#     embedding = get_embedding(chunk.page_content)\n",
    "#     # Extract metadata\n",
    "#     metadata = chunk.metadata\n",
    "#     text = chunk.page_content\n",
    "#     # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "#     vector_id = f\"vec_{count}\"\n",
    "\n",
    "#     # Upsert the embedding along with its metadata\n",
    "#     index.upsert(vectors=[(vector_id, embedding, metadata, text)])\n",
    "\n",
    "#     print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "#     count += 1\n",
    "#       # Ensure data is written immediately\n",
    "#   print(f\"All {count} Embeddings have been upserted to pinecone\")\n",
    "\n",
    "\n",
    "def upsert_chunks_to_pinecone(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get the embedding for the chunk\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "\n",
    "        # Extract metadata and add text as part of the metadata\n",
    "        metadata = chunk.metadata\n",
    "        metadata[\"text\"] = chunk.page_content  # Store text in metadata\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Upsert the embedding along with its metadata\n",
    "        index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "\n",
    "        print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"All {count-1} Embeddings have been upserted to Pinecone\")\n",
    "\n",
    "\n",
    "#upsert_chunks_to_pinecone(index, chunks)\n",
    "\n",
    "# query_embeddings = embedding_model.encode(user_query).tolist()\n",
    "# query_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Vectors Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pinecone_chunks(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get updated embedding\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "\n",
    "        # Extract metadata and page content\n",
    "        metadata = chunk.metadata\n",
    "        text = chunk.page_content\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Update the embedding and metadata\n",
    "        index.update(id=vector_id, values=embedding, set_metadata=metadata)\n",
    "\n",
    "        print(f\"Embedding {count} updated in Pinecone with new metadata\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"All {count-1} embeddings have been updated in Pinecone\")\n",
    "\n",
    "# update_pinecone_chunks(index, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your application is designed to answer a wide range of student queries and suggest relevant material, you want to retrieve enough content to cover different facets of a topic without overwhelming the LLM with too much information.\n",
    "\n",
    "# Starting Point:\n",
    "- A common starting point is to set top_k between **5 and 10.**\n",
    "- **top_k=5:** This can work well if your curated content is highly relevant and precise, ensuring that the top 5 matches are very close to the query.\n",
    "-  **top_k=10:** If you want the coach to consider a broader range of content—perhaps to provide diverse perspectives or cover a topic more comprehensively—increasing top_k to around 10 might be beneficial.\n",
    "\n",
    "# Experiment and Adjust:\n",
    "- The “best” value depends on factors such as the diversity of your content, how densely your data covers the topics, and the quality of the embedding matches. It’s a good idea to experiment with different top_k values and evaluate the quality and relevance of the responses in your specific\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=5, include_metadata=True)\n",
    "    return result['matches']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Groq LLM\n",
    "def query_groq(prompt: str) -> str:\n",
    "    response = requests.post(\n",
    "        GROQ_CHAT_URL,\n",
    "        headers=GROQ_HEADERS,\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 8192  # max from groq website\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error querying Groq: {response.text}\")\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# conversation_history = [\"# 🧑‍🏫 AI Coaching Assistant\\nWelcome! I'm here to help you learn. Type your question below.\"]\n",
    "\n",
    "# def process_user_query(user_query: str, conversation_history: list):\n",
    "\n",
    "#     print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "\n",
    "#     # Step 1: Generate embedding for the user query\n",
    "#     embedding = get_embedding(user_query)\n",
    "\n",
    "#     # Step 2: Query Pinecone for relevant chunks\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "\n",
    "#     # Prepare the context from relevant chunks\n",
    "#     context = \"\\n\".join([chunk['metadata'][\"text\"]\n",
    "#                         for chunk in relevant_chunks])\n",
    "#     print(\"CONTEXT: \", context)\n",
    "\n",
    "#     # Step 3: Combine conversation history with current user query\n",
    "#     conversation_history_str = \"\\n\".join(conversation_history)\n",
    "\n",
    "#     # Step 4: Craft a good coach prompt for the LLM\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner. \n",
    "#     Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level \n",
    "#     and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "#     Context from the student's material:\n",
    "#     {context}\n",
    "\n",
    "#     Conversation history:\n",
    "#     {conversation_history_str}\n",
    "\n",
    "#     The student has asked the following question:\n",
    "#     \"{user_query}\"\n",
    "\n",
    "#     Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and \n",
    "#     offer further guidance if needed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Step 5: Send the prepared prompt (with context and user query) to the LLM\n",
    "#     groq_response = query_groq(prompt)\n",
    "#     print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "\n",
    "#     # Step 6: Append the user query and model's response to conversation history\n",
    "#     conversation_history.append(f\"User: {user_query}\")\n",
    "#     conversation_history.append(f\"Coach: {groq_response}\")\n",
    "\n",
    "#     return groq_response\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "#         print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "#         user_query = input(\"Enter your query or press 0 to exit: \")\n",
    "#         if user_query == \"0\":\n",
    "#             break\n",
    "#         response = process_user_query(user_query, conversation_history)\n",
    "#         print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio GUI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_28824\\974765227.py:84: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n",
      "d:\\Disrupt Labs\\Rag Experiments\\env\\Lib\\site-packages\\gradio\\layouts\\column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* Running on public URL: https://b685d7e523f7472d75.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b685d7e523f7472d75.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query Tokens: 6\n",
      "CONTEXT: frequently feel uncomfortable. However, these abilities usually improve with practice and\n",
      "exposure to difficult circumstances. During this learning phase, instructors and mentors\n",
      "are essential in helping students navigate and achieve effective communication in a\n",
      "professional healthcare setting.\n",
      "Communication can fail for various reasons, with some of the most common causes\n",
      "outlined below:\n",
      "Perception of the Situation: Our perceptions of the environment may differ from those\n",
      "of others. Assumptions about what we see, hear, and understand are often based on\n",
      "our perceptions, which may not align with reality.\n",
      "Bias: Personal biases stem from preexisting opinions about individuals based on their\n",
      "affiliations, culture, economic status, or medical conditions. Bias hinders effective\n",
      "0 6\n",
      "0 2  C O M M U N I C A T I O N  A N D  T E A M W O R K\n",
      "4.  Observe Nonverbal Cues: \n",
      "Pay attention to facial expressions, body language, and other nonverbal signals that can\n",
      "provide additional context to the message.\n",
      "5.  Seek Clarification: \n",
      "Ask for further explanation to ensure complete understanding when uncertain.\n",
      "6. Paraphrase the Content: \n",
      "Restate the sender’s message in your own words to confirm mutual understanding.\n",
      "7. Maintain Focus on the Topic:\n",
      "If the speaker strays from the subject, gently guide the conversation by asking relevant\n",
      "questions or referring to the original issue.\n",
      "8. Address Attention Lapses: \n",
      "If your mind wanders, ask the speaker to repeat the last point to regain focus.\n",
      "9. Avoid Assumptions:\n",
      "3 5\n",
      "Case 1:\n",
      "You are one of six surgical technologists on an orthopedic team at a large hospital. The\n",
      "team leader is aggressive and rude towards you, and you believe his technical skills\n",
      "are inadequate. You suspect he was appointed team leader due to his relationships\n",
      "with sales representatives. How will you manage your working relationship with this\n",
      "individual? What steps can you take to reduce your stress?\n",
      "Case 2:\n",
      "As a new employee in a large hospital's operating room, you have been assigned a\n",
      "preceptor with whom you struggle to work. You feel you are not learning much\n",
      "because she only allows you to observe. When you express your desire to be more\n",
      "hands-on, she responds, \"You're not ready to do anything; just watch.\" After several\n",
      "Does your body language support what you're trying to say, or does it get in the way of\n",
      "communicating effectively? It's essential to pay attention to gestures since cultural\n",
      "interpretations of them might differ.\n",
      "Nonverbal Communication\n",
      "Older Patients5.8\n",
      "1 5\n",
      "0 5  S U P P O R T I N G  T H E  P S Y C H O S O C I A L  N E E D S  O F  T H E  P A T I E N T\n",
      "Elderly patients are often subjected to stereotyping, such as being spoken to as if they were\n",
      "children. This includes using short sentences and high-pitched voices or addressing them\n",
      "with diminutives like \"sweetie.\" Such communication can lead to poor medical outcomes.\n",
      "Instead, speak to older patients as you would any adult, providing necessary clarifications.\n",
      "Avoid using clichés and simple reassurances.\n",
      "Do not refer to patients by diminutives; use their\n",
      "proper names.\n",
      "Do not assume cognitive impairment; the\n",
      "normal aging process does not include\n",
      "dementia.\n",
      "Support patients by orienting them to their\n",
      "environment and providing necessary\n",
      "information.\n",
      "Response Tokens: 136\n",
      "User Query Tokens: 13\n",
      "CONTEXT: 0 5\n",
      "0 4  H E A L T H  C A R E  F A C I L I T Y  S T R U C T U R E  A N D  E N V I R O N M E N T\n",
      "4.1.8  Compressed Gasses\n",
      "4.1.9  Suction Systems\n",
      "4.1.10  Electricity\n",
      "4.1.11  Work Areas in Surgical Departments\n",
      "Types of Gases: Oxygen, compressed air, nitrous oxide, and nitrogen.\n",
      "Delivery Systems: Inline systems in hospitals and outpatient centers, with hoses and\n",
      "safety valves.\n",
      "Alternative Supply: Compressed gas cylinders for facilities without inline systems.\n",
      "Safety: Strictly monitored by The Joint Commission, with detailed guidelines on usage\n",
      "and storage\n",
      "Purpose: Evacuate fluids from surgical sites and patient airways.\n",
      "Delivery Systems: Inline systems terminating at outlets or hoses with safety valves.\n",
      "Adjustment: Vacuum strength is adjustable and measured in pounds per square inch.\n",
      "2 1\n",
      "0 2  C O M M U N I C A T I O N  A N D  T E A M W O R K\n",
      "Verbal Blocking \n",
      "Healthcare professionals occasionally obstruct communication between coworkers and\n",
      "patients. The most typical causes are as follows:\n",
      "Fear of feelings growing stronger.\n",
      "Uncertainty about how to handle a difficult subject.\n",
      "Insecurity about managing delicate conversations.\n",
      "Inadequate seclusion to talk about the matter.\n",
      "Inappropriate setting (too crowded, too noisy, too distracting)\n",
      "Lack of confidence in coworkers' abilities to keep information private.\n",
      "Click here https://www.youtube.com/watch?v=HxDqYEl20hI to watch a video to\n",
      "understand how non-verbal communication can convey mixed messages when it does\n",
      "not align with your words\n",
      "Click here!\n",
      "https://m.youtube.com/watch?v=L0PKWTta7lU\n",
      "0 3\n",
      "0 5  S U P P O R T I N G  T H E  P S Y C H O S O C I A L  N E E D S  O F  T H E  P A T I E N T\n",
      "5.2.1 Maslow’s Hierarchy of Needs\n",
      "Figure 5.2\n",
      "messages. Emotions such as anger, resentment, fear, and anxiety can make\n",
      "communication extremely difficult.\n",
      "Lack of Desire to Communicate: Effective communication requires a genuine desire to\n",
      "exchange information. The motivation to communicate enhances attention, focus,\n",
      "and concentration, which are crucial for clarity and understanding.\n",
      "Fear of Escalating Emotions: Concerns about emotional reactions can inhibit\n",
      "communication.\n",
      "Fear of Handling Difficult Topics: Uncertainty about managing sensitive issues can\n",
      "prevent open discussion.\n",
      "Lack of Confidence: Inadequate confidence in dealing with sensitive topics can be a\n",
      "barrier.\n",
      "Lack of Privacy: Inadequate privacy can hinder the ability to discuss topics freely.\n",
      "Unsuitable Environment: Noisy, distracting, or crowded environments can obstruct\n",
      "makes a mistake, the staff ST could be reluctant to give up control for fear that the\n",
      "surgeon will grow anxious or upset. Tension may result from the staff ST and student\n",
      "having different aims. The student and staff ST can accept the dispute and work together\n",
      "to create a solution that works for everyone by talking about their individual goals.\n",
      "A common issue in collaborative settings is role confusion, which occurs when individuals\n",
      "are unaware of their responsibilities at work. This is especially common in settings where\n",
      "work is difficult. Typical examples of role confusion are:\n",
      "\"Is this part of my responsibilities?\"1.\n",
      " \"I assumed you were handling that.\"2.\n",
      " \"What steps should I take in this situation?\"3.\n",
      "\"I wasn't informed this was within my duties.\"4.\n",
      "Response Tokens: 256\n",
      "User Query Tokens: 6\n",
      "CONTEXT: 3 8\n",
      "T E R M I N O L O G Y\n",
      "Therapeutic Touch: The intentional touching of another person to convey empathy,\n",
      "care, and tenderness.\n",
      "Win-Lose Solution: In conflict resolution, a solution where one party is satisfied while\n",
      "the other party is dissatisfied.\n",
      "02 Communication and Teamwork\n",
      "1 9\n",
      "0 5  S U P P O R T I N G  T H E  P S Y C H O S O C I A L  N E E D S  O F  T H E  P A T I E N T\n",
      "1. Define patient-centered care.\n",
      "Patient-centered care is an approach that considers patients as partners in their own\n",
      "care, respecting their values, preferences, and needs. It involves providing\n",
      "compassionate and coordinated care that addresses the whole person, not just their\n",
      "medical condition.\n",
      "2. Discuss the domains of Maslow’s hierarchy.\n",
      "  Maslow's hierarchy of needs includes:\n",
      "Physiological needs: Basic necessities like air, water, food, and shelter.\n",
      "Safety needs: Security, stability, and protection from harm.\n",
      "Belongingness and love needs: Social relationships, affection, and acceptance.\n",
      "Esteem needs: Self-respect, recognition, and achievement.\n",
      "3 7\n",
      "T E R M I N O L O G Y\n",
      "Assertiveness: The ability to communicate one's personal and professional needs\n",
      "while respecting the rights of others.\n",
      "Body Language: The use of facial expressions, posture, and gestures to convey\n",
      "messages.\n",
      "Consensus: Agreement reached among members of a group.\n",
      "Facilitator: A group leader who guides the direction and flow of a meeting without\n",
      "influencing the content, encouraging members to express ideas without fear of\n",
      "judgment.\n",
      "Feedback: The response to a communicated message, an essential element of\n",
      "effective communication.\n",
      "Groupthink: In sociology and group behavior theory, the tendency of a group to\n",
      "conform to a single way of thinking and behaving, creating factions of agreement\n",
      "(in-group) and disagreement (out-group), leading to workplace conflict.\n",
      "relationships between healthcare workers, patients, and their families:\n",
      "Formality: To preserve the appropriate emotional distance, communication between\n",
      "medical experts and patients or their carers must adhere to a particular degree of\n",
      "formality.\n",
      "Separation Anxiety: It's an important matter when patients are taken away from their\n",
      "families. Anxiety and worry are natural emotions in families. The healthcare provider\n",
      "frequently acts as the sole \"emotionally neutral\" person, offering the necessary\n",
      "comfort.\n",
      "Perception of Privileged Knowledge: Families might believe medical personnel are\n",
      "privy to knowledge that isn't provided. Interactions should acknowledge this view,\n",
      "which is not a sign of mistrust.\n",
      "1 8\n",
      "0 3  M E D I C O L E G A L  A S P E C T S  O F  S U R G I C A L  T E C H N O L O G Y\n",
      "There have been documented occasions where abusive team members have dropped or\n",
      "thrown instruments or equipment, causing physical harm to medical workers. Bullying\n",
      "frequently targets those who are seen as risks because of their background or other\n",
      "characteristics, as well as members of minority racial or cultural groups. Hazing is a\n",
      "particular kind of bullying that targets recent graduates with activities meant to cause\n",
      "them shame and embarrassment, sometimes even resulting in physical harm.\n",
      "Abuse can occur vertically, between ranks, or horizontally, between staff members of\n",
      "equal rank. While vertical abuse includes a person in authority assaulting a subordinate\n",
      "Response Tokens: 209\n"
     ]
    }
   ],
   "source": [
    "# system_message = f\"\"\"\n",
    "#     You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner. \n",
    "#     Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level \n",
    "#     and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "#     Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and \n",
    "#     offer further guidance if needed.\n",
    "#     \"\"\"\n",
    "\n",
    "# def gradio_interface(prompt,history =[]):\n",
    "#     output = process_user_query(prompt,history)\n",
    "#     history.append((prompt,output))\n",
    "#     return history\n",
    "\n",
    "# gr.Interface(fn=gradio_interface, inputs= ['text',\"state\"], outputs=[\"chatbot\",\"state\"]).launch(debug=True,share=True)\n",
    "    \n",
    "\n",
    "# ------------------------------------------- WORKING 1 -------------------------------------------\n",
    "\n",
    "# # Function to be used by Gradio for handling the query\n",
    "# def gradio_process(user_query):\n",
    "#     response = process_user_query(user_query, conversation_history)\n",
    "#     return response\n",
    "\n",
    "# # Create Gradio interface\n",
    "# interface = gr.Interface(fn=gradio_process, inputs=\"text\", outputs=\"text\", title=\"RAG-based Coaching System\")\n",
    "\n",
    "# # Launch Gradio app\n",
    "# interface.launch()\n",
    "# ------------------------------------------- WORKING 2 -------------------------------------------\n",
    "\n",
    "# Initialize empty conversation history (list of tuples)\n",
    "conversation_history = []\n",
    "\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk['metadata'][\"text\"] for chunk in relevant_chunks)\n",
    "    print(\"CONTEXT:\", context)\n",
    "\n",
    "    # Format conversation history for the prompt\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\" \n",
    "        for user, response in conversation_history\n",
    "    )\n",
    "\n",
    "    # Create structured prompt\n",
    "    prompt = f\"\"\"You are a knowledgeable and friendly coach. Follow these guidelines:\n",
    "    1. Provide clear, step-by-step explanations\n",
    "    2. Ask guiding questions to encourage critical thinking\n",
    "    3. Adapt to the student's knowledge level\n",
    "    4. Use examples from the provided context when relevant\n",
    "\n",
    "    Context from learning materials:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "\n",
    "    New student question:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    Provide a helpful response:\"\"\"\n",
    "\n",
    "    # Get LLM response\n",
    "    groq_response = query_groq(prompt)\n",
    "    print(f\"Response Tokens: {count_tokens(groq_response)}\")\n",
    "\n",
    "    # Return updated history with new interaction\n",
    "    return conversation_history + [(user_query, groq_response)]\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"# 🧑‍🏫 AI Coaching Assistant\")\n",
    "    gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "    \n",
    "    # State management\n",
    "    chat_history = gr.State(conversation_history)\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500)\n",
    "        with gr.Column(scale=0.5):\n",
    "            context_display = gr.Textbox(label=\"Relevant Context\", interactive=False)\n",
    "\n",
    "    user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        undo_btn = gr.Button(\"Undo Last\")\n",
    "        clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "    def handle_submit(user_input, history):\n",
    "        if not user_input.strip():\n",
    "            return gr.update(), history, \"\"\n",
    "        \n",
    "        # Process query and update history\n",
    "        new_history = process_user_query(user_input, history)\n",
    "        \n",
    "        # Get latest context for display\n",
    "        latest_context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in query_pinecone(\n",
    "            get_embedding(user_input)\n",
    "        )][:3])  # Show top 3 context snippets\n",
    "        \n",
    "        return \"\", new_history, latest_context\n",
    "\n",
    "    # Component interactions\n",
    "    submit_btn.click(\n",
    "        handle_submit,\n",
    "        [user_input, chat_history],\n",
    "        [user_input, chat_history, context_display]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    undo_btn.click(\n",
    "        lambda history: history[:-1] if history else [],\n",
    "        [chat_history],\n",
    "        [chat_history]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        lambda: [],\n",
    "        None,\n",
    "        [chat_history]\n",
    "    ).then(\n",
    "        lambda: ([], \"\"),\n",
    "        None,\n",
    "        [chatbot, context_display]\n",
    "    )\n",
    "\n",
    "interface.launch(share=True)\n",
    "# Just change the launch command to:\n",
    "#interface.launch(share=True, auth=(\"username\", \"password\"))  # Add basic auth\n",
    "\n",
    "\n",
    "# self hosting\n",
    "\n",
    "# # Run with:\n",
    "# interface.launch(\n",
    "#     server_name=\"0.0.0.0\",\n",
    "#     server_port=7860,\n",
    "#     show_error=True\n",
    "# )\n",
    "\n",
    "\n",
    "# ------------------------------------------- WORKING 3 (performance) -------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
