{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# from typing import List, Tuple\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "# PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_CHAT_URL = os.getenv(\"GROQ_CHAT_URL\")\n",
    "\n",
    "NVIDIA_API = os.getenv(\"NVIDIA_API\")\n",
    "NVIDIA_BASE_URL = os.getenv(\"NVIDIA_BASE_URL\")\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "# LLM_MODEL = \"llama3-70b-8192\"\n",
    "LLM_MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "\n",
    "# NVidia Embedding import\n",
    "client = OpenAI(\n",
    "    api_key=NVIDIA_API,\n",
    "    base_url=NVIDIA_BASE_URL,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    - Context window: 128K\n",
    "Ouput:\n",
    "    - Output Max Tokens: 32,768\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"[Time Tracker] `{func.__name__}` took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# # EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "\n",
    "# vo = voyageai.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "# index = pc.Index(\"ai-coach\")\n",
    "# index = pc.Index(\"ahsan-400pg-pdf-doc-test\")\n",
    "index = pc.Index(\"surgical-tech-complete\")  # -- COMPLETE SURGICAL TECH BOOTCAMP\n",
    "\n",
    "\n",
    "# embedding_model = AutoModel.from_pretrained(\n",
    "#     'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# # Function to generate embeddings without tokenization\n",
    "# def get_embedding(data):\n",
    "#     embeddings = embedding_model.encode(data).tolist()\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "@track_time\n",
    "def get_embedding(text=\"None\"):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"nvidia/nv-embed-v1\",\n",
    "        encoding_format=\"float\",\n",
    "        extra_body={\"input_type\": \"query\", \"truncate\": \"NONE\"},\n",
    "    )\n",
    "\n",
    "    # print(response.data[0].embedding)\n",
    "    # print(count_tokens(response.data[0].embedding))\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "get_embedding(\"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "@track_time\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=5, include_metadata=True)\n",
    "    return result[\"matches\"]\n",
    "\n",
    "\n",
    "print(query_pinecone(get_embedding(\"Pediatric surgery definition\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Use Case | Recommended top_p | Notes |\n",
    "|----------|------------------|-------|\n",
    "| Factual Q&A | 0.1 - 0.3 | Lower values for more deterministic, factual responses |\n",
    "| Code Generation | 0.2 - 0.5 | Precision matters more than creativity |\n",
    "| Technical Writing | 0.5 - 0.7 | Balanced approach for technical accuracy with clarity |\n",
    "| General Conversation | 0.7 - 0.9 | Good balance for most chatbot applications |\n",
    "| Creative Writing | 0.9 - 1.0 | Higher values for more diverse and creative outputs |\n",
    "\n",
    "\\n\n",
    "| Parameter Combination | Use Case |\n",
    "|----------------------|----------|\n",
    "| top_p=0.5, temperature=0.3 | Highly factual, consistent responses |\n",
    "| top_p=0.7, temperature=0.5 | Educational content with examples |\n",
    "| top_p=0.9, temperature=0.7 | Creative but coherent responses |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified query_groq function with more explicit streaming handling\n",
    "@track_time\n",
    "def query_groq(user_prompt, sys_prompt):\n",
    "    client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "    # Always use streaming mode\n",
    "    return client.chat.completions.create(\n",
    "        model=LLM_MODEL,  # or whichever model you're using\n",
    "        temperature=0.5,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        stream=True,\n",
    "        # top_p=0.7,  # testing for better results\n",
    "    )\n",
    "\n",
    "\n",
    "# Print all tool calls\n",
    "# print(completion.choices[0].message.executed_tools)\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "\"\"\"\n",
    "Putting tokenizer outside of the function to avoid reinitialization and optimize performance.\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "\n",
    "@track_time\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Query from LLM to get relevant context from Pinecone for references and sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groq and Gradio with Streaming Enabled\n",
    "\n",
    "- ### i.e. will start showing text as soon as it gets generated from groq inference\n",
    "- ### faster than optimized version\n",
    "\n",
    "## Query:\n",
    "- pediatic surgery\n",
    "## Response Time:\n",
    "User Query Tokens: 6\n",
    "[Time Tracker] `get_embedding` took 0.4752 seconds\n",
    "[Time Tracker] `query_pinecone` took 0.2222 seconds\n",
    "[Time Tracker] `query_groq` took 0.5060 seconds\n",
    "\n",
    "Total time: 1.19 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------- ## Groq and Gradio with Streaming Enabled -----------------------------------------------------\n",
    "# Modified process_user_query to properly yield streaming updates\n",
    "@track_time\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk[\"metadata\"][\"text\"] for chunk in relevant_chunks)\n",
    "\n",
    "    # Format conversation history for the prompt\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\" for user, response in conversation_history\n",
    "    )\n",
    "        # System prompt - contains instructions for the AI's behavior\n",
    "    system_prompt = f\"\"\"\n",
    "    \n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "    \n",
    "    learning materials:\n",
    "    {context}\n",
    "    \n",
    "    You are an expert, knowledgeable, and friendly coach. Follow these **guidelines** carefully:\n",
    "\n",
    "    1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "    2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "    3. Ask guiding questions to encourage critical thinking.\n",
    "    4. Adapt your explanation to match the student's knowledge level.\n",
    "    5. Strictly use terminologies provided in the given context.\n",
    "    6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "    7. Only answer based on the provided learning materials—do not speculate or include external information.\n",
    "    8. **Always provide all specific relevant sources with name from the context in your responses urls, video names, video timestamps, links, resources, **ebook names**, lesson names, lesson numbers, if the user query is not relevant to the context then don't provide any references and sources.**\n",
    "    9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "    10. Must provide all relevant video timestamp with video name from where to start watching and where to end watching , if you can't provide timestamp then tell the user to watch the whole video.\n",
    "    11. Provide a thoughtful and contextually accurate response\n",
    "    12. Avoid Repetition.\n",
    "    13. **If student asks something completely out of context then politely decline and ask the user to ask a question related to their course they are studying and don't provide any references and sources for this.**\n",
    "    14. Please avoid \"Bank Name\"\n",
    "    15. When users request questions, answers, quizzes or exams, generate high-quality educational assessments directly from the learning material by using for example:  multiple question types (multiple-choice with 4-5 options, true/false, short answer, fill-in-the-blanks, essay) with clear formatting; creating questions at various cognitive levels (recall, comprehension, application, analysis); providing detailed answer keys with explanations; organizing content with consistent formatting (numbered questions, lettered options, bold correct answers); adapting difficulty based on subject matter; and including step-by-step solutions for problem-solving questions.\n",
    "    16. Never generate fabricated information when providing references or sources. Only provide facts, references, citations, lesson names, e-book titles, video names, timestamps explicitly present in the provided learning materials\n",
    "    \"\"\"\n",
    "\n",
    "    # User prompt - contains the specific query and context\n",
    "    user_prompt = f\"\"\"\n",
    "    \n",
    "    \n",
    "    New student question:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Then in your query_groq function:\n",
    "    stream_response = query_groq(user_prompt, system_prompt)\n",
    "\n",
    "    # The function now directly yields the stream chunks for the Gradio interface to use\n",
    "    full_response = \"\"\n",
    "\n",
    "    # First, yield a response with empty text to set up the message\n",
    "    # This creates the user message immediately\n",
    "    temp_history = conversation_history.copy()\n",
    "    temp_history.append((user_query, \"\"))\n",
    "    yield temp_history, context\n",
    "\n",
    "    # Process the stream\n",
    "    for chunk in stream_response:\n",
    "        if (\n",
    "            hasattr(chunk.choices[0].delta, \"content\")\n",
    "            and chunk.choices[0].delta.content is not None\n",
    "        ):\n",
    "            content_chunk = chunk.choices[0].delta.content\n",
    "            full_response += content_chunk\n",
    "\n",
    "            # Create a temporary history with the current response\n",
    "            temp_history = conversation_history.copy()\n",
    "            temp_history.append((user_query, full_response))\n",
    "\n",
    "            # Yield the updated history for display\n",
    "            yield temp_history, context\n",
    "\n",
    "    # Return the final history with the complete response\n",
    "    final_history = conversation_history.copy()\n",
    "    final_history.append((user_query, full_response))\n",
    "    yield final_history, context\n",
    "\n",
    "\n",
    "@track_time\n",
    "def create_gradio_interface(conversation_history):\n",
    "    with gr.Blocks() as interface:\n",
    "        gr.Markdown(\"# 🧑‍🏫 AI Coaching Assistant\")\n",
    "        gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "\n",
    "        # State management\n",
    "        chat_history = gr.State(conversation_history)\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "            with gr.Column(scale=0.5):\n",
    "                context_display = gr.Textbox(\n",
    "                    label=\"Relevant Context\", interactive=False\n",
    "                )\n",
    "\n",
    "        user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            undo_btn = gr.Button(\"Undo Last\")\n",
    "            clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "        def handle_submit(user_query, history):\n",
    "            if not user_query.strip():\n",
    "                return gr.update(), history, \"\"\n",
    "\n",
    "            # Use the generator directly from process_user_query\n",
    "            # This will yield incremental updates as they arrive\n",
    "            response_generator = process_user_query(user_query, history)\n",
    "\n",
    "            for updated_history, context in response_generator:\n",
    "                # Directly update the chatbot with each streaming chunk\n",
    "                yield \"\", updated_history, context, updated_history\n",
    "\n",
    "        # Component interactions with streaming support\n",
    "        submit_btn.click(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        # Add submit on Enter key press\n",
    "        user_input.submit(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        undo_btn.click(\n",
    "            lambda history: history[:-1] if history else [],\n",
    "            [chat_history],\n",
    "            [chat_history],\n",
    "        ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "        clear_btn.click(lambda: [], None, [chat_history]).then(\n",
    "            lambda: ([], \"\"), None, [chatbot, context_display]\n",
    "        )\n",
    "\n",
    "    return interface\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for the application.\n",
    "\n",
    "    Initializes the conversation history with a welcome message,\n",
    "    creates the Gradio interface, and launches the web app.\n",
    "    \"\"\"\n",
    "    # Initialize conversation history with welcome message\n",
    "    welcome_message = \"Hi there! I'm your AI coach. I can help answer questions about your course materials, explain difficult concepts, and guide your learning journey. What would you like to know today?\"\n",
    "    initial_conversation_history = [(\"\", welcome_message)]\n",
    "\n",
    "    # Create and launch the interface\n",
    "    interface = create_gradio_interface(initial_conversation_history)\n",
    "    interface.launch(share=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
