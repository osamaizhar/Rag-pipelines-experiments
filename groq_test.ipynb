{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API pcsk_4bLR9o_3crxHE9zjHW76VdRnBPi2Xo794pQnKSifnRfQ9iQc6U3iqeqeyVEZ3RjBPYtoD4\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "from pinecone import (\n",
    "    Pinecone,\n",
    "    ServerlessSpec,\n",
    "    CloudProvider,\n",
    "    AwsRegion,\n",
    "    VectorType\n",
    ")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import tkinter as tk\n",
    "\n",
    "\n",
    "# Important: Import pinecone-client properly\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"PINECONE_API\", PINECONE_API)\n",
    "\n",
    "\n",
    "# Groq API settings\n",
    "GROQ_EMBED_URL = \"https://api.groq.com/openai/v1/embeddings\"\n",
    "GROQ_CHAT_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "LLM_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "\n",
    "# documents = pdf_load_documents()\n",
    "# documents[0]\n",
    "\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "#     \"\"\"Extract text from a PDF file.\"\"\"\n",
    "#     with open(pdf_path, 'r') as file:\n",
    "#         pdf_reader = PyPDF2.PdfReader(file)\n",
    "#         text = \"\"\n",
    "#         for page_num in range(len(pdf_reader.pages)):\n",
    "#             page = pdf_reader.pages[page_num]\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "#     return text\n",
    "# extract_text_from_pdf(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitting \\ Chunking using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False  # considers separators like '\\n\\n'if true\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "\n",
    "# chunks = split_documents(documents)\n",
    "# chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcsk_4bLR9o_3crxHE9zjHW76VdRnBPi2Xo794pQnKSifnRfQ9iQc6U3iqeqeyVEZ3RjBPYtoD4\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "print(PINECONE_API)\n",
    "\n",
    "\n",
    "#  --------------- initialize pinecone -----------------------------\n",
    "# pc.create_index_for_model(\n",
    "#     name=\"test-index\",\n",
    "#     cloud=\"aws\",\n",
    "#     region=\"us-east-1\",\n",
    "#     embed={\n",
    "#         \"model\":\"llama-text-embed-v2\",\n",
    "#         \"field_map\":{\"text\": \"page_content\"}\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What:\n",
    "**Use Upsert:**\n",
    "\n",
    "When you're adding new vectors or want to replace existing vectors with new data (including changing the vector values).\n",
    "When you need to add a completely new document or vector.\n",
    "When you want to update both the vector values and metadata.\n",
    "\n",
    "**Use Update:**\n",
    "\n",
    "When you're only modifying the metadata of an existing vector.\n",
    "When the vector values (embeddings) themselves are correct and only extra information like text, author, or document-related metadata needs to be updated.\n",
    "Summary:\n",
    "Upsert: Adds or replaces both the vector values and metadata. Use when inserting or completely replacing data.\n",
    "Update: Modifies the metadata without changing the vector values. Use when the vectors are correct, but metadata needs an update.\n",
    "For your case, if you just want to add or update the page_content or any other metadata for existing vectors, use update. If you want to re-upload vectors with new embeddings or metadata, use upsert.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings Via AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en'  and Upsert each to Pinecone one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(\"test-index\")\n",
    "\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "#user_query = \"user query\"\n",
    "# Function to generate embeddings without tokenization\n",
    "def get_embedding(data):\n",
    "    embeddings = embedding_model.encode(data).tolist()\n",
    "    return embeddings\n",
    "\n",
    "# def upsert_chunks_to_pinecone(index, chunks):\n",
    "#   count = 1\n",
    "#   for chunk in chunks:\n",
    "#     #embedding = embedding_model.encode(chunk.page_content).tolist()\n",
    "#     embedding = get_embedding(chunk.page_content)\n",
    "#     # Extract metadata\n",
    "#     metadata = chunk.metadata\n",
    "#     text = chunk.page_content\n",
    "#     # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "#     vector_id = f\"vec_{count}\"\n",
    "    \n",
    "#     # Upsert the embedding along with its metadata\n",
    "#     index.upsert(vectors=[(vector_id, embedding, metadata, text)])\n",
    "    \n",
    "#     print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "#     count += 1\n",
    "#       # Ensure data is written immediately\n",
    "#   print(f\"All {count} Embeddings have been upserted to pinecone\")\n",
    "\n",
    "\n",
    "def upsert_chunks_to_pinecone(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get the embedding for the chunk\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "        \n",
    "        # Extract metadata and add text as part of the metadata\n",
    "        metadata = chunk.metadata\n",
    "        metadata[\"text\"] = chunk.page_content  # Store text in metadata\n",
    "        \n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "        \n",
    "        # Upsert the embedding along with its metadata\n",
    "        index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "        \n",
    "        print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "        count += 1\n",
    "    \n",
    "    print(f\"All {count-1} Embeddings have been upserted to Pinecone\")\n",
    "\n",
    "# upsert_chunks_to_pinecone(index, chunks)\n",
    "\n",
    "# query_embeddings = embedding_model.encode(user_query).tolist()\n",
    "# query_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Vectors Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pinecone_chunks(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get updated embedding\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "        \n",
    "        # Extract metadata and page content\n",
    "        metadata = chunk.metadata\n",
    "        text = chunk.page_content\n",
    "        \n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "        \n",
    "        # Update the embedding and metadata\n",
    "        index.update(id=vector_id, values=embedding, set_metadata=metadata)\n",
    "        \n",
    "        print(f\"Embedding {count} updated in Pinecone with new metadata\")\n",
    "        count += 1\n",
    "    \n",
    "    print(f\"All {count-1} embeddings have been updated in Pinecone\")\n",
    "\n",
    "#update_pinecone_chunks(index, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your application is designed to answer a wide range of student queries and suggest relevant material, you want to retrieve enough content to cover different facets of a topic without overwhelming the LLM with too much information.\n",
    "\n",
    "# Starting Point:\n",
    "- A common starting point is to set top_k between **5 and 10.**\n",
    "- **top_k=5:** This can work well if your curated content is highly relevant and precise, ensuring that the top 5 matches are very close to the query.\n",
    "-  **top_k=10:** If you want the coach to consider a broader range of content—perhaps to provide diverse perspectives or cover a topic more comprehensively—increasing top_k to around 10 might be beneficial.\n",
    "\n",
    "# Experiment and Adjust:\n",
    "- The “best” value depends on factors such as the diversity of your content, how densely your data covers the topics, and the quality of the embedding matches. It’s a good idea to experiment with different top_k values and evaluate the quality and relevance of the responses in your specific\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=10, include_metadata=True)\n",
    "    return result['matches']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Groq LLM\n",
    "def query_groq(prompt: str) -> str:\n",
    "    response = requests.post(\n",
    "        GROQ_CHAT_URL,\n",
    "        headers=GROQ_HEADERS,\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 8192 # max from groq website\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error querying Groq: {response.text}\")\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query Tokens : 5\n",
      "Groq Response Tokens : 690\n",
      "Parity checking is a fundamental concept in computer networks, and I'm happy to help you understand it better.\n",
      "\n",
      "Parity checking is a method used to detect errors in transmitted data. The basic idea is to add an extra bit, called the parity bit, to the data bits being transmitted. This parity bit is calculated based on the data bits, and its purpose is to help the receiver detect if any errors occurred during transmission.\n",
      "\n",
      "Let's dive deeper into single-bit parity checking. In this method, a single parity bit is added to the data bits. The parity bit is calculated such that the total number of 1's in the data bits and the parity bit is even (even parity) or odd (odd parity). At the receiver's end, the parity of the received data bits and the parity bit is calculated. If the calculated parity doesn't match the expected parity, an error is detected.\n",
      "\n",
      "For example, let's say we have 8 data bits: 01110001. To calculate the parity bit, we count the number of 1's in the data bits. In this case, there are 3 ones. To make the total number of 1's even, we add a parity bit of 1, making the total number of 1's 4 (an even number). The transmitted data would be 011100011.\n",
      "\n",
      "Now, let's say an error occurs during transmission, and the received data is 011100010. When the receiver calculates the parity, they'll find that the total number of 1's is 3, which is an odd number. This mismatch indicates an error, and the receiver can request retransmission.\n",
      "\n",
      "Two-dimensional parity checking is an extension of single-bit parity checking. In this method, both row and column parities are calculated and added to the data bits. This allows the receiver to detect and correct single-bit errors without retransmission.\n",
      "\n",
      "Another important concept is the Internet checksum, which is used in UDP (User Datagram Protocol) to detect errors in transmitted segments. The sender calculates the checksum by treating the contents of the UDP segment as a sequence of 16-bit integers and adding them up using one's complement arithmetic. The receiver calculates the checksum in the same way and compares it with the received checksum value. If they don't match, an error is detected.\n",
      "\n",
      "Cyclic Redundancy Check (CRC) is a more powerful error-detection technique used in many protocols, including Ethernet and Wi-Fi. In CRC, a generator polynomial (G) is used to calculate the remainder of the division of the data bits by G. The sender appends the remainder to the data bits, and the receiver calculates the remainder in the same way. If the calculated remainder doesn't match the received remainder, an error is detected.\n",
      "\n",
      "Now, I'd like to ask you a question: Can you think of a scenario where parity checking might not detect an error? For example, what if two bits flip during transmission? How would parity checking handle this situation?\n",
      "\n",
      "Feel free to ask me any questions or clarify any doubts you have about parity checking. I'm here to help you understand this concept better!\n",
      "User Query Tokens : 15\n",
      "Groq Response Tokens : 645\n",
      "The connection between Cyclic Redundancy Check (CRC) and parity checking is a great topic to explore.\n",
      "\n",
      "At first glance, CRC and parity checking might seem like two distinct error-detection techniques. However, they share a common goal: to detect errors in transmitted data. Let's dive deeper into how they relate to each other.\n",
      "\n",
      "Parity checking, as we discussed earlier, involves adding an extra bit to the data bits to ensure that the total number of 1's is even (even parity) or odd (odd parity). This simple method can detect single-bit errors, but it has limitations. For instance, if two bits flip during transmission, the parity bit might still be correct, and the error would go undetected.\n",
      "\n",
      "Cyclic Redundancy Check (CRC), on the other hand, is a more powerful error-detection technique. It uses a generator polynomial (G) to calculate the remainder of the division of the data bits by G. The sender appends this remainder to the data bits, and the receiver calculates the remainder in the same way. If the calculated remainder doesn't match the received remainder, an error is detected.\n",
      "\n",
      "Now, here's where the correlation between CRC and parity checking comes in: CRC can be thought of as an extension of parity checking. In fact, CRC can be viewed as a multi-bit parity checking method.\n",
      "\n",
      "Think of it this way: parity checking is like checking the parity of a single bit, whereas CRC is like checking the parity of multiple bits (the remainder) calculated using a generator polynomial. This makes CRC more robust and capable of detecting a wider range of errors, including burst errors (errors that occur in a sequence of bits).\n",
      "\n",
      "To illustrate this connection, let's consider a simple example. Imagine we have a data bit sequence 1010, and we want to calculate a 2-bit CRC using a generator polynomial G = 1101. The calculation would involve dividing the data bits by G and taking the remainder. Let's say the remainder is 11. The sender would append this remainder to the data bits, making the transmitted data 101011.\n",
      "\n",
      "At the receiver's end, the calculation would be repeated, and if the calculated remainder doesn't match the received remainder (11), an error would be detected.\n",
      "\n",
      "Notice that the CRC calculation involves a division operation, which can be thought of as a more complex form of parity checking. The remainder calculated using the generator polynomial is essentially a multi-bit parity check that can detect errors in a more robust way.\n",
      "\n",
      "In summary, CRC is a more advanced error-detection technique that builds upon the principles of parity checking. While parity checking is limited to detecting single-bit errors, CRC can detect a wider range of errors, including burst errors, making it a more reliable and widely used technique in many protocols.\n",
      "\n",
      "I hope this explanation helps you understand the connection between CRC and parity checking! Do you have any further questions or would you like me to elaborate on any specific aspect of this topic?\n",
      "User Query Tokens : 36\n",
      "Groq Response Tokens : 805\n",
      "I totally get it! It can be overwhelming when you're introduced to new concepts, especially when they seem complex and abstract. Parity checking, in particular, can be a bit tricky to grasp at first, but don't worry, I'm here to help you break it down and make sense of it.\n",
      "\n",
      "First, take a deep breath and let's acknowledge that it's normal to feel confused or frustrated when learning new material. It's a natural part of the learning process, and it doesn't mean you're not capable of understanding the concept. It simply means you need a bit more guidance and practice to get comfortable with it.\n",
      "\n",
      "Now, let's take a step back and revisit the basics of parity checking. Parity checking is a simple error-detection technique used in computer networks to detect single-bit errors in transmitted data. The idea is to add an extra bit, called the parity bit, to the data bits being transmitted. This parity bit is calculated based on the data bits, and its purpose is to help the receiver detect if any errors occurred during transmission.\n",
      "\n",
      "Think of it like a simple checksum. Imagine you're sending a message to a friend, and you want to make sure they receive it correctly. You could add a special code at the end of the message that indicates whether the message is correct or not. That's basically what parity checking does, but instead of a code, it uses a single bit to indicate whether the data bits are correct or not.\n",
      "\n",
      "Here's an example to help illustrate this concept:\n",
      "\n",
      "Let's say you want to send the data bits 01110001. To calculate the parity bit, you count the number of 1's in the data bits. In this case, there are 3 ones. To make the total number of 1's even (even parity), you add a parity bit of 1, making the total number of 1's 4 (an even number). The transmitted data would be 011100011.\n",
      "\n",
      "Now, let's say an error occurs during transmission, and the received data is 011100010. When the receiver calculates the parity, they'll find that the total number of 1's is 3, which is an odd number. This mismatch indicates an error, and the receiver can request retransmission.\n",
      "\n",
      "The key thing to remember is that parity checking is a simple technique that can detect single-bit errors, but it has limitations. For instance, if two bits flip during transmission, the parity bit might still be correct, and the error would go undetected.\n",
      "\n",
      "If you're still feeling overwhelmed, I'd like to offer some guidance to help you better understand parity checking:\n",
      "\n",
      "1. **Practice, practice, practice**: Try working through some examples of parity checking on your own. This will help you get a feel for how the technique works and how it can detect errors.\n",
      "2. **Visualize the process**: Imagine you're sending data bits over a network, and you need to add a parity bit to detect errors. Think about how you would calculate the parity bit and how the receiver would use it to detect errors.\n",
      "3. **Compare with other error-detection techniques**: Look at how other error-detection techniques, like CRC or checksum, work. This will help you see how parity checking fits into the bigger picture of error detection in computer networks.\n",
      "\n",
      "Remember, understanding parity checking takes time and practice. Don't be too hard on yourself if you don't get it right away. You're doing great, and with a bit more effort, you'll be a pro at parity checking in no time!\n",
      "\n",
      "How do you feel now? Do you have any specific questions or areas of confusion that I can help with?\n"
     ]
    }
   ],
   "source": [
    "# Main function to handle user query\n",
    "# def process_user_query(user_query: str):\n",
    "#     print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "#     # Step 1: Generate embedding for the user query\n",
    "#     embedding = get_embedding(user_query)\n",
    "\n",
    "#     # Step 2: Query Pinecone for relevant chunks\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     print(f\"Relevant Chunks : {relevant_chunks[0]}\")\n",
    "#     # Step 3: Prepare the content for the Groq LLM\n",
    "#     context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in relevant_chunks])\n",
    "#     print(\"------------------------------------ Context ------------------------------------------ : \", context)\n",
    "#     # Step 4: Send the retrieved content as the prompt to Groq LLM\n",
    "#     groq_response = query_groq(context)\n",
    "#     print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "#     return groq_response\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     user_query = \"What are the Link Layer?\"\n",
    "#     response = process_user_query(user_query)\n",
    "#     print(response)\n",
    "\n",
    "# def process_user_query(user_query: str):\n",
    "#     print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "\n",
    "#     # Step 1: Generate embedding for the user query\n",
    "#     embedding = get_embedding(user_query)\n",
    "\n",
    "#     # Step 2: Query Pinecone for relevant chunks\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     #print(f\"Relevant Chunks : {relevant_chunks}\")\n",
    "\n",
    "#     # Step 3: Prepare the content (context) for the LLM\n",
    "#     #context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in relevant_chunks])\n",
    "#     #print(\"------------------------------------ Context ------------------------------------------ : \", context)\n",
    "\n",
    "#     # Step 4: Craft a good coach prompt for the LLM\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner.\n",
    "#     Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level\n",
    "#     and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "#     Context from the student's material:\n",
    "#     {relevant_chunks}\n",
    "\n",
    "#     The student has asked the following question:\n",
    "#     \"{user_query}\"\n",
    "\n",
    "#     Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and\n",
    "#     offer further guidance if needed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Step 5: Send the prepared prompt (with context and user query) to the LLM\n",
    "#     groq_response = query_groq(prompt)\n",
    "#     print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "\n",
    "#     return groq_response\n",
    "\n",
    "\n",
    "# # # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "#         user_query = input(\"Enter your query or press 0 to exit: \")\n",
    "#         if user_query == \"0\":\n",
    "#             break\n",
    "#         response = process_user_query(user_query)\n",
    "#         print(response)\n",
    "\n",
    "\n",
    "# Initialize an empty list to store conversation history\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "\n",
    "    # Step 1: Generate embedding for the user query\n",
    "    embedding = get_embedding(user_query)\n",
    "\n",
    "    # Step 2: Query Pinecone for relevant chunks\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "\n",
    "    # Prepare the context from relevant chunks\n",
    "    context = \"\\n\".join([chunk['metadata'][\"text\"]\n",
    "                        for chunk in relevant_chunks])\n",
    "\n",
    "    # Step 3: Combine conversation history with current user query\n",
    "    conversation_history_str = \"\\n\".join(conversation_history)\n",
    "\n",
    "    # Step 4: Craft a good coach prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner. \n",
    "    Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level \n",
    "    and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "    Context from the student's material:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {conversation_history_str}\n",
    "\n",
    "    The student has asked the following question:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and \n",
    "    offer further guidance if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 5: Send the prepared prompt (with context and user query) to the LLM\n",
    "    groq_response = query_groq(prompt)\n",
    "    print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "\n",
    "    # Step 6: Append the user query and model's response to conversation history\n",
    "    conversation_history.append(f\"User: {user_query}\")\n",
    "    conversation_history.append(f\"Coach: {groq_response}\")\n",
    "\n",
    "    return groq_response\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_query = input(\"Enter your query or press 0 to exit: \")\n",
    "        if user_query == \"0\":\n",
    "            break\n",
    "        response = process_user_query(user_query, conversation_history)\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit Interface (Too heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate function to handle the Streamlit interface\n",
    "# def run_streamlit_app():\n",
    "#     st.title(\"AI Student Coach\")\n",
    "#     st.write(\"Ask your question to the AI student coach, and it will provide a thoughtful response based on your learning material.\")\n",
    "\n",
    "#     # Chatbox: User's input\n",
    "#     user_input = st.text_input(\"Enter your question:\", \"\")\n",
    "\n",
    "#     # Button to submit the query\n",
    "#     if st.button(\"Submit\"):\n",
    "#         if user_input:\n",
    "#             with st.spinner('Processing your query...'):\n",
    "#                 # Call the function to process user query\n",
    "#                 response = handle_user_query(user_input)\n",
    "#                 # Display the response from the LLM\n",
    "#                 st.markdown(f\"### AI Response:\")\n",
    "#                 st.write(response)\n",
    "#         else:\n",
    "#             st.warning(\"Please enter a question before submitting.\")\n",
    "\n",
    "\n",
    "# # Separate function to handle user query and call process_user_query\n",
    "# def handle_user_query(user_query: str):\n",
    "#     try:\n",
    "#         # Process the user query using the relevant function\n",
    "#         response = process_user_query(user_query)\n",
    "#         return response\n",
    "#     except Exception as e:\n",
    "#         # Handle any errors that occur during query processing\n",
    "#         st.error(f\"An error occurred: {str(e)}\")\n",
    "#         return \"Sorry, something went wrong.\"\n",
    "\n",
    "\n",
    "# # Main entry point for the app\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Running Streamlit app...\")\n",
    "#     run_streamlit_app()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to fix it:**\n",
    "To run Streamlit properly, you need to execute it with the streamlit run command from the terminal:\n",
    "\n",
    "- Open your terminal (or command prompt).\n",
    "- Navigate to the folder where your Streamlit script is located.\n",
    "\n",
    "**Run the following command:**\n",
    "- streamlit run your_script.py\n",
    "\n",
    "- Replace your_script.py with the name of your Python file containing the Streamlit code.\n",
    "\n",
    "**For example:**\n",
    "- streamlit run app.py\n",
    "\n",
    "Once you run Streamlit in this way, the warning should disappear, and the app will launch in your browser, providing full functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import scrolledtext\n",
    "\n",
    "# # Function to process user query (replace this with your actual implementation)\n",
    "# # def process_user_query(query):\n",
    "# #     # Example: Simulate actual processing of the query\n",
    "# #     response = process_user_query(query)\n",
    "# #     #print(response)\n",
    "# #     return response\n",
    "\n",
    "# # Function to handle the submit button click\n",
    "# def handle_user_query():\n",
    "#     user_input = query_entry.get(\"1.0\", \"end-1c\").strip()\n",
    "    \n",
    "#     if user_input:\n",
    "#         # Update status\n",
    "#         status_label.config(text=\"Processing...\")\n",
    "#         submit_button.config(state=tk.DISABLED)\n",
    "        \n",
    "#         # Clear previous results\n",
    "#         result_box.config(state=tk.NORMAL)\n",
    "#         result_box.delete(1.0, tk.END)\n",
    "        \n",
    "#         # Process the query\n",
    "#         response = process_user_query(user_input)  # Correctly call process_user_query\n",
    "        \n",
    "#         # Display the response\n",
    "#         result_box.insert(tk.END, response)\n",
    "#         result_box.config(state=tk.DISABLED)\n",
    "        \n",
    "#         # Reset status\n",
    "#         status_label.config(text=\"Ready\")\n",
    "#         submit_button.config(state=tk.NORMAL)\n",
    "#     else:\n",
    "#         result_box.config(state=tk.NORMAL)\n",
    "#         result_box.delete(1.0, tk.END)\n",
    "#         result_box.insert(tk.END, \"Please enter a query.\")\n",
    "#         result_box.config(state=tk.DISABLED)\n",
    "\n",
    "# # Set up the main window\n",
    "# window = tk.Tk()\n",
    "# window.title(\"AI Coach\")\n",
    "# window.geometry(\"600x500\")\n",
    "# window.configure(bg=\"#f5f5f5\")\n",
    "\n",
    "# # Create padding frame\n",
    "# main_frame = tk.Frame(window, bg=\"#f5f5f5\")\n",
    "# main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=15)\n",
    "\n",
    "# # Title label\n",
    "# title_label = tk.Label(main_frame, text=\"AI Coach\", font=(\"Arial\", 16, \"bold\"), bg=\"#f5f5f5\")\n",
    "# title_label.pack(pady=(0, 15))\n",
    "\n",
    "# # Query section\n",
    "# query_frame = tk.Frame(main_frame, bg=\"#f5f5f5\")\n",
    "# query_frame.pack(fill=tk.X, pady=5)\n",
    "\n",
    "# query_label = tk.Label(query_frame, text=\"Your question:\", font=(\"Arial\", 11), bg=\"#f5f5f5\", anchor=\"w\")\n",
    "# query_label.pack(fill=tk.X)\n",
    "\n",
    "# query_entry = scrolledtext.ScrolledText(main_frame, height=4, wrap=tk.WORD, font=(\"Arial\", 11))\n",
    "# query_entry.pack(fill=tk.X, pady=5)\n",
    "# query_entry.focus_set()\n",
    "\n",
    "# # Button frame\n",
    "# button_frame = tk.Frame(main_frame, bg=\"#f5f5f5\")\n",
    "# button_frame.pack(fill=tk.X, pady=10)\n",
    "\n",
    "# # Submit button\n",
    "# submit_button = tk.Button(button_frame, text=\"Ask Coach\", font=(\"Arial\", 11), \n",
    "#                          bg=\"#4a86e8\", fg=\"white\", padx=15, pady=8,\n",
    "#                          command=handle_user_query)\n",
    "# submit_button.pack(side=tk.RIGHT)\n",
    "\n",
    "# # Status label\n",
    "# status_label = tk.Label(button_frame, text=\"Ready\", font=(\"Arial\", 10), fg=\"#555555\", bg=\"#f5f5f5\")\n",
    "# status_label.pack(side=tk.LEFT, pady=10)\n",
    "\n",
    "# # Response section\n",
    "# response_label = tk.Label(main_frame, text=\"AI Coach's Response:\", font=(\"Arial\", 11), bg=\"#f5f5f5\", anchor=\"w\")\n",
    "# response_label.pack(fill=tk.X, pady=(10, 5))\n",
    "\n",
    "# result_box = scrolledtext.ScrolledText(main_frame, height=10, wrap=tk.WORD, font=(\"Arial\", 11), state=tk.DISABLED)\n",
    "# result_box.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# # Start the GUI event loop\n",
    "# window.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
