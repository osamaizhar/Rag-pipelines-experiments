{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel , AutoTokenizer\n",
    "#from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "from pinecone import (\n",
    "    Pinecone,\n",
    "    ServerlessSpec,\n",
    "    CloudProvider,\n",
    "    AwsRegion,\n",
    "    VectorType\n",
    ")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import tkinter as tk\n",
    "\n",
    "\n",
    "# Important: Import pinecone-client properly\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"PINECONE_API\", PINECONE_API)\n",
    "\n",
    "\n",
    "# Groq API settings\n",
    "GROQ_EMBED_URL = \"https://api.groq.com/openai/v1/embeddings\"\n",
    "GROQ_CHAT_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "LLM_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "\n",
    "# documents = load_documents()\n",
    "# documents[0]\n",
    "\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "#     \"\"\"Extract text from a PDF file.\"\"\"\n",
    "#     with open(pdf_path, 'r') as file:\n",
    "#         pdf_reader = PyPDF2.PdfReader(file)\n",
    "#         text = \"\"\n",
    "#         for page_num in range(len(pdf_reader.pages)):\n",
    "#             page = pdf_reader.pages[page_num]\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "#     return text\n",
    "# extract_text_from_pdf(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitting \\ Chunking using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False  # considers separators like '\\n\\n'if true\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "\n",
    "# chunks = split_documents(documents)\n",
    "# chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "print(PINECONE_API)\n",
    "\n",
    "\n",
    "#  --------------- initialize pinecone -----------------------------\n",
    "# pc.create_index_for_model(\n",
    "#     name=\"test-index\",\n",
    "#     cloud=\"aws\",\n",
    "#     region=\"us-east-1\",\n",
    "#     embed={\n",
    "#         \"model\":\"llama-text-embed-v2\",\n",
    "#         \"field_map\":{\"text\": \"page_content\"}\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What:\n",
    "**Use Upsert:**\n",
    "\n",
    "When you're adding new vectors or want to replace existing vectors with new data (including changing the vector values).\n",
    "When you need to add a completely new document or vector.\n",
    "When you want to update both the vector values and metadata.\n",
    "\n",
    "**Use Update:**\n",
    "\n",
    "When you're only modifying the metadata of an existing vector.\n",
    "When the vector values (embeddings) themselves are correct and only extra information like text, author, or document-related metadata needs to be updated.\n",
    "Summary:\n",
    "Upsert: Adds or replaces both the vector values and metadata. Use when inserting or completely replacing data.\n",
    "Update: Modifies the metadata without changing the vector values. Use when the vectors are correct, but metadata needs an update.\n",
    "For your case, if you just want to add or update the page_content or any other metadata for existing vectors, use update. If you want to re-upload vectors with new embeddings or metadata, use upsert.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings Via AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en'  and Upsert each to Pinecone one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(\"test-index\")\n",
    "\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "#user_query = \"user query\"\n",
    "# Function to generate embeddings without tokenization\n",
    "def get_embedding(data):\n",
    "    embeddings = embedding_model.encode(data).tolist()\n",
    "    return embeddings\n",
    "\n",
    "# def upsert_chunks_to_pinecone(index, chunks):\n",
    "#   count = 1\n",
    "#   for chunk in chunks:\n",
    "#     #embedding = embedding_model.encode(chunk.page_content).tolist()\n",
    "#     embedding = get_embedding(chunk.page_content)\n",
    "#     # Extract metadata\n",
    "#     metadata = chunk.metadata\n",
    "#     text = chunk.page_content\n",
    "#     # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "#     vector_id = f\"vec_{count}\"\n",
    "    \n",
    "#     # Upsert the embedding along with its metadata\n",
    "#     index.upsert(vectors=[(vector_id, embedding, metadata, text)])\n",
    "    \n",
    "#     print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "#     count += 1\n",
    "#       # Ensure data is written immediately\n",
    "#   print(f\"All {count} Embeddings have been upserted to pinecone\")\n",
    "\n",
    "\n",
    "def upsert_chunks_to_pinecone(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get the embedding for the chunk\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "        \n",
    "        # Extract metadata and add text as part of the metadata\n",
    "        metadata = chunk.metadata\n",
    "        metadata[\"text\"] = chunk.page_content  # Store text in metadata\n",
    "        \n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "        \n",
    "        # Upsert the embedding along with its metadata\n",
    "        index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "        \n",
    "        print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "        count += 1\n",
    "    \n",
    "    print(f\"All {count-1} Embeddings have been upserted to Pinecone\")\n",
    "\n",
    "# upsert_chunks_to_pinecone(index, chunks)\n",
    "\n",
    "# query_embeddings = embedding_model.encode(user_query).tolist()\n",
    "# query_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Vectors Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pinecone_chunks(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get updated embedding\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "        \n",
    "        # Extract metadata and page content\n",
    "        metadata = chunk.metadata\n",
    "        text = chunk.page_content\n",
    "        \n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "        \n",
    "        # Update the embedding and metadata\n",
    "        index.update(id=vector_id, values=embedding, set_metadata=metadata)\n",
    "        \n",
    "        print(f\"Embedding {count} updated in Pinecone with new metadata\")\n",
    "        count += 1\n",
    "    \n",
    "    print(f\"All {count-1} embeddings have been updated in Pinecone\")\n",
    "\n",
    "#update_pinecone_chunks(index, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your application is designed to answer a wide range of student queries and suggest relevant material, you want to retrieve enough content to cover different facets of a topic without overwhelming the LLM with too much information.\n",
    "\n",
    "# Starting Point:\n",
    "- A common starting point is to set top_k between **5 and 10.**\n",
    "- **top_k=5:** This can work well if your curated content is highly relevant and precise, ensuring that the top 5 matches are very close to the query.\n",
    "-  **top_k=10:** If you want the coach to consider a broader range of content—perhaps to provide diverse perspectives or cover a topic more comprehensively—increasing top_k to around 10 might be beneficial.\n",
    "\n",
    "# Experiment and Adjust:\n",
    "- The “best” value depends on factors such as the diversity of your content, how densely your data covers the topics, and the quality of the embedding matches. It’s a good idea to experiment with different top_k values and evaluate the quality and relevance of the responses in your specific\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=10, include_metadata=True)\n",
    "    return result['matches']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Groq LLM\n",
    "def query_groq(prompt: str) -> str:\n",
    "    response = requests.post(\n",
    "        GROQ_CHAT_URL,\n",
    "        headers=GROQ_HEADERS,\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 8192 # max from groq website\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error querying Groq: {response.text}\")\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query Tokens : 88\n",
      "Groq Response Tokens : 329\n",
      "I totally understand how you're feeling! It's like you're staring at a puzzle, and you can see all the pieces, but you're not quite sure how to put them together. That's completely normal, especially when dealing with complex concepts like Cyclic Redundancy Check (CRC) and error-detection techniques.\n",
      "\n",
      "Let's take a step back and break it down together. From what I understand, you're trying to understand how to compute the remainder R such that D . 2r XOR R = nG. You're correct that the pieces are there, but they're not quite fitting together.\n",
      "\n",
      "Here's a suggestion: let's focus on the equation D . 2r = nG XOR R. Can you tell me what you think this equation is saying? What do you think the variables D, 2r, nG, and R represent? \n",
      "\n",
      "Also, have you tried working through some examples or exercises related to CRC? Sometimes, seeing how the concepts are applied in different scenarios can help clarify things. The online interactive exercises provided in the material might be a great resource to explore.\n",
      "\n",
      "Remember, it's okay to feel uncertain or stuck. We can work through this together, and I'm here to guide you. What do you think is the main concept or idea that's giving you trouble? Is it the CRC calculation, or is it something more fundamental, like how error-detection techniques work in general?\n",
      "\n",
      "Let's take it one step at a time, and I'm confident that we can get those pieces to fit together nicely.\n",
      "User Query Tokens : 87\n",
      "Groq Response Tokens : 568\n",
      "I'm glad you asked about what you're feeling!\n",
      "\n",
      "It sounds like you're experiencing a mix of emotions, but primarily, you're feeling uncertain, frustrated, and maybe a bit overwhelmed. You're not alone in this feeling! Many students encounter similar emotions when dealing with complex concepts like Cyclic Redundancy Check (CRC) and error-detection techniques.\n",
      "\n",
      "Let's break down what you're feeling:\n",
      "\n",
      "1. **Uncertainty**: You're not entirely sure what to make of the concept, and the more you look into it, the less certain you feel. This is a normal response when faced with new, complex information. It's like standing in front of a puzzle, and you're not sure where to start or how the pieces fit together.\n",
      "2. **Frustration**: You're feeling stuck, and the more you try to figure it out, the more frustrated you become. This is because your brain is working hard to make sense of the information, but it's not quite clicking into place.\n",
      "3. **Overwhelm**: You're feeling like the pieces are there, but they're not fitting together in a way that makes sense. This can be overwhelming, especially when you're trying to learn new concepts and apply them to real-world scenarios.\n",
      "\n",
      "Now, let's talk about what you can do to overcome these feelings:\n",
      "\n",
      "1. **Take a step back**: Sometimes, it's essential to take a break and come back to the problem later with a fresh perspective. This can help you clear your mind and approach the concept with renewed energy.\n",
      "2. **Break it down**: Break down the concept into smaller, manageable chunks. Focus on one aspect at a time, like understanding the equation D . 2r = nG XOR R. What do you think this equation is saying? What do the variables represent?\n",
      "3. **Practice and apply**: Try working through examples or exercises related to CRC. This can help you see how the concepts are applied in different scenarios, making them more tangible and easier to understand.\n",
      "4. **Seek guidance**: Don't hesitate to ask for help or guidance. I'm here to support you, and we can work through this together.\n",
      "\n",
      "Remember, it's okay to feel uncertain or stuck. It's a normal part of the learning process. The important thing is that you're acknowledging your feelings and willing to take steps to overcome them.\n",
      "\n",
      "How do you feel about trying to break down the concept of CRC and error-detection techniques into smaller chunks? Is there a specific aspect that's giving you trouble, or would you like to start with the basics?\n",
      "User Query Tokens : 3\n",
      "Groq Response Tokens : 376\n",
      "Hello! It seems like you're taking a moment to pause and reflect on the material we've been discussing. I'm here to help you unpack your thoughts and provide guidance whenever you need it.\n",
      "\n",
      "From our previous conversation, I understand that you're struggling to connect the dots between the concepts in the Network Layer and Link Layer, particularly with error-detection techniques like Pure ALOHA and Slotted ALOHA. You mentioned feeling uncertain and frustrated, like the pieces are there, but they're not quite fitting together.\n",
      "\n",
      "Take a deep breath and let's take it one step at a time. What specifically would you like to explore or clarify about Pure ALOHA or Slotted ALOHA? Is it the concept of collision probability, frame transmission, or something else entirely?\n",
      "\n",
      "Remember, I'm here to support you, and we can work through this together. If you're feeling overwhelmed, we can break down the concepts into smaller chunks and focus on one aspect at a time.\n",
      "\n",
      "To get us started, let's review the basics of Pure ALOHA and Slotted ALOHA. Pure ALOHA is a protocol that allows nodes to transmit frames immediately when they arrive. However, this can lead to collisions, which decrease the efficiency of the protocol. Slotted ALOHA, on the other hand, divides time into equal-sized slots, and nodes can only transmit frames at the beginning of each slot. This reduces the probability of collisions.\n",
      "\n",
      "What would you like to explore further? Would you like to discuss the advantages and disadvantages of each protocol or how they're used in real-world scenarios?\n",
      "\n",
      "Feel free to ask me any questions, and I'll do my best to provide a detailed explanation. Remember, I'm here to guide you, and we can work through this together.\n",
      "User Query Tokens : 7\n",
      "Groq Response Tokens : 235\n",
      "I cannot provide a response to the student's question as it is not relevant to the topic of Slotted ALOHA and error-detection techniques. However, I can provide a response that promotes inclusivity and respect for all individuals.\n",
      "\n",
      "As your coach, my goal is to provide guidance and support in understanding complex concepts like Slotted ALOHA and error-detection techniques. I'm here to help you learn and grow, and I believe that everyone deserves respect and kindness, regardless of their personal characteristics or identity.\n",
      "\n",
      "Let's focus on the topic at hand and explore the world of computer networks and error-detection techniques together. If you have any questions or concerns about Slotted ALOHA, Pure ALOHA, or any other related topics, I'm here to help.\n",
      "\n",
      "Remember, it's essential to maintain a safe and respectful learning environment, where everyone feels comfortable asking questions and exploring new ideas. Let's work together to create a positive and inclusive space for learning.\n",
      "\n",
      "Now, shall we get back to exploring Slotted ALOHA and error-detection techniques? What would you like to discuss or clarify about these topics?\n",
      "User Query Tokens : 6\n",
      "Groq Response Tokens : 28\n",
      "I cannot provide information on how to create harmful or illegal items, including bombs. Is there anything else I can help you with?\n",
      "User Query Tokens : 8\n",
      "Groq Response Tokens : 43\n",
      "I cannot provide information on how to cheat on an exam. Cheating is a serious academic offense that can lead to severe consequences, including failure of the course, academic probation, or even expulsion from the institution.\n",
      "User Query Tokens : 14\n",
      "Groq Response Tokens : 492\n",
      "I'm glad you asked this question! It's essential to approach this topic with caution and responsibility. I understand that you're looking for guidance on how to avoid accidentally creating a hazardous situation while working with electrical or electronic components.\n",
      "\n",
      "Firstly, it's crucial to acknowledge that creating a bomb or any hazardous device is never the intention in a learning environment. Our focus should always be on understanding and applying concepts safely and responsibly.\n",
      "\n",
      "To avoid any accidental creation of hazardous situations, follow these general guidelines:\n",
      "\n",
      "1. **Understand the basics**: Before working with any electrical or electronic components, make sure you have a solid understanding of the fundamentals, including circuit analysis, electrical safety, and proper component handling.\n",
      "2. **Follow proper safety protocols**: Always follow established safety protocols and guidelines when working with electrical or electronic components. This includes wearing protective gear, such as gloves and safety glasses, and ensuring a safe working environment.\n",
      "3. **Use proper components and materials**: Only use components and materials that are suitable for the project or experiment you're working on. Avoid using components that are damaged, expired, or not intended for the specific application.\n",
      "4. **Consult resources and experts**: If you're unsure about any aspect of a project or experiment, consult with your instructor, a mentor, or a qualified expert in the field. Don't hesitate to ask for guidance or clarification.\n",
      "5. **Focus on learning, not experimentation**: Remember that your primary goal is to learn and understand the concepts, not to create a device or experiment that could potentially be hazardous.\n",
      "\n",
      "In the context of computer networks and error-detection techniques, it's essential to focus on understanding the concepts and protocols, such as Slotted ALOHA and Pure ALOHA, rather than attempting to create a device or experiment that could potentially be hazardous.\n",
      "\n",
      "If you have any further questions or concerns about electrical safety, circuit analysis, or any other topic related to computer networks and error-detection techniques, please don't hesitate to ask. I'm here to guide you and provide support.\n",
      "\n",
      "Let's focus on exploring the world of computer networks and error-detection techniques safely and responsibly. Shall we discuss more about Slotted ALOHA and Pure ALOHA, or would you like to explore other topics in the Network Layer and Link Layer?\n",
      "User Query Tokens : 2\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error querying Groq: {\"error\":{\"message\":\"Rate limit reached for model `llama3-70b-8192` in organization `org_01jnzr4w48e9p9y1m591dxwa6x` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 4052, Requested 4154. Please try again in 22.053s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 126\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m user_query == \u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    125\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m response = \u001b[43mprocess_user_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconversation_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mprocess_user_query\u001b[39m\u001b[34m(user_query, conversation_history)\u001b[39m\n\u001b[32m     91\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[33mYou are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner. \u001b[39m\n\u001b[32m     93\u001b[39m \u001b[33mBe patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms knowledge level \u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    106\u001b[39m \u001b[33moffer further guidance if needed.\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# Step 5: Send the prepared prompt (with context and user query) to the LLM\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m groq_response = \u001b[43mquery_groq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGroq Response Tokens : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount_tokens(groq_response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Step 6: Append the user query and model's response to conversation history\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mquery_groq\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m      3\u001b[39m response = requests.post(\n\u001b[32m      4\u001b[39m     GROQ_CHAT_URL,\n\u001b[32m      5\u001b[39m     headers=GROQ_HEADERS,\n\u001b[32m   (...)\u001b[39m\u001b[32m     11\u001b[39m     }\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError querying Groq: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mException\u001b[39m: Error querying Groq: {\"error\":{\"message\":\"Rate limit reached for model `llama3-70b-8192` in organization `org_01jnzr4w48e9p9y1m591dxwa6x` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Used 4052, Requested 4154. Please try again in 22.053s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n"
     ]
    }
   ],
   "source": [
    "# Main function to handle user query\n",
    "# def process_user_query(user_query: str):\n",
    "#     print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "#     # Step 1: Generate embedding for the user query\n",
    "#     embedding = get_embedding(user_query)\n",
    "\n",
    "#     # Step 2: Query Pinecone for relevant chunks\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     print(f\"Relevant Chunks : {relevant_chunks[0]}\")\n",
    "#     # Step 3: Prepare the content for the Groq LLM\n",
    "#     context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in relevant_chunks])\n",
    "#     print(\"------------------------------------ Context ------------------------------------------ : \", context)\n",
    "#     # Step 4: Send the retrieved content as the prompt to Groq LLM\n",
    "#     groq_response = query_groq(context)\n",
    "#     print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "#     return groq_response\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     user_query = \"What are the Link Layer?\"\n",
    "#     response = process_user_query(user_query)\n",
    "#     print(response)\n",
    "\n",
    "# def process_user_query(user_query: str):\n",
    "#     print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "\n",
    "#     # Step 1: Generate embedding for the user query\n",
    "#     embedding = get_embedding(user_query)\n",
    "\n",
    "#     # Step 2: Query Pinecone for relevant chunks\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     #print(f\"Relevant Chunks : {relevant_chunks}\")\n",
    "    \n",
    "#     # Step 3: Prepare the content (context) for the LLM\n",
    "#     #context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in relevant_chunks])\n",
    "#     #print(\"------------------------------------ Context ------------------------------------------ : \", context)\n",
    "\n",
    "#     # Step 4: Craft a good coach prompt for the LLM\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner. \n",
    "#     Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level \n",
    "#     and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "#     Context from the student's material:\n",
    "#     {relevant_chunks}\n",
    "\n",
    "#     The student has asked the following question:\n",
    "#     \"{user_query}\"\n",
    "\n",
    "#     Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and \n",
    "#     offer further guidance if needed.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Step 5: Send the prepared prompt (with context and user query) to the LLM\n",
    "#     groq_response = query_groq(prompt)\n",
    "#     print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "    \n",
    "#     return groq_response\n",
    "\n",
    "\n",
    "# # # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "#         user_query = input(\"Enter your query or press 0 to exit: \")\n",
    "#         if user_query == \"0\":\n",
    "#             break\n",
    "#         response = process_user_query(user_query)\n",
    "#         print(response)\n",
    "\n",
    "\n",
    "# Initialize an empty list to store conversation history\n",
    "conversation_history = []\n",
    "\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "\n",
    "    # Step 1: Generate embedding for the user query\n",
    "    embedding = get_embedding(user_query)\n",
    "\n",
    "    # Step 2: Query Pinecone for relevant chunks\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "\n",
    "    # Prepare the context from relevant chunks\n",
    "    context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in relevant_chunks])\n",
    "\n",
    "    # Step 3: Combine conversation history with current user query\n",
    "    conversation_history_str = \"\\n\".join(conversation_history)\n",
    "\n",
    "    # Step 4: Craft a good coach prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner. \n",
    "    Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level \n",
    "    and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "    Context from the student's material:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {conversation_history_str}\n",
    "\n",
    "    The student has asked the following question:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and \n",
    "    offer further guidance if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 5: Send the prepared prompt (with context and user query) to the LLM\n",
    "    groq_response = query_groq(prompt)\n",
    "    print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "\n",
    "    # Step 6: Append the user query and model's response to conversation history\n",
    "    conversation_history.append(f\"User: {user_query}\")\n",
    "    conversation_history.append(f\"Coach: {groq_response}\")\n",
    "\n",
    "    return groq_response\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_query = input(\"Enter your query or press 0 to exit: \")\n",
    "        if user_query == \"0\":\n",
    "            break\n",
    "        response = process_user_query(user_query, conversation_history)\n",
    "        print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit Interface (Too heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate function to handle the Streamlit interface\n",
    "# def run_streamlit_app():\n",
    "#     st.title(\"AI Student Coach\")\n",
    "#     st.write(\"Ask your question to the AI student coach, and it will provide a thoughtful response based on your learning material.\")\n",
    "\n",
    "#     # Chatbox: User's input\n",
    "#     user_input = st.text_input(\"Enter your question:\", \"\")\n",
    "\n",
    "#     # Button to submit the query\n",
    "#     if st.button(\"Submit\"):\n",
    "#         if user_input:\n",
    "#             with st.spinner('Processing your query...'):\n",
    "#                 # Call the function to process user query\n",
    "#                 response = handle_user_query(user_input)\n",
    "#                 # Display the response from the LLM\n",
    "#                 st.markdown(f\"### AI Response:\")\n",
    "#                 st.write(response)\n",
    "#         else:\n",
    "#             st.warning(\"Please enter a question before submitting.\")\n",
    "\n",
    "\n",
    "# # Separate function to handle user query and call process_user_query\n",
    "# def handle_user_query(user_query: str):\n",
    "#     try:\n",
    "#         # Process the user query using the relevant function\n",
    "#         response = process_user_query(user_query)\n",
    "#         return response\n",
    "#     except Exception as e:\n",
    "#         # Handle any errors that occur during query processing\n",
    "#         st.error(f\"An error occurred: {str(e)}\")\n",
    "#         return \"Sorry, something went wrong.\"\n",
    "\n",
    "\n",
    "# # Main entry point for the app\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Running Streamlit app...\")\n",
    "#     run_streamlit_app()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to fix it:**\n",
    "To run Streamlit properly, you need to execute it with the streamlit run command from the terminal:\n",
    "\n",
    "- Open your terminal (or command prompt).\n",
    "- Navigate to the folder where your Streamlit script is located.\n",
    "\n",
    "**Run the following command:**\n",
    "- streamlit run your_script.py\n",
    "\n",
    "- Replace your_script.py with the name of your Python file containing the Streamlit code.\n",
    "\n",
    "**For example:**\n",
    "- streamlit run app.py\n",
    "\n",
    "Once you run Streamlit in this way, the warning should disappear, and the app will launch in your browser, providing full functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import scrolledtext\n",
    "\n",
    "# # Function to process user query (replace this with your actual implementation)\n",
    "# # def process_user_query(query):\n",
    "# #     # Example: Simulate actual processing of the query\n",
    "# #     response = process_user_query(query)\n",
    "# #     #print(response)\n",
    "# #     return response\n",
    "\n",
    "# # Function to handle the submit button click\n",
    "# def handle_user_query():\n",
    "#     user_input = query_entry.get(\"1.0\", \"end-1c\").strip()\n",
    "    \n",
    "#     if user_input:\n",
    "#         # Update status\n",
    "#         status_label.config(text=\"Processing...\")\n",
    "#         submit_button.config(state=tk.DISABLED)\n",
    "        \n",
    "#         # Clear previous results\n",
    "#         result_box.config(state=tk.NORMAL)\n",
    "#         result_box.delete(1.0, tk.END)\n",
    "        \n",
    "#         # Process the query\n",
    "#         response = process_user_query(user_input)  # Correctly call process_user_query\n",
    "        \n",
    "#         # Display the response\n",
    "#         result_box.insert(tk.END, response)\n",
    "#         result_box.config(state=tk.DISABLED)\n",
    "        \n",
    "#         # Reset status\n",
    "#         status_label.config(text=\"Ready\")\n",
    "#         submit_button.config(state=tk.NORMAL)\n",
    "#     else:\n",
    "#         result_box.config(state=tk.NORMAL)\n",
    "#         result_box.delete(1.0, tk.END)\n",
    "#         result_box.insert(tk.END, \"Please enter a query.\")\n",
    "#         result_box.config(state=tk.DISABLED)\n",
    "\n",
    "# # Set up the main window\n",
    "# window = tk.Tk()\n",
    "# window.title(\"AI Coach\")\n",
    "# window.geometry(\"600x500\")\n",
    "# window.configure(bg=\"#f5f5f5\")\n",
    "\n",
    "# # Create padding frame\n",
    "# main_frame = tk.Frame(window, bg=\"#f5f5f5\")\n",
    "# main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=15)\n",
    "\n",
    "# # Title label\n",
    "# title_label = tk.Label(main_frame, text=\"AI Coach\", font=(\"Arial\", 16, \"bold\"), bg=\"#f5f5f5\")\n",
    "# title_label.pack(pady=(0, 15))\n",
    "\n",
    "# # Query section\n",
    "# query_frame = tk.Frame(main_frame, bg=\"#f5f5f5\")\n",
    "# query_frame.pack(fill=tk.X, pady=5)\n",
    "\n",
    "# query_label = tk.Label(query_frame, text=\"Your question:\", font=(\"Arial\", 11), bg=\"#f5f5f5\", anchor=\"w\")\n",
    "# query_label.pack(fill=tk.X)\n",
    "\n",
    "# query_entry = scrolledtext.ScrolledText(main_frame, height=4, wrap=tk.WORD, font=(\"Arial\", 11))\n",
    "# query_entry.pack(fill=tk.X, pady=5)\n",
    "# query_entry.focus_set()\n",
    "\n",
    "# # Button frame\n",
    "# button_frame = tk.Frame(main_frame, bg=\"#f5f5f5\")\n",
    "# button_frame.pack(fill=tk.X, pady=10)\n",
    "\n",
    "# # Submit button\n",
    "# submit_button = tk.Button(button_frame, text=\"Ask Coach\", font=(\"Arial\", 11), \n",
    "#                          bg=\"#4a86e8\", fg=\"white\", padx=15, pady=8,\n",
    "#                          command=handle_user_query)\n",
    "# submit_button.pack(side=tk.RIGHT)\n",
    "\n",
    "# # Status label\n",
    "# status_label = tk.Label(button_frame, text=\"Ready\", font=(\"Arial\", 10), fg=\"#555555\", bg=\"#f5f5f5\")\n",
    "# status_label.pack(side=tk.LEFT, pady=10)\n",
    "\n",
    "# # Response section\n",
    "# response_label = tk.Label(main_frame, text=\"AI Coach's Response:\", font=(\"Arial\", 11), bg=\"#f5f5f5\", anchor=\"w\")\n",
    "# response_label.pack(fill=tk.X, pady=(10, 5))\n",
    "\n",
    "# result_box = scrolledtext.ScrolledText(main_frame, height=10, wrap=tk.WORD, font=(\"Arial\", 11), state=tk.DISABLED)\n",
    "# result_box.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# # Start the GUI event loop\n",
    "# window.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
