{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Excel and pdf parser and handling for multiple directories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API pcsk_4bLR9o_3crxHE9zjHW76VdRnBPi2Xo794pQnKSifnRfQ9iQc6U3iqeqeyVEZ3RjBPYtoD4\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "from pinecone import (\n",
    "    Pinecone,\n",
    "    ServerlessSpec,\n",
    "    CloudProvider,\n",
    "    AwsRegion,\n",
    "    VectorType\n",
    ")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import tkinter as tk\n",
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "import concurrent.futures\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Important: Import pinecone-client properly\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"PINECONE_API\", PINECONE_API)\n",
    "\n",
    "\n",
    "# Groq API settings\n",
    "GROQ_EMBED_URL = \"https://api.groq.com/openai/v1/embeddings\"\n",
    "GROQ_CHAT_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "LLM_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "\n",
    "# documents = pdf_load_documents()\n",
    "# documents\n",
    "\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "#     \"\"\"Extract text from a PDF file.\"\"\"\n",
    "#     with open(pdf_path, 'r') as file:\n",
    "#         pdf_reader = PyPDF2.PdfReader(file)\n",
    "#         text = \"\"\n",
    "#         for page_num in range(len(pdf_reader.pages)):\n",
    "#             page = pdf_reader.pages[page_num]\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "#     return text\n",
    "# extract_text_from_pdf(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF and Excel Parser (Maqbool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files parsed:\n",
      "\n",
      "Total documents loaded: 360\n"
     ]
    }
   ],
   "source": [
    "# def load_documents():\n",
    "#     documents = []\n",
    "\n",
    "#     # Load PDFs\n",
    "#     pdf_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "#     pdf_documents = pdf_loader.load()\n",
    "#     documents.extend(pdf_documents)\n",
    "\n",
    "#     # Load Excel files\n",
    "#     excel_files = glob.glob(os.path.join(DATA_PATH, \"*.xlsx\"))\n",
    "#     if excel_files:\n",
    "#         excel_documents = [pd.read_excel(file) for file in excel_files]\n",
    "#         documents.extend(excel_documents)\n",
    "\n",
    "#     if not documents:\n",
    "#         raise FileNotFoundError(f\"No PDF or Excel files found in {DATA_PATH}\")\n",
    "\n",
    "#     return documents\n",
    "\n",
    "# # Load documents\n",
    "# documents = load_documents()\n",
    "\n",
    "# print(documents)\n",
    "# # Display the first document (PDF or Excel)\n",
    "# if isinstance(documents[0], pd.DataFrame):\n",
    "#     print(documents[0].head())  # Print first few rows if it's an Excel file\n",
    "# else:\n",
    "#     print(documents[0])  # Print the content if it's a PDF document\n",
    "\n",
    "\n",
    "\n",
    "def load_documents():\n",
    "    documents = []\n",
    "    file_names = []\n",
    "\n",
    "    # Load PDFs\n",
    "    pdf_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    pdf_documents = pdf_loader.load()\n",
    "    documents.extend(pdf_documents)\n",
    "    \n",
    "    # Get PDF filenames\n",
    "    pdf_files = glob.glob(os.path.join(DATA_PATH, \"*.pdf\"))\n",
    "    file_names.extend([os.path.basename(file) for file in pdf_files])\n",
    "\n",
    "    # Load Excel files\n",
    "    excel_files = glob.glob(os.path.join(DATA_PATH, \"*.xlsx\"))\n",
    "    if excel_files:\n",
    "        for file in excel_files:\n",
    "            documents.append(pd.read_excel(file))\n",
    "            file_names.append(os.path.basename(file))\n",
    "\n",
    "    if not documents:\n",
    "        raise FileNotFoundError(f\"No PDF or Excel files found in {DATA_PATH}\")\n",
    "\n",
    "    return documents, file_names\n",
    "\n",
    "# Load documents\n",
    "documents, file_names = load_documents()\n",
    "\n",
    "# Print file names\n",
    "print(\"Files parsed:\")\n",
    "for name in file_names:\n",
    "    print(f\"- {name}\")\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No PDF or Excel files found in D:\\Disrupt Labs\\Rag Experiments\\env\\Rag-pipelines-experiments/data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load PDF and Excel files from DATA_PATH and its subdirectories.\n",
    "    Returns a list of documents with content and metadata.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "\n",
    "    # Load PDFs from all subdirectories\n",
    "    pdf_loader = PyPDFDirectoryLoader(DATA_PATH, recursive=True)\n",
    "    pdf_docs = pdf_loader.load()\n",
    "    for doc in pdf_docs:\n",
    "        documents.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": {\"source\": doc.metadata.get(\"source\", \"unknown\"), \"file_type\": \"pdf\"}\n",
    "        })\n",
    "\n",
    "    # Load Excel files from all subdirectories\n",
    "    for root, _, files in os.walk(DATA_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xlsx\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_excel(file_path)\n",
    "                headers = df.columns.tolist()\n",
    "                for _, row in df.iterrows():\n",
    "                    content = \" \".join([f\"{col}: {str(row[col])}\" for col in headers])\n",
    "                    documents.append({\n",
    "                        \"content\": content,\n",
    "                        \"metadata\": {\"source\": file_path, \"file_type\": \"excel\"}\n",
    "                    })\n",
    "\n",
    "    if not documents:\n",
    "        print(f\"No PDF or Excel files found in {DATA_PATH}\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "    return documents\n",
    "documents =  load_documents()\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitting \\ Chunking using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False  # considers separators like '\\n\\n'if true\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "\n",
    "# chunks = split_documents(documents)\n",
    "# chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcsk_4bLR9o_3crxHE9zjHW76VdRnBPi2Xo794pQnKSifnRfQ9iQc6U3iqeqeyVEZ3RjBPYtoD4\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "print(PINECONE_API)\n",
    "\n",
    "\n",
    "#  --------------- initialize pinecone -----------------------------\n",
    "# pc.create_index_for_model(\n",
    "#     name=\"test-index\",\n",
    "#     cloud=\"aws\",\n",
    "#     region=\"us-east-1\",\n",
    "#     embed={\n",
    "#         \"model\":\"llama-text-embed-v2\",\n",
    "#         \"field_map\":{\"text\": \"page_content\"}\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What:\n",
    "**Use Upsert:**\n",
    "\n",
    "When you're adding new vectors or want to replace existing vectors with new data (including changing the vector values).\n",
    "When you need to add a completely new document or vector.\n",
    "When you want to update both the vector values and metadata.\n",
    "\n",
    "**Use Update:**\n",
    "\n",
    "When you're only modifying the metadata of an existing vector.\n",
    "When the vector values (embeddings) themselves are correct and only extra information like text, author, or document-related metadata needs to be updated.\n",
    "Summary:\n",
    "Upsert: Adds or replaces both the vector values and metadata. Use when inserting or completely replacing data.\n",
    "Update: Modifies the metadata without changing the vector values. Use when the vectors are correct, but metadata needs an update.\n",
    "For your case, if you just want to add or update the page_content or any other metadata for existing vectors, use update. If you want to re-upload vectors with new embeddings or metadata, use upsert.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings Via AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en'  and Upsert each to Pinecone one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(\"ai-coach\")\n",
    "#index = pc.Index(\"ahsan-400pg-pdf-doc-test\")\n",
    "\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained(\n",
    "    'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "# user_query = \"user query\"\n",
    "# Function to generate embeddings without tokenization\n",
    "\n",
    "\n",
    "def get_embedding(data):\n",
    "    embeddings = embedding_model.encode(data).tolist()\n",
    "    return embeddings\n",
    "\n",
    "# def upsert_chunks_to_pinecone(index, chunks):\n",
    "#   count = 1\n",
    "#   for chunk in chunks:\n",
    "#     #embedding = embedding_model.encode(chunk.page_content).tolist()\n",
    "#     embedding = get_embedding(chunk.page_content)\n",
    "#     # Extract metadata\n",
    "#     metadata = chunk.metadata\n",
    "#     text = chunk.page_content\n",
    "#     # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "#     vector_id = f\"vec_{count}\"\n",
    "\n",
    "#     # Upsert the embedding along with its metadata\n",
    "#     index.upsert(vectors=[(vector_id, embedding, metadata, text)])\n",
    "\n",
    "#     print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "#     count += 1\n",
    "#       # Ensure data is written immediately\n",
    "#   print(f\"All {count} Embeddings have been upserted to pinecone\")\n",
    "\n",
    "\n",
    "def upsert_chunks_to_pinecone(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get the embedding for the chunk\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "\n",
    "        # Extract metadata and add text as part of the metadata\n",
    "        metadata = chunk.metadata\n",
    "        metadata[\"text\"] = chunk.page_content  # Store text in metadata\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Upsert the embedding along with its metadata\n",
    "        index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "\n",
    "        print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"All {count-1} Embeddings have been upserted to Pinecone\")\n",
    "\n",
    "\n",
    "#upsert_chunks_to_pinecone(index, chunks)\n",
    "\n",
    "# query_embeddings = embedding_model.encode(user_query).tolist()\n",
    "# query_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Vectors Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pinecone_chunks(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get updated embedding\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "\n",
    "        # Extract metadata and page content\n",
    "        metadata = chunk.metadata\n",
    "        text = chunk.page_content\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Update the embedding and metadata\n",
    "        index.update(id=vector_id, values=embedding, set_metadata=metadata)\n",
    "\n",
    "        print(f\"Embedding {count} updated in Pinecone with new metadata\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"All {count-1} embeddings have been updated in Pinecone\")\n",
    "\n",
    "# update_pinecone_chunks(index, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your application is designed to answer a wide range of student queries and suggest relevant material, you want to retrieve enough content to cover different facets of a topic without overwhelming the LLM with too much information.\n",
    "\n",
    "# Starting Point:\n",
    "- A common starting point is to set top_k between **5 and 10.**\n",
    "- **top_k=5:** This can work well if your curated content is highly relevant and precise, ensuring that the top 5 matches are very close to the query.\n",
    "-  **top_k=10:** If you want the coach to consider a broader range of content—perhaps to provide diverse perspectives or cover a topic more comprehensively—increasing top_k to around 10 might be beneficial.\n",
    "\n",
    "# Experiment and Adjust:\n",
    "- The “best” value depends on factors such as the diversity of your content, how densely your data covers the topics, and the quality of the embedding matches. It’s a good idea to experiment with different top_k values and evaluate the quality and relevance of the responses in your specific\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=10, include_metadata=True)\n",
    "    return result['matches']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Groq LLM\n",
    "def query_groq(prompt: str) -> str:\n",
    "    response = requests.post(\n",
    "        GROQ_CHAT_URL,\n",
    "        headers=GROQ_HEADERS,\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 8192  # max from groq website\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error querying Groq: {response.text}\")\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation_history = [\"# 🧑‍🏫 AI Coaching Assistant\\nWelcome! I'm here to help you learn. Type your question below.\"]\n",
    "\n",
    "# def process_user_query(user_query: str, conversation_history: list):\n",
    "\n",
    "#     print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "\n",
    "#     # Step 1: Generate embedding for the user query\n",
    "#     embedding = get_embedding(user_query)\n",
    "\n",
    "#     # Step 2: Query Pinecone for relevant chunks\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "\n",
    "#     # Prepare the context from relevant chunks\n",
    "#     context = \"\\n\".join([chunk['metadata'][\"text\"]\n",
    "#                         for chunk in relevant_chunks])\n",
    "#     print(\"CONTEXT: \", context)\n",
    "\n",
    "#     # Step 3: Combine conversation history with current user query\n",
    "#     conversation_history_str = \"\\n\".join(conversation_history)\n",
    "\n",
    "#     # Step 4: Craft a good coach prompt for the LLM\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner. \n",
    "#     Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level \n",
    "#     and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "#     Context from the student's material:\n",
    "#     {context}\n",
    "\n",
    "#     Conversation history:\n",
    "#     {conversation_history_str}\n",
    "\n",
    "#     The student has asked the following question:\n",
    "#     \"{user_query}\"\n",
    "\n",
    "#     Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and \n",
    "#     offer further guidance if needed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Step 5: Send the prepared prompt (with context and user query) to the LLM\n",
    "#     groq_response = query_groq(prompt)\n",
    "#     print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "\n",
    "#     # Step 6: Append the user query and model's response to conversation history\n",
    "#     conversation_history.append(f\"User: {user_query}\")\n",
    "#     conversation_history.append(f\"Coach: {groq_response}\")\n",
    "\n",
    "#     return groq_response\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "#         print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "#         user_query = input(\"Enter your query or press 0 to exit: \")\n",
    "#         if user_query == \"0\":\n",
    "#             break\n",
    "#         response = process_user_query(user_query, conversation_history)\n",
    "#         print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio GUI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_27732\\262375163.py:208: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n",
      "D:\\Disrupt Labs\\Rag Experiments\\env\\Lib\\site-packages\\gradio\\layouts\\column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://f355d4827c8ec27526.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f355d4827c8ec27526.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query Tokens: 3\n",
      "CONTEXT: Click here!\n",
      "https://m.youtube.com/watch?v=L0PKWTta7lU\n",
      "0 3\n",
      "0 5  S U P P O R T I N G  T H E  P S Y C H O S O C I A L  N E E D S  O F  T H E  P A T I E N T\n",
      "5.2.1 Maslow’s Hierarchy of Needs\n",
      "Figure 5.2\n",
      "1 5\n",
      "0 2  C O M M U N I C A T I O N  A N D  T E A M W O R K\n",
      "Conciseness: Business emails ought to be concise and direct. Use bullet points\n",
      "whenever it is practical.\n",
      "Title Heading: Always briefly describe the email's objective in the subject line. Don't\n",
      "reuse old threads for new topics; make a new subject line if the material differs.\n",
      "Greeting: Begin an email by addressing the recipient, much like you would in a letter\n",
      "(e.g., \"Dear Dr. X...\"). Include a greeting, even when communicating with just one's first\n",
      "name (e.g., \"Dear Ibrahim\").\n",
      "Language: Avoid using strong or emotive language in any business email.\n",
      "Emojis and Abbreviations: Avoid using text abbreviations and emojis in business\n",
      "emails.\n",
      "Forwarding: Please respect the author's privacy by not forwarding emails or\n",
      "0 7\n",
      "0 2  C O M M U N I C A T I O N  A N D  T E A M W O R K\n",
      "All handwritten and written content must be readable, except signatures, which are\n",
      "identifiable by their style. Serious medical errors and severe time wastage might result\n",
      "from illegible notes.\n",
      "Important Considerations \n",
      "Handwritten notes containing medicine names and dosages are subject to specific rules\n",
      "emphasizing the need for accuracy and clarity to prevent medication errors.\n",
      "Body Language\n",
      "Body language is the communication of ideas and messages through posture, gestures,\n",
      "and facial expressions. These nonverbal clues have the power to convey an utterly\n",
      "separate interpretation or to support the intended message. Observing our body\n",
      "language in social situations is crucial in mastering effective interpersonal interaction.\n",
      "support patient recovery and health.\n",
      "1 2\n",
      "0 4  H E A L T H  C A R E  F A C I L I T Y  S T R U C T U R E  A N D  E N V I R O N M E N T\n",
      "COMMUNICATION AND TEAMWORK\n",
      "CHAPTER 02:\n",
      "E x a m in e  c ritic a l e le m e n ts  a n d  b a rrie rs  o f e ffe c tiv e\n",
      "c o m m u n ic a tio n , ty p e s  o f v e rb a l a b u s e , p ro b le m a tic  w o rk p la c e\n",
      "b e h a v io rs , te a m w o rk , a n d  th e  u tiliz a tio n  o f c o m p u te rs  in  th e\n",
      "p e rio p e ra tiv e  e n v iro n m e n t.\n",
      "1 9\n",
      "0 2  C O M M U N I C A T I O N  A N D  T E A M W O R K\n",
      "Emotional Self-Consciousness: Family members may find it awkward to\n",
      "communicate their feelings to medical professionals. The healthcare provider must\n",
      "keep a professional distance from the situation and resist the need to become\n",
      "emotionally involved.\n",
      "Professional Discomfort: When managing the feelings and relationships of patients\n",
      "and their families, healthcare personnel, mainly those new to the sector, may\n",
      "experience uneasiness. This unease may cause one to say offensive or poorly worded\n",
      "things when conversing.\n",
      "Developing communication skills \n",
      "Communication Failure\n",
      "When learning to communicate with patients and their families, healthcare students\n",
      "0 6\n",
      "0 2  C O M M U N I C A T I O N  A N D  T E A M W O R K\n",
      "4.  Observe Nonverbal Cues: \n",
      "Pay attention to facial expressions, body language, and other nonverbal signals that can\n",
      "provide additional context to the message.\n",
      "5.  Seek Clarification: \n",
      "Ask for further explanation to ensure complete understanding when uncertain.\n",
      "6. Paraphrase the Content: \n",
      "Restate the sender’s message in your own words to confirm mutual understanding.\n",
      "7. Maintain Focus on the Topic:\n",
      "If the speaker strays from the subject, gently guide the conversation by asking relevant\n",
      "questions or referring to the original issue.\n",
      "8. Address Attention Lapses: \n",
      "If your mind wanders, ask the speaker to repeat the last point to regain focus.\n",
      "9. Avoid Assumptions:\n",
      "2 3\n",
      "T E R M I N O L O G Y\n",
      "Anesthesia Awareness: This rare phenomenon occurs when a fully unconscious\n",
      "patient under general anesthesia retains the sense of hearing and pain during surgery\n",
      "and can recall the experience.\n",
      "Anxiolytic: Medications that prevent anxiety, often administered preoperatively to\n",
      "highly anxious patients.\n",
      "Body Image: An individual's perception of their physical appearance in the eyes of\n",
      "others.\n",
      "Maslow’s Hierarchy of Human Needs: A model developed by psychologist Abraham\n",
      "Maslow outlining human needs in a hierarchical structure.\n",
      "Patient-Centered Care: Therapeutic care, communication, and intervention tailored\n",
      "to the unique needs of the patient and centered around those needs.\n",
      "Self-Actualization: The ability of a person to plan for and achieve their life goals.\n",
      "Understanding Human Needs5.2\n",
      "Healthcare workers often study the importance of basic human needs in patient care. Two\n",
      "key theories in understanding these needs are Maslow’s Hierarchy of Needs and Rogers’\n",
      "Patient-Centered Care.\n",
      "Developed by psychologist Abraham Maslow in the 1970s,\n",
      "this model depicts human needs in a hierarchical structure,\n",
      "typically visualized as a pyramid. The most basic\n",
      "physiological needs form the base, and higher-level needs\n",
      "like self-actualization are at the top. According to Maslow,\n",
      "basic needs must be met before higher-level needs can be\n",
      "achieved. This model is a valuable guide for patient care\n",
      "and helps prioritize patient needs. (see figure 5.2)\n",
      "Click here!\n",
      "https://m.youtube.com/watch?v=L0PKWTta7lU\n",
      "0 3\n",
      "Older Patients5.8\n",
      "1 5\n",
      "0 5  S U P P O R T I N G  T H E  P S Y C H O S O C I A L  N E E D S  O F  T H E  P A T I E N T\n",
      "Elderly patients are often subjected to stereotyping, such as being spoken to as if they were\n",
      "children. This includes using short sentences and high-pitched voices or addressing them\n",
      "with diminutives like \"sweetie.\" Such communication can lead to poor medical outcomes.\n",
      "Instead, speak to older patients as you would any adult, providing necessary clarifications.\n",
      "Avoid using clichés and simple reassurances.\n",
      "Do not refer to patients by diminutives; use their\n",
      "proper names.\n",
      "Do not assume cognitive impairment; the\n",
      "normal aging process does not include\n",
      "dementia.\n",
      "Support patients by orienting them to their\n",
      "environment and providing necessary\n",
      "information.\n"
     ]
    }
   ],
   "source": [
    "# system_message = f\"\"\"\n",
    "#     You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner. \n",
    "#     Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level \n",
    "#     and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "#     Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and \n",
    "#     offer further guidance if needed.\n",
    "#     \"\"\"\n",
    "\n",
    "# def gradio_interface(prompt,history =[]):\n",
    "#     output = process_user_query(prompt,history)\n",
    "#     history.append((prompt,output))\n",
    "#     return history\n",
    "\n",
    "# gr.Interface(fn=gradio_interface, inputs= ['text',\"state\"], outputs=[\"chatbot\",\"state\"]).launch(debug=True,share=True)\n",
    "    \n",
    "\n",
    "# ------------------------------------------- WORKING 1 -------------------------------------------\n",
    "\n",
    "# # Function to be used by Gradio for handling the query\n",
    "# def gradio_process(user_query):\n",
    "#     response = process_user_query(user_query, conversation_history)\n",
    "#     return response\n",
    "\n",
    "# # Create Gradio interface\n",
    "# interface = gr.Interface(fn=gradio_process, inputs=\"text\", outputs=\"text\", title=\"RAG-based Coaching System\")\n",
    "\n",
    "# # Launch Gradio app\n",
    "# interface.launch()\n",
    "# ------------------------------------------- WORKING 2 -------------------------------------------\n",
    "\n",
    "# Initialize empty conversation history (list of tuples)\n",
    "# conversation_history = []\n",
    "\n",
    "# def process_user_query(user_query: str, conversation_history: list):\n",
    "#     print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "#     # Generate embedding and get relevant context\n",
    "#     embedding = get_embedding(user_query)\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     context = \"\\n\".join(chunk['metadata'][\"text\"] for chunk in relevant_chunks)\n",
    "#     print(\"CONTEXT:\", context)\n",
    "\n",
    "#     # Format conversation history for the prompt\n",
    "#     history_str = \"\\n\".join(\n",
    "#         f\"User: {user}\\nCoach: {response}\" \n",
    "#         for user, response in conversation_history\n",
    "#     )\n",
    "\n",
    "#     # Create structured prompt\n",
    "#     prompt = f\"\"\"You are a knowledgeable and friendly coach. Follow these guidelines:\n",
    "#     1. Provide clear, step-by-step explanations\n",
    "#     2. Ask guiding questions to encourage critical thinking\n",
    "#     3. Adapt to the student's knowledge level\n",
    "#     4. Use examples from the provided context when relevant\n",
    "\n",
    "#     Context from learning materials:\n",
    "#     {context}\n",
    "\n",
    "#     Conversation history:\n",
    "#     {history_str}\n",
    "\n",
    "#     New student question:\n",
    "#     \"{user_query}\"\n",
    "\n",
    "#     Provide a helpful response:\"\"\"\n",
    "\n",
    "#     # Get LLM response\n",
    "#     groq_response = query_groq(prompt)\n",
    "#     print(f\"Response Tokens: {count_tokens(groq_response)}\")\n",
    "\n",
    "#     # Return updated history with new interaction\n",
    "#     return conversation_history + [(user_query, groq_response)]\n",
    "\n",
    "# # Gradio Interface\n",
    "# with gr.Blocks() as interface:\n",
    "#     gr.Markdown(\"# 🧑‍🏫 AI Coaching Assistant\")\n",
    "#     gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "    \n",
    "#     # State management\n",
    "#     chat_history = gr.State(conversation_history)\n",
    "    \n",
    "#     with gr.Row():\n",
    "#         chatbot = gr.Chatbot(height=500)\n",
    "#         with gr.Column(scale=0.5):\n",
    "#             context_display = gr.Textbox(label=\"Relevant Context\", interactive=False)\n",
    "\n",
    "#     user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "    \n",
    "#     with gr.Row():\n",
    "#         submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "#         undo_btn = gr.Button(\"Undo Last\")\n",
    "#         clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "#     def handle_submit(user_input, history):\n",
    "#         if not user_input.strip():\n",
    "#             return gr.update(), history, \"\"\n",
    "        \n",
    "#         # Process query and update history\n",
    "#         new_history = process_user_query(user_input, history)\n",
    "        \n",
    "#         # Get latest context for display\n",
    "#         latest_context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in query_pinecone(\n",
    "#             get_embedding(user_input)\n",
    "#         )][:3])  # Show top 3 context snippets\n",
    "        \n",
    "#         return \"\", new_history, latest_context\n",
    "\n",
    "#     # Component interactions\n",
    "#     submit_btn.click(\n",
    "#         handle_submit,\n",
    "#         [user_input, chat_history],\n",
    "#         [user_input, chat_history, context_display]\n",
    "#     ).then(\n",
    "#         lambda x: x,\n",
    "#         [chat_history],\n",
    "#         [chatbot]\n",
    "#     )\n",
    "\n",
    "#     undo_btn.click(\n",
    "#         lambda history: history[:-1] if history else [],\n",
    "#         [chat_history],\n",
    "#         [chat_history]\n",
    "#     ).then(\n",
    "#         lambda x: x,\n",
    "#         [chat_history],\n",
    "#         [chatbot]\n",
    "#     )\n",
    "\n",
    "#     clear_btn.click(\n",
    "#         lambda: [],\n",
    "#         None,\n",
    "#         [chat_history]\n",
    "#     ).then(\n",
    "#         lambda: ([], \"\"),\n",
    "#         None,\n",
    "#         [chatbot, context_display]\n",
    "#     )\n",
    "\n",
    "# interface.launch(share=True)\n",
    "# Just change the launch command to:\n",
    "#interface.launch(share=True, auth=(\"username\", \"password\"))  # Add basic auth\n",
    "\n",
    "\n",
    "# self hosting\n",
    "\n",
    "# # Run with:\n",
    "# interface.launch(\n",
    "#     server_name=\"0.0.0.0\",\n",
    "#     server_port=7860,\n",
    "#     show_error=True\n",
    "# )\n",
    "\n",
    "\n",
    "# ------------------------------------------- WORKING 3 Enter key submits user query -------------------------------------------\n",
    "# Initialize empty conversation history (list of tuples)\n",
    "conversation_history = []\n",
    "\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk['metadata'][\"text\"] for chunk in relevant_chunks)\n",
    "    print(\"CONTEXT:\", context)\n",
    "\n",
    "    # Format conversation history for the prompt\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\" \n",
    "        for user, response in conversation_history\n",
    "    )\n",
    "\n",
    "    # Create structured prompt\n",
    "    prompt = f\"\"\"You are a knowledgeable and friendly coach. Follow these guidelines:\n",
    "    1. Provide clear, step-by-step explanations\n",
    "    2. Ask guiding questions to encourage critical thinking\n",
    "    3. Adapt to the student's knowledge level\n",
    "    4. Use examples from the provided context when relevant\n",
    "\n",
    "    Context from learning materials:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "\n",
    "    New student question:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    Provide a helpful response:\"\"\"\n",
    "\n",
    "    # Get LLM response\n",
    "    groq_response = query_groq(prompt)\n",
    "    print(f\"Response Tokens: {count_tokens(groq_response)}\")\n",
    "\n",
    "    # Return updated history with new interaction\n",
    "    return conversation_history + [(user_query, groq_response)]\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"# 🧑‍🏫 AI Coaching Assistant\")\n",
    "    gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "    \n",
    "    # State management\n",
    "    chat_history = gr.State(conversation_history)\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500)\n",
    "        with gr.Column(scale=0.5):\n",
    "            context_display = gr.Textbox(label=\"Relevant Context\", interactive=False)\n",
    "\n",
    "    user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        undo_btn = gr.Button(\"Undo Last\")\n",
    "        clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "    def handle_submit(user_input, history):\n",
    "        if not user_input.strip():\n",
    "            return gr.update(), history, \"\"\n",
    "        \n",
    "        # Process query and update history\n",
    "        new_history = process_user_query(user_input, history)\n",
    "        \n",
    "        # Get latest context for display\n",
    "        latest_context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in query_pinecone(\n",
    "            get_embedding(user_input)\n",
    "        )][:3])  # Show top 3 context snippets\n",
    "        \n",
    "        return \"\", new_history, latest_context\n",
    "\n",
    "    # Component interactions\n",
    "    submit_btn.click(\n",
    "        handle_submit,\n",
    "        [user_input, chat_history],\n",
    "        [user_input, chat_history, context_display]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "    \n",
    "    # Add submit on Enter key press\n",
    "    user_input.submit(\n",
    "        handle_submit,\n",
    "        [user_input, chat_history],\n",
    "        [user_input, chat_history, context_display]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    undo_btn.click(\n",
    "        lambda history: history[:-1] if history else [],\n",
    "        [chat_history],\n",
    "        [chat_history]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        lambda: [],\n",
    "        None,\n",
    "        [chat_history]\n",
    "    ).then(\n",
    "        lambda: ([], \"\"),\n",
    "        None,\n",
    "        [chatbot, context_display]\n",
    "    )\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
