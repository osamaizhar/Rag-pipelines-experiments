{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits (for creating and testing excel and ppt combined parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API pcsk_4bLR9o_3crxHE9zjHW76VdRnBPi2Xo794pQnKSifnRfQ9iQc6U3iqeqeyVEZ3RjBPYtoD4\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "from pinecone import (\n",
    "    Pinecone,\n",
    "    ServerlessSpec,\n",
    "    CloudProvider,\n",
    "    AwsRegion,\n",
    "    VectorType\n",
    ")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import tkinter as tk\n",
    "import glob\n",
    "\n",
    "\n",
    "# Important: Import pinecone-client properly\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"PINECONE_API\", PINECONE_API)\n",
    "\n",
    "\n",
    "# Groq API settings\n",
    "GROQ_EMBED_URL = \"https://api.groq.com/openai/v1/embeddings\"\n",
    "GROQ_CHAT_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "LLM_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "\n",
    "# documents = pdf_load_documents()\n",
    "# print(documents)\n",
    "\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "#     \"\"\"Extract text from a PDF file.\"\"\"\n",
    "#     with open(pdf_path, 'r') as file:\n",
    "#         pdf_reader = PyPDF2.PdfReader(file)\n",
    "#         text = \"\"\n",
    "#         for page_num in range(len(pdf_reader.pages)):\n",
    "#             page = pdf_reader.pages[page_num]\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "\n",
    "\n",
    "# #     return text\n",
    "# extract_text_from_pdf(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    # Find all .xlsx files in the directory\n",
    "    excel_files = glob.glob(os.path.join(DATA_PATH, \"*.xlsx\"))\n",
    "\n",
    "    if not excel_files:\n",
    "        raise FileNotFoundError(f\"No Excel files found in {DATA_PATH}\")\n",
    "\n",
    "    # Read all Excel files into a list of DataFrames\n",
    "    documents = [pd.read_excel(file) for file in excel_files]\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Load documents\n",
    "documents = load_documents()\n",
    "\n",
    "# Display first row of the first file\n",
    "print(documents[0].head()) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitting \\ Chunking using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=80,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False  # considers separators like '\\n\\n'if true\n",
    "    )\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "\n",
    "# chunks = split_documents(documents)\n",
    "# chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "print(PINECONE_API)\n",
    "\n",
    "\n",
    "#  --------------- initialize pinecone -----------------------------\n",
    "# pc.create_index_for_model(\n",
    "#     name=\"test-index\",\n",
    "#     cloud=\"aws\",\n",
    "#     region=\"us-east-1\",\n",
    "#     embed={\n",
    "#         \"model\":\"llama-text-embed-v2\",\n",
    "#         \"field_map\":{\"text\": \"page_content\"}\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What:\n",
    "**Use Upsert:**\n",
    "\n",
    "When you're adding new vectors or want to replace existing vectors with new data (including changing the vector values).\n",
    "When you need to add a completely new document or vector.\n",
    "When you want to update both the vector values and metadata.\n",
    "\n",
    "**Use Update:**\n",
    "\n",
    "When you're only modifying the metadata of an existing vector.\n",
    "When the vector values (embeddings) themselves are correct and only extra information like text, author, or document-related metadata needs to be updated.\n",
    "Summary:\n",
    "Upsert: Adds or replaces both the vector values and metadata. Use when inserting or completely replacing data.\n",
    "Update: Modifies the metadata without changing the vector values. Use when the vectors are correct, but metadata needs an update.\n",
    "For your case, if you just want to add or update the page_content or any other metadata for existing vectors, use update. If you want to re-upload vectors with new embeddings or metadata, use upsert.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings Via AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en'  and Upsert each to Pinecone one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(\"test-index\")\n",
    "\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained(\n",
    "    'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "# user_query = \"user query\"\n",
    "# Function to generate embeddings without tokenization\n",
    "\n",
    "\n",
    "def get_embedding(data):\n",
    "    embeddings = embedding_model.encode(data).tolist()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def upsert_chunks_to_pinecone(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get the embedding for the chunk\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "\n",
    "        # Extract metadata and add text as part of the metadata\n",
    "        metadata = chunk.metadata\n",
    "        metadata[\"text\"] = chunk.page_content  # Store text in metadata\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Upsert the embedding along with its metadata\n",
    "        index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "\n",
    "        print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"All {count-1} Embeddings have been upserted to Pinecone\")\n",
    "\n",
    "# upsert_chunks_to_pinecone(index, chunks)\n",
    "\n",
    "# query_embeddings = embedding_model.encode(user_query).tolist()\n",
    "# query_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Vectors Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pinecone_chunks(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get updated embedding\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "\n",
    "        # Extract metadata and page content\n",
    "        metadata = chunk.metadata\n",
    "        text = chunk.page_content\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Update the embedding and metadata\n",
    "        index.update(id=vector_id, values=embedding, set_metadata=metadata)\n",
    "\n",
    "        print(f\"Embedding {count} updated in Pinecone with new metadata\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"All {count-1} embeddings have been updated in Pinecone\")\n",
    "\n",
    "# update_pinecone_chunks(index, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your application is designed to answer a wide range of student queries and suggest relevant material, you want to retrieve enough content to cover different facets of a topic without overwhelming the LLM with too much information.\n",
    "\n",
    "# Starting Point:\n",
    "- A common starting point is to set top_k between **5 and 10.**\n",
    "- **top_k=5:** This can work well if your curated content is highly relevant and precise, ensuring that the top 5 matches are very close to the query.\n",
    "-  **top_k=10:** If you want the coach to consider a broader range of content—perhaps to provide diverse perspectives or cover a topic more comprehensively—increasing top_k to around 10 might be beneficial.\n",
    "\n",
    "# Experiment and Adjust:\n",
    "- The “best” value depends on factors such as the diversity of your content, how densely your data covers the topics, and the quality of the embedding matches. It’s a good idea to experiment with different top_k values and evaluate the quality and relevance of the responses in your specific\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=5, include_metadata=True)\n",
    "    return result['matches']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Groq LLM\n",
    "def query_groq(prompt: str) -> str:\n",
    "    response = requests.post(\n",
    "        GROQ_CHAT_URL,\n",
    "        headers=GROQ_HEADERS,\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 8192  # max from groq website\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error querying Groq: {response.text}\")\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to handle user query\n",
    "# def process_user_query(user_query: str):\n",
    "#     print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "#     # Step 1: Generate embedding for the user query\n",
    "#     embedding = get_embedding(user_query)\n",
    "\n",
    "#     # Step 2: Query Pinecone for relevant chunks\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     print(f\"Relevant Chunks : {relevant_chunks[0]}\")\n",
    "#     # Step 3: Prepare the content for the Groq LLM\n",
    "#     context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in relevant_chunks])\n",
    "#     print(\"------------------------------------ Context ------------------------------------------ : \", context)\n",
    "#     # Step 4: Send the retrieved content as the prompt to Groq LLM\n",
    "#     groq_response = query_groq(context)\n",
    "#     print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "#     return groq_response\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     user_query = \"What are the Link Layer?\"\n",
    "#     response = process_user_query(user_query)\n",
    "#     print(response)\n",
    "\n",
    "# def process_user_query(user_query: str):\n",
    "#     print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "\n",
    "#     # Step 1: Generate embedding for the user query\n",
    "#     embedding = get_embedding(user_query)\n",
    "\n",
    "#     # Step 2: Query Pinecone for relevant chunks\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     #print(f\"Relevant Chunks : {relevant_chunks}\")\n",
    "\n",
    "#     # Step 3: Prepare the content (context) for the LLM\n",
    "#     #context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in relevant_chunks])\n",
    "#     #print(\"------------------------------------ Context ------------------------------------------ : \", context)\n",
    "\n",
    "#     # Step 4: Craft a good coach prompt for the LLM\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner.\n",
    "#     Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level\n",
    "#     and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "#     Context from the student's material:\n",
    "#     {relevant_chunks}\n",
    "\n",
    "#     The student has asked the following question:\n",
    "#     \"{user_query}\"\n",
    "\n",
    "#     Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and\n",
    "#     offer further guidance if needed.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Step 5: Send the prepared prompt (with context and user query) to the LLM\n",
    "#     groq_response = query_groq(prompt)\n",
    "#     print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "\n",
    "#     return groq_response\n",
    "\n",
    "\n",
    "# # # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     while True:\n",
    "#         user_query = input(\"Enter your query or press 0 to exit: \")\n",
    "#         if user_query == \"0\":\n",
    "#             break\n",
    "#         response = process_user_query(user_query)\n",
    "#         print(response)\n",
    "\n",
    "\n",
    "# Initialize an empty list to store conversation history\n",
    "\n",
    "\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    conversation_history = []\n",
    "\n",
    "    print(f\"User Query Tokens : {count_tokens(user_query)}\")\n",
    "\n",
    "    # Step 1: Generate embedding for the user query\n",
    "    embedding = get_embedding(user_query)\n",
    "\n",
    "    # Step 2: Query Pinecone for relevant chunks\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "\n",
    "    # Prepare the context from relevant chunks\n",
    "    context = \"\\n\".join([chunk['metadata'][\"text\"]\n",
    "                        for chunk in relevant_chunks])\n",
    "    print(\"CONTEXT: \", context)\n",
    "\n",
    "    # Step 3: Combine conversation history with current user query\n",
    "    conversation_history_str = \"\\n\".join(conversation_history)\n",
    "\n",
    "    # Step 4: Craft a good coach prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner. \n",
    "    Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level \n",
    "    and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "    Context from the student's material:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {conversation_history_str}\n",
    "\n",
    "    The student has asked the following question:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and \n",
    "    offer further guidance if needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 5: Send the prepared prompt (with context and user query) to the LLM\n",
    "    groq_response = query_groq(prompt)\n",
    "    print(f\"Groq Response Tokens : {count_tokens(groq_response)}\")\n",
    "\n",
    "    # Step 6: Append the user query and model's response to conversation history\n",
    "    conversation_history.append(f\"User: {user_query}\")\n",
    "    conversation_history.append(f\"Coach: {groq_response}\")\n",
    "\n",
    "    return groq_response\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        print(\"------------------------------------------------------------------------------------------------------------------------\")\n",
    "        user_query = input(\"Enter your query or press 0 to exit: \")\n",
    "        if user_query == \"0\":\n",
    "            break\n",
    "        response = process_user_query(user_query, conversation_history)\n",
    "        print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streamlit Interface (Too heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate function to handle the Streamlit interface\n",
    "# def run_streamlit_app():\n",
    "#     st.title(\"AI Student Coach\")\n",
    "#     st.write(\"Ask your question to the AI student coach, and it will provide a thoughtful response based on your learning material.\")\n",
    "\n",
    "#     # Chatbox: User's input\n",
    "#     user_input = st.text_input(\"Enter your question:\", \"\")\n",
    "\n",
    "#     # Button to submit the query\n",
    "#     if st.button(\"Submit\"):\n",
    "#         if user_input:\n",
    "#             with st.spinner('Processing your query...'):\n",
    "#                 # Call the function to process user query\n",
    "#                 response = handle_user_query(user_input)\n",
    "#                 # Display the response from the LLM\n",
    "#                 st.markdown(f\"### AI Response:\")\n",
    "#                 st.write(response)\n",
    "#         else:\n",
    "#             st.warning(\"Please enter a question before submitting.\")\n",
    "\n",
    "\n",
    "# # Separate function to handle user query and call process_user_query\n",
    "# def handle_user_query(user_query: str):\n",
    "#     try:\n",
    "#         # Process the user query using the relevant function\n",
    "#         response = process_user_query(user_query)\n",
    "#         return response\n",
    "#     except Exception as e:\n",
    "#         # Handle any errors that occur during query processing\n",
    "#         st.error(f\"An error occurred: {str(e)}\")\n",
    "#         return \"Sorry, something went wrong.\"\n",
    "\n",
    "\n",
    "# # Main entry point for the app\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"Running Streamlit app...\")\n",
    "#     run_streamlit_app()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to fix it:**\n",
    "To run Streamlit properly, you need to execute it with the streamlit run command from the terminal:\n",
    "\n",
    "- Open your terminal (or command prompt).\n",
    "- Navigate to the folder where your Streamlit script is located.\n",
    "\n",
    "**Run the following command:**\n",
    "- streamlit run your_script.py\n",
    "\n",
    "- Replace your_script.py with the name of your Python file containing the Streamlit code.\n",
    "\n",
    "**For example:**\n",
    "- streamlit run app.py\n",
    "\n",
    "Once you run Streamlit in this way, the warning should disappear, and the app will launch in your browser, providing full functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tkinter as tk\n",
    "# from tkinter import scrolledtext\n",
    "\n",
    "# # Function to process user query (replace this with your actual implementation)\n",
    "# # def process_user_query(query):\n",
    "# #     # Example: Simulate actual processing of the query\n",
    "# #     response = process_user_query(query)\n",
    "# #     #print(response)\n",
    "# #     return response\n",
    "\n",
    "# # Function to handle the submit button click\n",
    "# def handle_user_query():\n",
    "#     user_input = query_entry.get(\"1.0\", \"end-1c\").strip()\n",
    "\n",
    "#     if user_input:\n",
    "#         # Update status\n",
    "#         status_label.config(text=\"Processing...\")\n",
    "#         submit_button.config(state=tk.DISABLED)\n",
    "\n",
    "#         # Clear previous results\n",
    "#         result_box.config(state=tk.NORMAL)\n",
    "#         result_box.delete(1.0, tk.END)\n",
    "\n",
    "#         # Process the query\n",
    "#         response = process_user_query(user_input)  # Correctly call process_user_query\n",
    "\n",
    "#         # Display the response\n",
    "#         result_box.insert(tk.END, response)\n",
    "#         result_box.config(state=tk.DISABLED)\n",
    "\n",
    "#         # Reset status\n",
    "#         status_label.config(text=\"Ready\")\n",
    "#         submit_button.config(state=tk.NORMAL)\n",
    "#     else:\n",
    "#         result_box.config(state=tk.NORMAL)\n",
    "#         result_box.delete(1.0, tk.END)\n",
    "#         result_box.insert(tk.END, \"Please enter a query.\")\n",
    "#         result_box.config(state=tk.DISABLED)\n",
    "\n",
    "# # Set up the main window\n",
    "# window = tk.Tk()\n",
    "# window.title(\"AI Coach\")\n",
    "# window.geometry(\"600x500\")\n",
    "# window.configure(bg=\"#f5f5f5\")\n",
    "\n",
    "# # Create padding frame\n",
    "# main_frame = tk.Frame(window, bg=\"#f5f5f5\")\n",
    "# main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=15)\n",
    "\n",
    "# # Title label\n",
    "# title_label = tk.Label(main_frame, text=\"AI Coach\", font=(\"Arial\", 16, \"bold\"), bg=\"#f5f5f5\")\n",
    "# title_label.pack(pady=(0, 15))\n",
    "\n",
    "# # Query section\n",
    "# query_frame = tk.Frame(main_frame, bg=\"#f5f5f5\")\n",
    "# query_frame.pack(fill=tk.X, pady=5)\n",
    "\n",
    "# query_label = tk.Label(query_frame, text=\"Your question:\", font=(\"Arial\", 11), bg=\"#f5f5f5\", anchor=\"w\")\n",
    "# query_label.pack(fill=tk.X)\n",
    "\n",
    "# query_entry = scrolledtext.ScrolledText(main_frame, height=4, wrap=tk.WORD, font=(\"Arial\", 11))\n",
    "# query_entry.pack(fill=tk.X, pady=5)\n",
    "# query_entry.focus_set()\n",
    "\n",
    "# # Button frame\n",
    "# button_frame = tk.Frame(main_frame, bg=\"#f5f5f5\")\n",
    "# button_frame.pack(fill=tk.X, pady=10)\n",
    "\n",
    "# # Submit button\n",
    "# submit_button = tk.Button(button_frame, text=\"Ask Coach\", font=(\"Arial\", 11),\n",
    "#                          bg=\"#4a86e8\", fg=\"white\", padx=15, pady=8,\n",
    "#                          command=handle_user_query)\n",
    "# submit_button.pack(side=tk.RIGHT)\n",
    "\n",
    "# # Status label\n",
    "# status_label = tk.Label(button_frame, text=\"Ready\", font=(\"Arial\", 10), fg=\"#555555\", bg=\"#f5f5f5\")\n",
    "# status_label.pack(side=tk.LEFT, pady=10)\n",
    "\n",
    "# # Response section\n",
    "# response_label = tk.Label(main_frame, text=\"AI Coach's Response:\", font=(\"Arial\", 11), bg=\"#f5f5f5\", anchor=\"w\")\n",
    "# response_label.pack(fill=tk.X, pady=(10, 5))\n",
    "\n",
    "# result_box = scrolledtext.ScrolledText(main_frame, height=10, wrap=tk.WORD, font=(\"Arial\", 11), state=tk.DISABLED)\n",
    "# result_box.pack(fill=tk.BOTH, expand=True)\n",
    "\n",
    "# # Start the GUI event loop\n",
    "# window.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
