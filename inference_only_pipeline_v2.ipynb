{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# from typing import List, Tuple\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "# PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_CHAT_URL = os.getenv(\"GROQ_CHAT_URL\")\n",
    "\n",
    "NVIDIA_API = os.getenv(\"NVIDIA_API\")\n",
    "NVIDIA_BASE_URL = os.getenv(\"NVIDIA_BASE_URL\")\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "# LLM_MODEL = \"llama3-70b-8192\"\n",
    "LLM_MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "\n",
    "# NVidia Embedding import\n",
    "client = OpenAI(\n",
    "    api_key=NVIDIA_API,\n",
    "    base_url=NVIDIA_BASE_URL,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    - Context window: 128K\n",
    "Ouput:\n",
    "    - Output Max Tokens: 32,768\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"[Time Tracker] `{func.__name__}` took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# # EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "\n",
    "# vo = voyageai.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `get_embedding` took 0.6691 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.01506805419921875,\n",
       " -0.01165008544921875,\n",
       " 0.02081298828125,\n",
       " 0.0252227783203125,\n",
       " -0.0198211669921875,\n",
       " 0.011077880859375,\n",
       " 0.00782012939453125,\n",
       " 0.037872314453125,\n",
       " -0.00841522216796875,\n",
       " 0.00612640380859375,\n",
       " -0.0089569091796875,\n",
       " 0.01071929931640625,\n",
       " 0.00724029541015625,\n",
       " 0.0088653564453125,\n",
       " 0.004428863525390625,\n",
       " -0.00255584716796875,\n",
       " 0.0171356201171875,\n",
       " -0.0015773773193359375,\n",
       " 0.0038318634033203125,\n",
       " -0.01238250732421875,\n",
       " 0.020904541015625,\n",
       " -0.00855255126953125,\n",
       " 0.0176849365234375,\n",
       " 0.0019550323486328125,\n",
       " -0.0012798309326171875,\n",
       " 0.0123748779296875,\n",
       " -0.001995086669921875,\n",
       " 0.006771087646484375,\n",
       " 0.032135009765625,\n",
       " -0.00537872314453125,\n",
       " -0.02215576171875,\n",
       " 0.007434844970703125,\n",
       " -0.014984130859375,\n",
       " -0.0027523040771484375,\n",
       " -0.008514404296875,\n",
       " -0.00858306884765625,\n",
       " -0.005031585693359375,\n",
       " -0.00081634521484375,\n",
       " -0.010833740234375,\n",
       " -0.0025196075439453125,\n",
       " 0.008209228515625,\n",
       " -0.0005240440368652344,\n",
       " 0.017333984375,\n",
       " -0.0037689208984375,\n",
       " -0.0098876953125,\n",
       " 0.01116180419921875,\n",
       " -0.0036411285400390625,\n",
       " -0.0017528533935546875,\n",
       " 0.0169830322265625,\n",
       " 0.01062774658203125,\n",
       " 0.0003657341003417969,\n",
       " -0.021820068359375,\n",
       " 0.01107025146484375,\n",
       " -0.103271484375,\n",
       " -0.01129150390625,\n",
       " -0.022674560546875,\n",
       " -0.005260467529296875,\n",
       " 0.008209228515625,\n",
       " -0.03521728515625,\n",
       " -0.01702880859375,\n",
       " -0.0012693405151367188,\n",
       " 0.007175445556640625,\n",
       " -0.006317138671875,\n",
       " -0.01294708251953125,\n",
       " 0.0135650634765625,\n",
       " 0.0018339157104492188,\n",
       " -0.0109710693359375,\n",
       " -0.007625579833984375,\n",
       " -0.0169830322265625,\n",
       " 0.0176239013671875,\n",
       " -0.00629425048828125,\n",
       " -0.0300445556640625,\n",
       " 0.00615692138671875,\n",
       " 0.00335693359375,\n",
       " -0.0157012939453125,\n",
       " 0.000644683837890625,\n",
       " -0.03851318359375,\n",
       " 0.0240020751953125,\n",
       " 0.0211181640625,\n",
       " -0.0008244514465332031,\n",
       " -0.032684326171875,\n",
       " -0.01122283935546875,\n",
       " -0.007732391357421875,\n",
       " 0.008575439453125,\n",
       " 0.00579833984375,\n",
       " 0.0247039794921875,\n",
       " -0.0001842975616455078,\n",
       " -0.005908966064453125,\n",
       " 0.0174102783203125,\n",
       " 0.0187225341796875,\n",
       " 0.0011835098266601562,\n",
       " 0.0246734619140625,\n",
       " -0.0009417533874511719,\n",
       " 0.02978515625,\n",
       " 0.006885528564453125,\n",
       " 0.00597381591796875,\n",
       " -0.00040602684020996094,\n",
       " 0.01195526123046875,\n",
       " -0.01139068603515625,\n",
       " 0.0009784698486328125,\n",
       " -0.0105743408203125,\n",
       " -0.0143890380859375,\n",
       " -0.0020809173583984375,\n",
       " -0.0037364959716796875,\n",
       " -0.00437164306640625,\n",
       " -0.0008025169372558594,\n",
       " 0.014556884765625,\n",
       " 0.0117034912109375,\n",
       " 0.0070648193359375,\n",
       " -0.01019287109375,\n",
       " 0.0102996826171875,\n",
       " -0.0238037109375,\n",
       " -0.0255279541015625,\n",
       " 0.0179443359375,\n",
       " -0.01837158203125,\n",
       " -0.004062652587890625,\n",
       " 0.00341796875,\n",
       " -0.0250701904296875,\n",
       " 0.00441741943359375,\n",
       " -0.019378662109375,\n",
       " -0.00856781005859375,\n",
       " -0.004276275634765625,\n",
       " -0.0050811767578125,\n",
       " 0.01284027099609375,\n",
       " 0.00981903076171875,\n",
       " -0.0002455711364746094,\n",
       " -0.0214691162109375,\n",
       " -0.00434112548828125,\n",
       " -0.0035343170166015625,\n",
       " 0.00196075439453125,\n",
       " 0.0193634033203125,\n",
       " 0.0303955078125,\n",
       " 0.044769287109375,\n",
       " -0.012237548828125,\n",
       " 0.002094268798828125,\n",
       " -0.004169464111328125,\n",
       " -0.007598876953125,\n",
       " -0.007293701171875,\n",
       " 0.01092529296875,\n",
       " -0.015960693359375,\n",
       " -0.002796173095703125,\n",
       " -0.016021728515625,\n",
       " 0.00608062744140625,\n",
       " 0.0026683807373046875,\n",
       " -0.0002894401550292969,\n",
       " 0.017791748046875,\n",
       " -0.00847625732421875,\n",
       " -0.01273345947265625,\n",
       " -0.0008955001831054688,\n",
       " 0.034515380859375,\n",
       " 0.0057373046875,\n",
       " 0.01702880859375,\n",
       " 0.005252838134765625,\n",
       " 0.00379180908203125,\n",
       " 0.0017881393432617188,\n",
       " -0.004810333251953125,\n",
       " -0.004985809326171875,\n",
       " -0.004123687744140625,\n",
       " -0.0182037353515625,\n",
       " -0.0023097991943359375,\n",
       " -0.006732940673828125,\n",
       " 0.005420684814453125,\n",
       " 0.024017333984375,\n",
       " 0.00600433349609375,\n",
       " -0.0008244514465332031,\n",
       " 0.01294708251953125,\n",
       " -0.003753662109375,\n",
       " 0.030242919921875,\n",
       " 0.004795074462890625,\n",
       " -0.00930023193359375,\n",
       " 0.018951416015625,\n",
       " -0.0168914794921875,\n",
       " 0.0045166015625,\n",
       " -0.0105133056640625,\n",
       " 0.01149749755859375,\n",
       " 0.001010894775390625,\n",
       " 0.0028228759765625,\n",
       " 0.006175994873046875,\n",
       " -0.01129150390625,\n",
       " -0.013214111328125,\n",
       " -0.0259857177734375,\n",
       " 0.006465911865234375,\n",
       " -0.00434112548828125,\n",
       " -0.00821685791015625,\n",
       " 9.28044319152832e-05,\n",
       " 0.008880615234375,\n",
       " -0.0012722015380859375,\n",
       " 0.00719451904296875,\n",
       " 0.00608062744140625,\n",
       " 0.0184173583984375,\n",
       " 0.0098724365234375,\n",
       " -0.019866943359375,\n",
       " 0.005306243896484375,\n",
       " 0.01430511474609375,\n",
       " 0.0108795166015625,\n",
       " 0.00539398193359375,\n",
       " -0.01517486572265625,\n",
       " -0.018524169921875,\n",
       " -0.0177764892578125,\n",
       " -0.00502777099609375,\n",
       " -0.018646240234375,\n",
       " -0.026336669921875,\n",
       " 0.0197906494140625,\n",
       " -0.017822265625,\n",
       " 0.007740020751953125,\n",
       " 0.029998779296875,\n",
       " 0.0102996826171875,\n",
       " -0.0145416259765625,\n",
       " 0.021636962890625,\n",
       " -0.0027256011962890625,\n",
       " -0.001659393310546875,\n",
       " 0.01142120361328125,\n",
       " 0.003170013427734375,\n",
       " 0.008941650390625,\n",
       " -0.01117706298828125,\n",
       " 0.01065826416015625,\n",
       " -0.021636962890625,\n",
       " -0.0093536376953125,\n",
       " 0.0003330707550048828,\n",
       " 0.0091094970703125,\n",
       " 0.0293426513671875,\n",
       " 0.01084136962890625,\n",
       " -0.008087158203125,\n",
       " -0.005985260009765625,\n",
       " 0.0060272216796875,\n",
       " -0.0012559890747070312,\n",
       " 0.02154541015625,\n",
       " 0.021820068359375,\n",
       " -0.01100921630859375,\n",
       " -0.00843048095703125,\n",
       " -0.0201416015625,\n",
       " -0.01309967041015625,\n",
       " -0.00029659271240234375,\n",
       " 0.0108184814453125,\n",
       " 0.004238128662109375,\n",
       " -0.01476287841796875,\n",
       " -0.0006866455078125,\n",
       " -0.0076141357421875,\n",
       " 0.023101806640625,\n",
       " 0.01227569580078125,\n",
       " 0.0079193115234375,\n",
       " -0.0003056526184082031,\n",
       " 0.01467132568359375,\n",
       " -0.0010538101196289062,\n",
       " -0.0036487579345703125,\n",
       " 0.00019657611846923828,\n",
       " 0.01242828369140625,\n",
       " -0.01137542724609375,\n",
       " -0.030426025390625,\n",
       " 0.0093231201171875,\n",
       " -0.0003521442413330078,\n",
       " -2.181529998779297e-05,\n",
       " -0.00957489013671875,\n",
       " 0.01288604736328125,\n",
       " -0.00524139404296875,\n",
       " 0.017913818359375,\n",
       " 0.023712158203125,\n",
       " 0.01107025146484375,\n",
       " 0.034271240234375,\n",
       " 0.00392913818359375,\n",
       " -0.0052337646484375,\n",
       " -0.00540924072265625,\n",
       " 0.0010805130004882812,\n",
       " 0.023590087890625,\n",
       " -0.0012559890747070312,\n",
       " 0.027313232421875,\n",
       " -0.01268768310546875,\n",
       " 0.008087158203125,\n",
       " -0.0011892318725585938,\n",
       " 0.0173797607421875,\n",
       " -0.009185791015625,\n",
       " -0.018646240234375,\n",
       " 0.0007028579711914062,\n",
       " 0.00644683837890625,\n",
       " 0.024566650390625,\n",
       " -0.011505126953125,\n",
       " -0.0031147003173828125,\n",
       " 0.01580810546875,\n",
       " 0.00630950927734375,\n",
       " -0.01435089111328125,\n",
       " -0.01263427734375,\n",
       " -0.003208160400390625,\n",
       " 0.0005211830139160156,\n",
       " -0.00861358642578125,\n",
       " -0.00782012939453125,\n",
       " 0.004611968994140625,\n",
       " 0.0116729736328125,\n",
       " -0.0288238525390625,\n",
       " -0.0238037109375,\n",
       " -0.010009765625,\n",
       " 0.0040283203125,\n",
       " -0.0078582763671875,\n",
       " 0.0049285888671875,\n",
       " -0.01303863525390625,\n",
       " -0.018341064453125,\n",
       " -0.01541900634765625,\n",
       " 0.006443023681640625,\n",
       " 9.351968765258789e-05,\n",
       " 0.012237548828125,\n",
       " -0.00968170166015625,\n",
       " 0.0166168212890625,\n",
       " 0.0155181884765625,\n",
       " 0.00566864013671875,\n",
       " -0.02288818359375,\n",
       " -0.005344390869140625,\n",
       " 0.00896453857421875,\n",
       " -0.01139068603515625,\n",
       " 0.0015802383422851562,\n",
       " 0.0222930908203125,\n",
       " 0.0096435546875,\n",
       " 0.024627685546875,\n",
       " -0.005458831787109375,\n",
       " -0.0025043487548828125,\n",
       " -0.0115509033203125,\n",
       " -0.03094482421875,\n",
       " 0.00821685791015625,\n",
       " 0.0338134765625,\n",
       " -0.01172637939453125,\n",
       " -0.0012416839599609375,\n",
       " -0.0009636878967285156,\n",
       " -0.0230865478515625,\n",
       " 0.006809234619140625,\n",
       " 0.006626129150390625,\n",
       " -0.020172119140625,\n",
       " 0.01480865478515625,\n",
       " -0.00566864013671875,\n",
       " 0.006549835205078125,\n",
       " 0.0095672607421875,\n",
       " 0.004421234130859375,\n",
       " -0.00695037841796875,\n",
       " -0.0089111328125,\n",
       " -0.006168365478515625,\n",
       " 0.0179901123046875,\n",
       " 0.0113983154296875,\n",
       " 0.03564453125,\n",
       " 0.00598907470703125,\n",
       " 0.01529693603515625,\n",
       " -0.00519561767578125,\n",
       " 0.0085296630859375,\n",
       " 0.0034008026123046875,\n",
       " 0.005672454833984375,\n",
       " 0.01169586181640625,\n",
       " 0.0035877227783203125,\n",
       " -0.01476287841796875,\n",
       " 0.02301025390625,\n",
       " 0.0147857666015625,\n",
       " -0.00545501708984375,\n",
       " -0.0179290771484375,\n",
       " 0.01242828369140625,\n",
       " 0.0001773834228515625,\n",
       " -0.00923919677734375,\n",
       " 0.01258087158203125,\n",
       " -0.01436614990234375,\n",
       " -0.01525115966796875,\n",
       " -0.0109710693359375,\n",
       " 0.016265869140625,\n",
       " -0.00983428955078125,\n",
       " 0.01041412353515625,\n",
       " -0.00838470458984375,\n",
       " 0.01397705078125,\n",
       " -0.0303497314453125,\n",
       " -0.0016107559204101562,\n",
       " -0.0066986083984375,\n",
       " 0.00504302978515625,\n",
       " 0.0033130645751953125,\n",
       " -0.009674072265625,\n",
       " -0.020477294921875,\n",
       " -0.01009368896484375,\n",
       " -0.0007944107055664062,\n",
       " -0.00608062744140625,\n",
       " -0.0216827392578125,\n",
       " -0.00688934326171875,\n",
       " 0.00585174560546875,\n",
       " -0.00548553466796875,\n",
       " 0.001861572265625,\n",
       " 0.004467010498046875,\n",
       " -0.00514984130859375,\n",
       " 0.007076263427734375,\n",
       " -0.015289306640625,\n",
       " 0.0013952255249023438,\n",
       " -0.0224456787109375,\n",
       " -0.0010385513305664062,\n",
       " 0.00615692138671875,\n",
       " -0.0032405853271484375,\n",
       " 0.0203704833984375,\n",
       " 0.0217437744140625,\n",
       " -0.009613037109375,\n",
       " -0.015899658203125,\n",
       " -0.0322265625,\n",
       " -0.00856781005859375,\n",
       " 0.0160980224609375,\n",
       " -0.01300811767578125,\n",
       " -0.01430511474609375,\n",
       " 0.0081329345703125,\n",
       " -0.01309967041015625,\n",
       " -0.01284027099609375,\n",
       " 0.01104736328125,\n",
       " -0.0130615234375,\n",
       " -0.0215606689453125,\n",
       " 0.0006594657897949219,\n",
       " 0.0214080810546875,\n",
       " -0.0281219482421875,\n",
       " -0.0121002197265625,\n",
       " -0.004840850830078125,\n",
       " -0.00972747802734375,\n",
       " 0.0269012451171875,\n",
       " 0.01197052001953125,\n",
       " 0.00472259521484375,\n",
       " -0.011474609375,\n",
       " -0.01506805419921875,\n",
       " -0.0006003379821777344,\n",
       " -0.0020580291748046875,\n",
       " 0.006153106689453125,\n",
       " 0.009033203125,\n",
       " -0.0157623291015625,\n",
       " -0.00504302978515625,\n",
       " 0.004825592041015625,\n",
       " -0.00330352783203125,\n",
       " 0.01061248779296875,\n",
       " 0.0061798095703125,\n",
       " -0.008941650390625,\n",
       " 0.0007753372192382812,\n",
       " 0.00850677490234375,\n",
       " 0.006519317626953125,\n",
       " 0.0177154541015625,\n",
       " 0.00615692138671875,\n",
       " 0.00797271728515625,\n",
       " 0.0178375244140625,\n",
       " 0.004955291748046875,\n",
       " -0.005619049072265625,\n",
       " 0.0131988525390625,\n",
       " 0.0019779205322265625,\n",
       " 0.0025577545166015625,\n",
       " -0.01312255859375,\n",
       " 0.0154266357421875,\n",
       " -0.003482818603515625,\n",
       " 0.026336669921875,\n",
       " -0.0159149169921875,\n",
       " 0.001708984375,\n",
       " -0.0035152435302734375,\n",
       " -0.017913818359375,\n",
       " 0.002330780029296875,\n",
       " 0.014862060546875,\n",
       " -0.0071258544921875,\n",
       " -0.01605224609375,\n",
       " 0.0012731552124023438,\n",
       " 0.0193939208984375,\n",
       " 0.0019550323486328125,\n",
       " 0.00016236305236816406,\n",
       " 0.0003001689910888672,\n",
       " 0.00792694091796875,\n",
       " 0.0236663818359375,\n",
       " 0.02032470703125,\n",
       " 0.034271240234375,\n",
       " -0.0209503173828125,\n",
       " -0.018707275390625,\n",
       " 0.01425933837890625,\n",
       " 0.00844573974609375,\n",
       " -0.0239410400390625,\n",
       " -0.01045989990234375,\n",
       " 0.005458831787109375,\n",
       " 0.031097412109375,\n",
       " 0.013671875,\n",
       " 0.01123809814453125,\n",
       " 0.0012912750244140625,\n",
       " -0.0125274658203125,\n",
       " -0.0242767333984375,\n",
       " 0.006732940673828125,\n",
       " 0.025177001953125,\n",
       " -0.0198974609375,\n",
       " 0.023040771484375,\n",
       " 0.0096282958984375,\n",
       " -0.0013475418090820312,\n",
       " -0.005706787109375,\n",
       " 0.012451171875,\n",
       " 0.0245819091796875,\n",
       " -0.004024505615234375,\n",
       " 0.01580810546875,\n",
       " -0.00945281982421875,\n",
       " 0.01336669921875,\n",
       " -0.00574493408203125,\n",
       " 0.0004298686981201172,\n",
       " -4.64320182800293e-05,\n",
       " 0.0268402099609375,\n",
       " 0.005657196044921875,\n",
       " -0.0052947998046875,\n",
       " -0.01611328125,\n",
       " -0.007415771484375,\n",
       " -0.0017957687377929688,\n",
       " -0.00804901123046875,\n",
       " 0.01331329345703125,\n",
       " 0.007434844970703125,\n",
       " -0.0009121894836425781,\n",
       " 0.00312042236328125,\n",
       " -0.0013551712036132812,\n",
       " 0.00690460205078125,\n",
       " -0.01367950439453125,\n",
       " 0.004673004150390625,\n",
       " -0.0016736984252929688,\n",
       " 0.0036144256591796875,\n",
       " -0.00524139404296875,\n",
       " -0.0037784576416015625,\n",
       " -0.005954742431640625,\n",
       " 0.0054473876953125,\n",
       " -0.0292816162109375,\n",
       " 0.01425933837890625,\n",
       " 0.00975799560546875,\n",
       " 0.0159759521484375,\n",
       " 0.0,\n",
       " -0.0194854736328125,\n",
       " 9.554624557495117e-05,\n",
       " -0.0136566162109375,\n",
       " -0.00873565673828125,\n",
       " 0.0209197998046875,\n",
       " 0.0036678314208984375,\n",
       " -0.007396697998046875,\n",
       " -0.01352691650390625,\n",
       " 0.0325927734375,\n",
       " -0.03955078125,\n",
       " -0.0238037109375,\n",
       " 0.0010166168212890625,\n",
       " -0.0084381103515625,\n",
       " 0.01568603515625,\n",
       " -0.00746917724609375,\n",
       " 0.00818634033203125,\n",
       " -0.01050567626953125,\n",
       " 0.01175689697265625,\n",
       " -0.0004394054412841797,\n",
       " -0.004955291748046875,\n",
       " -0.01349639892578125,\n",
       " -0.0034961700439453125,\n",
       " -0.0013895034790039062,\n",
       " 0.00281524658203125,\n",
       " -0.0078277587890625,\n",
       " -0.002857208251953125,\n",
       " -0.0238037109375,\n",
       " -0.007354736328125,\n",
       " -0.0182952880859375,\n",
       " -0.020355224609375,\n",
       " -0.01788330078125,\n",
       " 0.026123046875,\n",
       " -0.0019283294677734375,\n",
       " -0.0217437744140625,\n",
       " 0.0034008026123046875,\n",
       " -0.004486083984375,\n",
       " -0.00156402587890625,\n",
       " 0.0163726806640625,\n",
       " 0.02178955078125,\n",
       " -0.00775146484375,\n",
       " -0.01702880859375,\n",
       " 0.01078033447265625,\n",
       " -0.0182647705078125,\n",
       " -0.02520751953125,\n",
       " 0.0034427642822265625,\n",
       " -0.00823974609375,\n",
       " -0.01461029052734375,\n",
       " -0.011199951171875,\n",
       " -0.0012788772583007812,\n",
       " 0.00652313232421875,\n",
       " -0.00638580322265625,\n",
       " -0.0148468017578125,\n",
       " -0.018585205078125,\n",
       " -0.01812744140625,\n",
       " -0.0005512237548828125,\n",
       " 0.021697998046875,\n",
       " 0.00731658935546875,\n",
       " 0.00959014892578125,\n",
       " -0.00789642333984375,\n",
       " 0.00911712646484375,\n",
       " 0.008056640625,\n",
       " -0.01409912109375,\n",
       " -0.00995635986328125,\n",
       " 0.007099151611328125,\n",
       " 0.0008082389831542969,\n",
       " 0.007720947265625,\n",
       " 0.00566864013671875,\n",
       " 0.006610870361328125,\n",
       " -0.0169525146484375,\n",
       " 0.017608642578125,\n",
       " 0.0023899078369140625,\n",
       " -0.00930023193359375,\n",
       " -0.043212890625,\n",
       " -0.0285186767578125,\n",
       " -0.003192901611328125,\n",
       " 0.0012178421020507812,\n",
       " 0.0007205009460449219,\n",
       " 0.01345062255859375,\n",
       " -0.018157958984375,\n",
       " -0.0151519775390625,\n",
       " 0.023406982421875,\n",
       " -0.00909423828125,\n",
       " 0.0009794235229492188,\n",
       " -0.00733184814453125,\n",
       " -0.00798797607421875,\n",
       " 0.0076751708984375,\n",
       " -0.006748199462890625,\n",
       " -0.0023975372314453125,\n",
       " -0.01617431640625,\n",
       " -0.00909423828125,\n",
       " 0.005039215087890625,\n",
       " -0.020416259765625,\n",
       " -0.034332275390625,\n",
       " 0.0040130615234375,\n",
       " 0.003406524658203125,\n",
       " 0.0014524459838867188,\n",
       " -0.0066375732421875,\n",
       " -0.01183319091796875,\n",
       " 0.004405975341796875,\n",
       " 0.01467132568359375,\n",
       " 0.0218963623046875,\n",
       " 0.0019702911376953125,\n",
       " 0.01033782958984375,\n",
       " -0.006496429443359375,\n",
       " 0.0199432373046875,\n",
       " -0.00945281982421875,\n",
       " -0.0006933212280273438,\n",
       " -0.0174407958984375,\n",
       " -0.007228851318359375,\n",
       " 0.006786346435546875,\n",
       " 0.0175933837890625,\n",
       " 0.01511383056640625,\n",
       " 0.00644683837890625,\n",
       " 0.0224761962890625,\n",
       " -0.006557464599609375,\n",
       " 0.006038665771484375,\n",
       " 0.0136260986328125,\n",
       " -0.0028228759765625,\n",
       " 0.0181732177734375,\n",
       " -0.004924774169921875,\n",
       " 0.01071929931640625,\n",
       " 0.01430511474609375,\n",
       " -0.0099029541015625,\n",
       " -0.020111083984375,\n",
       " 0.00637054443359375,\n",
       " 0.0143585205078125,\n",
       " 0.006687164306640625,\n",
       " 0.01348114013671875,\n",
       " 0.00579833984375,\n",
       " -0.0004177093505859375,\n",
       " 0.01268768310546875,\n",
       " -0.0193023681640625,\n",
       " -0.02972412109375,\n",
       " -0.00228118896484375,\n",
       " 0.005809783935546875,\n",
       " 0.024810791015625,\n",
       " 0.003391265869140625,\n",
       " 0.000926971435546875,\n",
       " 0.0054931640625,\n",
       " 0.0242767333984375,\n",
       " -0.0185394287109375,\n",
       " 0.0081024169921875,\n",
       " 9.828805923461914e-05,\n",
       " -0.005947113037109375,\n",
       " -0.0185089111328125,\n",
       " -0.00909423828125,\n",
       " 0.017333984375,\n",
       " 0.0221405029296875,\n",
       " -0.0014524459838867188,\n",
       " 0.006137847900390625,\n",
       " -0.0034008026123046875,\n",
       " -0.0029430389404296875,\n",
       " 0.0190277099609375,\n",
       " 0.04962158203125,\n",
       " 0.005245208740234375,\n",
       " -0.0152740478515625,\n",
       " 0.0022716522216796875,\n",
       " 0.03521728515625,\n",
       " 0.00382232666015625,\n",
       " 0.0279693603515625,\n",
       " 0.00977325439453125,\n",
       " -0.00811004638671875,\n",
       " -0.0120849609375,\n",
       " 0.0173797607421875,\n",
       " 0.004352569580078125,\n",
       " 0.023284912109375,\n",
       " -0.0121307373046875,\n",
       " -0.0301513671875,\n",
       " -0.002216339111328125,\n",
       " -0.00792694091796875,\n",
       " -0.01096343994140625,\n",
       " -0.0024871826171875,\n",
       " -0.004909515380859375,\n",
       " 0.004795074462890625,\n",
       " 0.00716400146484375,\n",
       " 0.006687164306640625,\n",
       " 0.005466461181640625,\n",
       " -0.015472412109375,\n",
       " 0.0271453857421875,\n",
       " -0.040863037109375,\n",
       " 0.01251220703125,\n",
       " -0.018646240234375,\n",
       " -0.00868988037109375,\n",
       " 0.0002770423889160156,\n",
       " 0.033966064453125,\n",
       " -0.0021915435791015625,\n",
       " -0.0171051025390625,\n",
       " -0.0213165283203125,\n",
       " -0.00666046142578125,\n",
       " 0.036346435546875,\n",
       " -0.0038700103759765625,\n",
       " -0.00481414794921875,\n",
       " 0.018798828125,\n",
       " -0.0160980224609375,\n",
       " 0.04010009765625,\n",
       " -0.0200042724609375,\n",
       " -0.00197601318359375,\n",
       " -0.01325225830078125,\n",
       " -0.005828857421875,\n",
       " 0.00835418701171875,\n",
       " -0.004177093505859375,\n",
       " -0.01336669921875,\n",
       " 0.0007958412170410156,\n",
       " -0.0013647079467773438,\n",
       " 0.0164947509765625,\n",
       " -0.01152801513671875,\n",
       " -0.00555419921875,\n",
       " 0.01349639892578125,\n",
       " -0.004795074462890625,\n",
       " -0.02777099609375,\n",
       " -0.00238800048828125,\n",
       " 0.02239990234375,\n",
       " 0.01491546630859375,\n",
       " 0.02313232421875,\n",
       " 0.020660400390625,\n",
       " -0.02911376953125,\n",
       " 0.0002701282501220703,\n",
       " -0.00260162353515625,\n",
       " 0.00872802734375,\n",
       " 0.01522064208984375,\n",
       " 0.01540374755859375,\n",
       " 0.0232391357421875,\n",
       " 0.0276947021484375,\n",
       " 0.00739288330078125,\n",
       " 0.00724029541015625,\n",
       " -0.01390838623046875,\n",
       " -0.0094451904296875,\n",
       " 0.006256103515625,\n",
       " -0.00504302978515625,\n",
       " 0.010589599609375,\n",
       " -0.018463134765625,\n",
       " -0.00899505615234375,\n",
       " 0.042266845703125,\n",
       " -0.0114898681640625,\n",
       " 0.00452423095703125,\n",
       " -0.007511138916015625,\n",
       " 0.00658416748046875,\n",
       " -0.004917144775390625,\n",
       " -0.005168914794921875,\n",
       " 0.00714111328125,\n",
       " -0.013641357421875,\n",
       " 0.019866943359375,\n",
       " -0.00701904296875,\n",
       " -0.0002865791320800781,\n",
       " -0.035186767578125,\n",
       " -0.002925872802734375,\n",
       " -0.0160980224609375,\n",
       " 0.00579071044921875,\n",
       " -0.01238250732421875,\n",
       " 0.01922607421875,\n",
       " -0.03717041015625,\n",
       " 0.0021038055419921875,\n",
       " 0.005077362060546875,\n",
       " 0.015838623046875,\n",
       " -0.0060272216796875,\n",
       " -0.00749969482421875,\n",
       " -0.0079803466796875,\n",
       " 0.00678253173828125,\n",
       " 0.0025310516357421875,\n",
       " -0.00974273681640625,\n",
       " 0.00914764404296875,\n",
       " -0.0066986083984375,\n",
       " -0.03204345703125,\n",
       " -0.0035190582275390625,\n",
       " -0.0111846923828125,\n",
       " -0.0006427764892578125,\n",
       " -0.0233154296875,\n",
       " 0.0036144256591796875,\n",
       " -0.0167999267578125,\n",
       " 0.01174163818359375,\n",
       " -0.0041961669921875,\n",
       " -0.005359649658203125,\n",
       " -0.013397216796875,\n",
       " 0.01348114013671875,\n",
       " 0.0047607421875,\n",
       " 0.011444091796875,\n",
       " 0.002925872802734375,\n",
       " -0.01451873779296875,\n",
       " -0.00959014892578125,\n",
       " -0.00977325439453125,\n",
       " -0.011016845703125,\n",
       " 0.010833740234375,\n",
       " -0.001544952392578125,\n",
       " -0.01398468017578125,\n",
       " -0.00605010986328125,\n",
       " -0.0218353271484375,\n",
       " -0.01467132568359375,\n",
       " -0.00658416748046875,\n",
       " -0.0222320556640625,\n",
       " 0.022125244140625,\n",
       " 0.01018524169921875,\n",
       " -0.0291290283203125,\n",
       " 0.004428863525390625,\n",
       " 0.012969970703125,\n",
       " 0.0203704833984375,\n",
       " -0.0036716461181640625,\n",
       " 0.02435302734375,\n",
       " -0.020477294921875,\n",
       " 0.0195159912109375,\n",
       " -0.018585205078125,\n",
       " -0.0118560791015625,\n",
       " 0.045806884765625,\n",
       " 0.002002716064453125,\n",
       " -0.00801849365234375,\n",
       " -0.01465606689453125,\n",
       " 0.00942230224609375,\n",
       " 9.620189666748047e-05,\n",
       " 0.028717041015625,\n",
       " 0.004688262939453125,\n",
       " -0.008148193359375,\n",
       " 0.007228851318359375,\n",
       " 0.0057525634765625,\n",
       " 0.0093231201171875,\n",
       " 0.0006389617919921875,\n",
       " -0.016754150390625,\n",
       " -0.0210723876953125,\n",
       " -0.0019855499267578125,\n",
       " -0.012542724609375,\n",
       " 0.02685546875,\n",
       " 0.01436614990234375,\n",
       " -0.01560211181640625,\n",
       " -0.00991058349609375,\n",
       " 0.01131439208984375,\n",
       " 0.0010633468627929688,\n",
       " -0.01131439208984375,\n",
       " -0.006038665771484375,\n",
       " -0.001964569091796875,\n",
       " 0.0072784423828125,\n",
       " -0.01526641845703125,\n",
       " 0.004261016845703125,\n",
       " 0.00612640380859375,\n",
       " -0.0094146728515625,\n",
       " 0.0163726806640625,\n",
       " 0.00782012939453125,\n",
       " 0.0008654594421386719,\n",
       " 0.01806640625,\n",
       " 0.005523681640625,\n",
       " 0.00902557373046875,\n",
       " -0.0008463859558105469,\n",
       " -0.0008487701416015625,\n",
       " 0.0066375732421875,\n",
       " 0.0215301513671875,\n",
       " 0.005764007568359375,\n",
       " 0.01390838623046875,\n",
       " -0.006923675537109375,\n",
       " -0.011505126953125,\n",
       " -0.01071929931640625,\n",
       " -0.01134490966796875,\n",
       " -0.007534027099609375,\n",
       " -0.0105438232421875,\n",
       " -0.007320404052734375,\n",
       " -0.01529693603515625,\n",
       " -0.031494140625,\n",
       " -0.00386810302734375,\n",
       " -0.0159454345703125,\n",
       " 0.0019330978393554688,\n",
       " -0.016998291015625,\n",
       " -0.018402099609375,\n",
       " -0.01165771484375,\n",
       " -0.01024627685546875,\n",
       " -0.00514984130859375,\n",
       " -0.014556884765625,\n",
       " -0.0004982948303222656,\n",
       " 0.006671905517578125,\n",
       " -0.0070648193359375,\n",
       " 0.039581298828125,\n",
       " 0.00457000732421875,\n",
       " 0.01198577880859375,\n",
       " 0.0045166015625,\n",
       " 0.0105133056640625,\n",
       " -0.00862884521484375,\n",
       " 0.00899505615234375,\n",
       " -0.0253143310546875,\n",
       " -0.004772186279296875,\n",
       " 0.02703857421875,\n",
       " -0.008880615234375,\n",
       " -0.01364898681640625,\n",
       " -0.00914764404296875,\n",
       " 0.0191192626953125,\n",
       " -0.005687713623046875,\n",
       " -0.00402069091796875,\n",
       " 0.004150390625,\n",
       " -0.00511932373046875,\n",
       " 0.00305938720703125,\n",
       " 0.003360748291015625,\n",
       " 0.0217742919921875,\n",
       " 0.0159912109375,\n",
       " 0.003963470458984375,\n",
       " 0.00031256675720214844,\n",
       " 0.001190185546875,\n",
       " 0.0112152099609375,\n",
       " 0.01561737060546875,\n",
       " -0.016143798828125,\n",
       " -0.0021076202392578125,\n",
       " -0.0032711029052734375,\n",
       " 0.0191802978515625,\n",
       " 0.0097198486328125,\n",
       " -0.0011463165283203125,\n",
       " -0.019500732421875,\n",
       " -0.0014944076538085938,\n",
       " -0.01041412353515625,\n",
       " 0.018402099609375,\n",
       " -0.01161956787109375,\n",
       " -0.007904052734375,\n",
       " -0.0197906494140625,\n",
       " -0.00392913818359375,\n",
       " 0.0100860595703125,\n",
       " 0.014556884765625,\n",
       " -0.00652313232421875,\n",
       " 0.0038547515869140625,\n",
       " -0.00839996337890625,\n",
       " -0.0155487060546875,\n",
       " -0.0020809173583984375,\n",
       " -0.00872039794921875,\n",
       " -0.01172637939453125,\n",
       " 0.0009307861328125,\n",
       " -0.00907135009765625,\n",
       " -0.006061553955078125,\n",
       " 0.004871368408203125,\n",
       " -0.0119476318359375,\n",
       " 0.0220794677734375,\n",
       " 0.0091400146484375,\n",
       " 0.005657196044921875,\n",
       " -0.004871368408203125,\n",
       " -0.013214111328125,\n",
       " 0.00324249267578125,\n",
       " 0.021453857421875,\n",
       " 0.007659912109375,\n",
       " -0.018798828125,\n",
       " -0.0016374588012695312,\n",
       " 0.0120086669921875,\n",
       " 0.01418304443359375,\n",
       " -0.0294342041015625,\n",
       " 0.0005240440368652344,\n",
       " -9.006261825561523e-05,\n",
       " -0.0017766952514648438,\n",
       " 0.002513885498046875,\n",
       " -0.00678253173828125,\n",
       " 0.02447509765625,\n",
       " -0.01303863525390625,\n",
       " -0.01435089111328125,\n",
       " -0.00806427001953125,\n",
       " -0.01441192626953125,\n",
       " 0.015472412109375,\n",
       " 0.004886627197265625,\n",
       " -0.01039886474609375,\n",
       " -0.01242828369140625,\n",
       " 0.0205230712890625,\n",
       " -0.022796630859375,\n",
       " -0.02911376953125,\n",
       " 0.005504608154296875,\n",
       " -0.0215911865234375,\n",
       " -0.0011072158813476562,\n",
       " 0.00708770751953125,\n",
       " -0.005886077880859375,\n",
       " -0.00714111328125,\n",
       " 0.02520751953125,\n",
       " -0.0035247802734375,\n",
       " -0.0160675048828125,\n",
       " -0.02288818359375,\n",
       " 0.0027523040771484375,\n",
       " 0.02508544921875,\n",
       " 0.01183319091796875,\n",
       " -0.025177001953125,\n",
       " 0.027740478515625,\n",
       " 0.015838623046875,\n",
       " -0.007801055908203125,\n",
       " -0.0008544921875,\n",
       " -0.0174713134765625,\n",
       " -0.030609130859375,\n",
       " -0.0137481689453125,\n",
       " -0.031585693359375,\n",
       " 0.02288818359375,\n",
       " 0.0006389617919921875,\n",
       " 0.009429931640625,\n",
       " -0.004497528076171875,\n",
       " 0.01788330078125,\n",
       " -0.0087432861328125,\n",
       " -0.01007843017578125,\n",
       " 0.01352691650390625,\n",
       " 0.007282257080078125,\n",
       " 0.006031036376953125,\n",
       " -0.0020732879638671875,\n",
       " 0.0025386810302734375,\n",
       " 0.017364501953125,\n",
       " -0.0089111328125,\n",
       " -0.031707763671875,\n",
       " 0.01100921630859375,\n",
       " -0.01471710205078125,\n",
       " -0.0140838623046875,\n",
       " 0.003139495849609375,\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to the index\n",
    "# index = pc.Index(\"ai-coach\")\n",
    "# index = pc.Index(\"ahsan-400pg-pdf-doc-test\")\n",
    "index = pc.Index(\"surgical-tech-complete\")  # -- COMPLETE SURGICAL TECH BOOTCAMP\n",
    "\n",
    "\n",
    "# embedding_model = AutoModel.from_pretrained(\n",
    "#     'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# # Function to generate embeddings without tokenization\n",
    "# def get_embedding(data):\n",
    "#     embeddings = embedding_model.encode(data).tolist()\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "@track_time\n",
    "def get_embedding(text=\"None\"):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"nvidia/nv-embed-v1\",\n",
    "        encoding_format=\"float\",\n",
    "        extra_body={\"input_type\": \"query\", \"truncate\": \"NONE\"},\n",
    "    )\n",
    "\n",
    "    # print(response.data[0].embedding)\n",
    "    # print(count_tokens(response.data[0].embedding))\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "get_embedding(\"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `get_embedding` took 0.6780 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.3213 seconds\n",
      "[{'id': 'vec_160',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'pdf',\n",
      "              'source': 'D:\\\\Disrupt Labs\\\\Rag '\n",
      "                        'Experiments\\\\env\\\\Rag-pipelines-experiments\\\\Surgical '\n",
      "                        'Technologist Bootcamp\\\\Course 6 - Additional Surgical '\n",
      "                        'Techniques\\\\Lessons\\\\Lesson 33 - Pediatric '\n",
      "                        'Surgery.pdf',\n",
      "              'text': 'Pediatric Surgery\\nAdditional Surgical Techniques'},\n",
      " 'score': 0.715669751,\n",
      " 'values': []}, {'id': 'vec_1032',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'excel',\n",
      "              'source': 'D:\\\\Disrupt Labs\\\\Rag '\n",
      "                        'Experiments\\\\env\\\\Rag-pipelines-experiments\\\\Surgical '\n",
      "                        'Technologist Bootcamp\\\\Course 6 - Additional Surgical '\n",
      "                        'Techniques\\\\Ebooks\\\\Meta Data - Ebooks Course 6.xlsx',\n",
      "              'text': 'Course Name: Additional Surgical Techniques File Name: '\n",
      "                      'Chapter 33 (Pediatric Surgery) Topic: Pediatric Surgery '\n",
      "                      'Description: This chapter introduces pediatric surgery, '\n",
      "                      'focusing on congenital anomalies, childhood disease '\n",
      "                      'treatments, and trauma care. It discusses physiological '\n",
      "                      'differences in pediatric patients, airway management, '\n",
      "                      'and anesthesia considerations. Keywords: Pediatric '\n",
      "                      'Surgery, Cleft Lip Repair, Esophageal Atresia, Airway '\n",
      "                      'Management, Neonatal Surgery, Pediatric Anesthesia, '\n",
      "                      'Congenital Anomalies, Fluid Balance, Myelomeningocele '\n",
      "                      'Repair, Psychosocial Care\\u200b'},\n",
      " 'score': 0.480490506,\n",
      " 'values': []}, {'id': 'vec_964',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'excel',\n",
      "              'source': 'D:\\\\Disrupt Labs\\\\Rag '\n",
      "                        'Experiments\\\\env\\\\Rag-pipelines-experiments\\\\Surgical '\n",
      "                        'Technologist Bootcamp\\\\Course 6 - Additional Surgical '\n",
      "                        'Techniques\\\\Assessments\\\\Meta Data - Assessments '\n",
      "                        'Course 6.xlsx',\n",
      "              'text': 'Course Name: Additional Surgical Techniques File Name: '\n",
      "                      'Quiz - 33 Topic: Pediatric Surgery Description: This '\n",
      "                      'lesson focuses on pediatric surgery, highlighting '\n",
      "                      'congenital anomalies, surgical treatment of childhood '\n",
      "                      'diseases, and trauma care. It covers airway management, '\n",
      "                      'fluid balance, and anesthetic considerations unique to '\n",
      "                      'pediatric patients. Common procedures such as cleft lip '\n",
      "                      'repair, esophageal atresia correction, and pediatric '\n",
      "                      'tumor removal are discussed.'},\n",
      " 'score': 0.474920332,\n",
      " 'values': []}, {'id': 'vec_1038',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'excel',\n",
      "              'source': 'D:\\\\Disrupt Labs\\\\Rag '\n",
      "                        'Experiments\\\\env\\\\Rag-pipelines-experiments\\\\Surgical '\n",
      "                        'Technologist Bootcamp\\\\Course 6 - Additional Surgical '\n",
      "                        'Techniques\\\\Lessons\\\\Meta Data - Lessons Course '\n",
      "                        '6.xlsx',\n",
      "              'text': 'Course Name: Additional Surgical Techniques File Name: '\n",
      "                      'Lesson 33 - Pediatric Surgery Topic: Pediatric Surgery '\n",
      "                      'Description: This lesson focuses on pediatric surgery, '\n",
      "                      'highlighting congenital anomalies, surgical treatment '\n",
      "                      'of childhood diseases, and trauma care. It covers '\n",
      "                      'airway management, fluid balance, and anesthetic '\n",
      "                      'considerations unique to pediatric patients. Common '\n",
      "                      'procedures such as cleft lip repair, esophageal atresia '\n",
      "                      'correction, and pediatric tumor removal are discussed. '\n",
      "                      'Keywords: Pediatric Surgery, Cleft Lip Repair, '\n",
      "                      'Esophageal Atresia, Airway Management, Neonatal '\n",
      "                      'Surgery, Pediatric Anesthesia, Congenital Anomalies, '\n",
      "                      'Fluid Balance, Myelomeningocele Repair, Psychosocial '\n",
      "                      'Care\\u200b'},\n",
      " 'score': 0.459426403,\n",
      " 'values': []}, {'id': 'vec_1049',\n",
      " 'metadata': {'chunk_id': 0.0,\n",
      "              'file_type': 'excel',\n",
      "              'source': 'D:\\\\Disrupt Labs\\\\Rag '\n",
      "                        'Experiments\\\\env\\\\Rag-pipelines-experiments\\\\Surgical '\n",
      "                        'Technologist Bootcamp\\\\Course 6 - Additional Surgical '\n",
      "                        'Techniques\\\\Videos - Transcripts\\\\Meta Data - Videos '\n",
      "                        'Course 6.xlsx',\n",
      "              'text': 'Course Name: Additional Surgical Techniques File Name: '\n",
      "                      'C6L33_YouTube 4 Topic: Inside Pediatric Surgery with '\n",
      "                      'Dr. Barksdale Description: Shares personal insights and '\n",
      "                      'philosophy of pediatric surgeon Dr. Edward Barksdale. '\n",
      "                      'Emphasizes emotional intelligence, humility, patient '\n",
      "                      'focus, and the holistic importance of head, hands, and '\n",
      "                      'heart in pediatric surgical care. Keywords: pediatric '\n",
      "                      'surgery, Barksdale, humility, focus, laparoscopic, '\n",
      "                      'patient care, family, heart, communication, mindset'},\n",
      " 'score': 0.392002195,\n",
      " 'values': []}]\n"
     ]
    }
   ],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "@track_time\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=5, include_metadata=True)\n",
    "    return result[\"matches\"]\n",
    "\n",
    "\n",
    "print(query_pinecone(get_embedding(\"Pediatric surgery definition\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Use Case | Recommended top_p | Notes |\n",
    "|----------|------------------|-------|\n",
    "| Factual Q&A | 0.1 - 0.3 | Lower values for more deterministic, factual responses |\n",
    "| Code Generation | 0.2 - 0.5 | Precision matters more than creativity |\n",
    "| Technical Writing | 0.5 - 0.7 | Balanced approach for technical accuracy with clarity |\n",
    "| General Conversation | 0.7 - 0.9 | Good balance for most chatbot applications |\n",
    "| Creative Writing | 0.9 - 1.0 | Higher values for more diverse and creative outputs |\n",
    "\n",
    "\\n\n",
    "| Parameter Combination | Use Case |\n",
    "|----------------------|----------|\n",
    "| top_p=0.5, temperature=0.3 | Highly factual, consistent responses |\n",
    "| top_p=0.7, temperature=0.5 | Educational content with examples |\n",
    "| top_p=0.9, temperature=0.7 | Creative but coherent responses |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified query_groq function with more explicit streaming handling\n",
    "@track_time\n",
    "def query_groq(user_prompt, sys_prompt):\n",
    "    client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "    # Always use streaming mode\n",
    "    return client.chat.completions.create(\n",
    "        model=LLM_MODEL,  # or whichever model you're using\n",
    "        temperature=0.5,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": sys_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        stream=True,\n",
    "        # top_p=0.7,  # testing for better results\n",
    "    )\n",
    "\n",
    "\n",
    "# Print all tool calls\n",
    "# print(completion.choices[0].message.executed_tools)\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "\"\"\"\n",
    "Putting tokenizer outside of the function to avoid reinitialization and optimize performance.\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "\n",
    "@track_time\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq and Gradio with Streaming Enabled\n",
    "\n",
    "- ### i.e. will start showing text as soon as it gets generated from groq inference\n",
    "- ### faster than optimized version\n",
    "\n",
    "## Query:\n",
    "- pediatic surgery\n",
    "## Response Time:\n",
    "User Query Tokens: 6\n",
    "[Time Tracker] `get_embedding` took 0.4752 seconds\n",
    "[Time Tracker] `query_pinecone` took 0.2222 seconds\n",
    "[Time Tracker] `query_groq` took 0.5060 seconds\n",
    "\n",
    "Total time: 1.19 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_17136\\645222516.py:139: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n",
      "d:\\Disrupt Labs\\Rag Experiments\\env\\Lib\\site-packages\\gradio\\layouts\\column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `create_gradio_interface` took 0.1031 seconds\n",
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "* Running on public URL: https://f1da8b0b9b119b5911.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f1da8b0b9b119b5911.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Modified query_groq function with more explicit streaming handling\n",
    "# @track_time\n",
    "# def query_groq(prompt):\n",
    "#     client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "#     # Always use streaming mode\n",
    "#     return client.chat.completions.create(\n",
    "#         model=\"llama3-70b-8192\",  # or whichever model you're using\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         stream=True,\n",
    "#     )\n",
    "\n",
    "# --------------------------------------------------------- ## Groq and Gradio with Streaming Enabled -----------------------------------------------------\n",
    "# Modified process_user_query to properly yield streaming updates\n",
    "@track_time\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk[\"metadata\"][\"text\"] for chunk in relevant_chunks)\n",
    "\n",
    "    # Format conversation history for the prompt\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\" for user, response in conversation_history\n",
    "    )\n",
    "    # ---------------------------------------- OLD SINGULAR PROMPT ------------------------------------------------------------------------\n",
    "    # # Create structured prompt\n",
    "    # prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow these guidelines carefully:\n",
    "\n",
    "    # 1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "    # 2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "    # 3. Ask guiding questions to encourage critical thinking.\n",
    "    # 4. Adapt your explanation to match the student's knowledge level.\n",
    "    # 5. Strictly use terminologies provided in the given context.\n",
    "    # 6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "    # 7. Only answer based on the provided contextdo not speculate or include external information.\n",
    "    # 8. Always provide all specific relevant sources from the context in your responses urls, video names, video timestamps , links , resources , ebook names, lesson names , lesson numbers and anything else you think would be relevant to the user query.\n",
    "    # 9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "    # 10. Must provide all relevant video timestamp from where to start watching and where to end watching\n",
    "    # Context from learning materials:\n",
    "    # {context}\n",
    "\n",
    "    # Conversation history:\n",
    "    # {history_str}\n",
    "\n",
    "    # New student question:\n",
    "    # \"{user_query}\"\n",
    "\n",
    "    # Provide a thoughtful and contextually accurate response now:\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # --------------------------------------------- UPDATED SYSTEM AND USER PROMPT ------------------------------------------------------------------------\n",
    "    # System prompt - contains instructions for the AI's behavior\n",
    "    system_prompt = f\"\"\"\n",
    "    \n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "    \n",
    "    learning materials:\n",
    "    {context}\n",
    "    \n",
    "    You are an expert, knowledgeable, and friendly coach. Follow these **guidelines** carefully:\n",
    "\n",
    "    1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "    2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "    3. Ask guiding questions to encourage critical thinking.\n",
    "    4. Adapt your explanation to match the student's knowledge level.\n",
    "    5. Strictly use terminologies provided in the given context.\n",
    "    6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "    7. Only answer based on the provided learning materialsdo not speculate or include external information.\n",
    "    8. **Always provide all specific relevant sources with name from the context in your responses urls, video names, video timestamps, links, resources, **ebook names**, lesson names, lesson numbers, if the user query is not relevant to the context then don't provide any references and sources.**\n",
    "    9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "    10. Must provide all relevant video timestamp with video name from where to start watching and where to end watching , if you can't provide timestamp then tell the user to watch the whole video.\n",
    "    11. Provide a thoughtful and contextually accurate response\n",
    "    12. Avoid Repetition.\n",
    "    13. **If student asks something completely out of context then politely decline and ask the user to ask a question related to their course they are studying and don't provide any references and sources for this.**\n",
    "    14. Please avoid \"Bank Name\"\n",
    "    15. When users request questions, answers, quizzes or exams, generate high-quality educational assessments directly from the learning material by using for example:  multiple question types (multiple-choice with 4-5 options, true/false, short answer, fill-in-the-blanks, essay) with clear formatting; creating questions at various cognitive levels (recall, comprehension, application, analysis); providing detailed answer keys with explanations; organizing content with consistent formatting (numbered questions, lettered options, bold correct answers); adapting difficulty based on subject matter; and including step-by-step solutions for problem-solving questions.\n",
    "    16. Never generate fabricated information when providing references or sources. Only provide facts, references, citations, lesson names, e-book titles, video names, timestamps explicitly present in the provided learning materials\n",
    "    \"\"\"\n",
    "\n",
    "    # User prompt - contains the specific query and context\n",
    "    user_prompt = f\"\"\"\n",
    "    \n",
    "    \n",
    "    New student question:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Then in your query_groq function:\n",
    "    stream_response = query_groq(user_prompt, system_prompt)\n",
    "\n",
    "    # The function now directly yields the stream chunks for the Gradio interface to use\n",
    "    full_response = \"\"\n",
    "\n",
    "    # First, yield a response with empty text to set up the message\n",
    "    # This creates the user message immediately\n",
    "    temp_history = conversation_history.copy()\n",
    "    temp_history.append((user_query, \"\"))\n",
    "    yield temp_history, context\n",
    "\n",
    "    # Process the stream\n",
    "    for chunk in stream_response:\n",
    "        if (\n",
    "            hasattr(chunk.choices[0].delta, \"content\")\n",
    "            and chunk.choices[0].delta.content is not None\n",
    "        ):\n",
    "            content_chunk = chunk.choices[0].delta.content\n",
    "            full_response += content_chunk\n",
    "\n",
    "            # Create a temporary history with the current response\n",
    "            temp_history = conversation_history.copy()\n",
    "            temp_history.append((user_query, full_response))\n",
    "\n",
    "            # Yield the updated history for display\n",
    "            yield temp_history, context\n",
    "\n",
    "    # Return the final history with the complete response\n",
    "    final_history = conversation_history.copy()\n",
    "    final_history.append((user_query, full_response))\n",
    "    yield final_history, context\n",
    "\n",
    "\n",
    "@track_time\n",
    "def create_gradio_interface(conversation_history):\n",
    "    with gr.Blocks() as interface:\n",
    "        gr.Markdown(\"#  AI Coaching Assistant\")\n",
    "        gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "\n",
    "        # State management\n",
    "        chat_history = gr.State(conversation_history)\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "            with gr.Column(scale=0.5):\n",
    "                context_display = gr.Textbox(\n",
    "                    label=\"Relevant Context\", interactive=False\n",
    "                )\n",
    "\n",
    "        user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            undo_btn = gr.Button(\"Undo Last\")\n",
    "            clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "        def handle_submit(user_query, history):\n",
    "            if not user_query.strip():\n",
    "                return gr.update(), history, \"\"\n",
    "\n",
    "            # Use the generator directly from process_user_query\n",
    "            # This will yield incremental updates as they arrive\n",
    "            response_generator = process_user_query(user_query, history)\n",
    "\n",
    "            for updated_history, context in response_generator:\n",
    "                # Directly update the chatbot with each streaming chunk\n",
    "                yield \"\", updated_history, context, updated_history\n",
    "\n",
    "        # Component interactions with streaming support\n",
    "        submit_btn.click(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        # Add submit on Enter key press\n",
    "        user_input.submit(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        undo_btn.click(\n",
    "            lambda history: history[:-1] if history else [],\n",
    "            [chat_history],\n",
    "            [chat_history],\n",
    "        ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "        clear_btn.click(lambda: [], None, [chat_history]).then(\n",
    "            lambda: ([], \"\"), None, [chatbot, context_display]\n",
    "        )\n",
    "\n",
    "    return interface\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Initialize conversation history\n",
    "#     initial_conversation_history = []\n",
    "\n",
    "#     # Create and launch the interface\n",
    "#     interface = create_gradio_interface(initial_conversation_history)\n",
    "#     interface.launch(share=True)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for the application.\n",
    "\n",
    "    Initializes the conversation history with a welcome message,\n",
    "    creates the Gradio interface, and launches the web app.\n",
    "    \"\"\"\n",
    "    # Initialize conversation history with welcome message\n",
    "    welcome_message = \"Hi there! I'm your AI coach. I can help answer questions about your course materials, explain difficult concepts, and guide your learning journey. What would you like to know today?\"\n",
    "    initial_conversation_history = [(\"\", welcome_message)]\n",
    "\n",
    "    # Create and launch the interface\n",
    "    interface = create_gradio_interface(initial_conversation_history)\n",
    "    interface.launch(share=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Version\n",
    "- ### slower than previous version\n",
    "- ### slightly better responses and source citations\n",
    "\n",
    "## Query:\n",
    "- pediatic surgery\n",
    "## Response Time:\n",
    "User Query Tokens: 7\n",
    "[Time Tracker] `get_embedding` took 0.7417 seconds\n",
    "[Time Tracker] `query_pinecone` took 1.4344 seconds\n",
    "[Time Tracker] `query_groq` took 0.4740 seconds\n",
    "\n",
    "Total time = 2.64 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-- Optimized Version ---\n",
    "Streaming AI Coaching Assistant\n",
    "\n",
    "This module implements a Gradio-based chatbot that uses streaming responses from Groq API\n",
    "to provide real-time feedback to the user. It uses RAG (Retrieval-Augmented Generation)\n",
    "to provide contextually relevant answers from a knowledge base.\n",
    "\n",
    "Key components:\n",
    "- query_groq: Interfaces with Groq API for streaming LLM responses\n",
    "- process_user_query: Manages the streaming process and context retrieval \n",
    "- create_gradio_interface: Creates and configures the Gradio UI\n",
    "\n",
    "The streaming implementation uses Python generators (with yield statements) to provide\n",
    "real-time updates to the Gradio UI as tokens arrive from the LLM, rather than waiting\n",
    "for the complete response.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Modified query_groq function with optimized streaming\n",
    "# @track_time\n",
    "# def query_groq(prompt):\n",
    "#     \"\"\"\n",
    "#     Send a prompt to Groq API with streaming enabled.\n",
    "\n",
    "#     Args:\n",
    "#         prompt (str): The prompt to send to the LLM\n",
    "\n",
    "#     Returns:\n",
    "#         A stream of response chunks from the Groq API\n",
    "#     \"\"\"\n",
    "#     client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "#     # Always use streaming mode\n",
    "#     return client.chat.completions.create(\n",
    "#         model=\"llama3-70b-8192\",  # or whichever model you're using\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         stream=True,\n",
    "#         # Add parameters to potentially improve response time\n",
    "#         temperature=0.7,\n",
    "#         top_p=0.9,\n",
    "#         max_tokens=2048,  # Adjust based on your needs\n",
    "#     )\n",
    "\n",
    "\n",
    "# Optimized process_user_query with better streaming performance\n",
    "@track_time\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    \"\"\"\n",
    "    Process a user query using streaming responses.\n",
    "\n",
    "    This generator function:\n",
    "    1. Retrieves relevant context from Pinecone\n",
    "    2. Sends the query to Groq\n",
    "    3. Yields updates as chunks arrive from the LLM\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's question\n",
    "        conversation_history (list): List of previous (query, response) tuples\n",
    "\n",
    "    Yields:\n",
    "        Tuples of (updated_history, context) as response chunks arrive\n",
    "    \"\"\"\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context - do this once upfront\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk[\"metadata\"][\"text\"] for chunk in relevant_chunks)\n",
    "\n",
    "    # Format conversation history for the prompt - do this once\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\" for user, response in conversation_history\n",
    "    )\n",
    "\n",
    "    # Create structured prompt\n",
    "    prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow these guidelines carefully:\n",
    "\n",
    "    1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "    2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "    3. Ask guiding questions to encourage critical thinking.\n",
    "    4. Adapt your explanation to match the student's knowledge level.\n",
    "    5. Strictly use terminologies provided in the given context.\n",
    "    6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "    7. Only answer based on the provided contextdo not speculate or include external information.\n",
    "    8. Always provide all specific relevant sources from the context in your responses urls, video names, video timestamps, links, resources, ebook names, lesson names, lesson numbers and anything else you think would be relevant to the user query.\n",
    "    9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "    10. Must provide all relevant video timestamp from where to start watching and where to end watching \n",
    "    \n",
    "    11. Please avoid \"Bank Name\" \n",
    "    Context from learning materials:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "\n",
    "    New student question:\n",
    "    \"{user_query}\"\n",
    "    \n",
    "    Provide a thoughtful and contextually accurate response now:\"\"\"\n",
    "\n",
    "    # Get streaming LLM response\n",
    "    stream_response = query_groq(prompt)\n",
    "\n",
    "    # Initialize response\n",
    "    full_response = \"\"\n",
    "\n",
    "    # First, yield a response with empty text to set up the message\n",
    "    # This creates the user message immediately\n",
    "    temp_history = conversation_history.copy()\n",
    "    temp_history.append((user_query, \"\"))\n",
    "    yield temp_history, context\n",
    "\n",
    "    # For efficiency, create these objects once outside the loop\n",
    "    buffer = \"\"\n",
    "    buffer_size = 10  # Characters to buffer before updating UI\n",
    "    update_frequency = 0  # Counter to track updates\n",
    "\n",
    "    # Process the stream\n",
    "    for chunk in stream_response:\n",
    "        if (\n",
    "            hasattr(chunk.choices[0].delta, \"content\")\n",
    "            and chunk.choices[0].delta.content is not None\n",
    "        ):\n",
    "            content_chunk = chunk.choices[0].delta.content\n",
    "            buffer += content_chunk\n",
    "            full_response += content_chunk\n",
    "\n",
    "            # Only update UI after buffer reaches certain size or every few chunks\n",
    "            # This reduces the number of updates while still maintaining responsiveness\n",
    "            update_frequency += 1\n",
    "            if len(buffer) >= buffer_size or update_frequency >= 5:\n",
    "                temp_history = conversation_history.copy()\n",
    "                temp_history.append((user_query, full_response))\n",
    "                yield temp_history, context\n",
    "                buffer = \"\"\n",
    "                update_frequency = 0\n",
    "\n",
    "    # Ensure final state is yielded if buffer has content\n",
    "    if buffer or update_frequency > 0:\n",
    "        final_history = conversation_history.copy()\n",
    "        final_history.append((user_query, full_response))\n",
    "        yield final_history, context\n",
    "\n",
    "\n",
    "@track_time\n",
    "def create_gradio_interface(conversation_history):\n",
    "    \"\"\"\n",
    "    Create a Gradio interface for the coaching assistant.\n",
    "\n",
    "    This function sets up the UI components and defines how they interact.\n",
    "    The interface includes real-time streaming of LLM responses.\n",
    "\n",
    "    Args:\n",
    "        conversation_history (list): Initial conversation history\n",
    "\n",
    "    Returns:\n",
    "        gr.Blocks: The configured Gradio interface\n",
    "    \"\"\"\n",
    "    # Welcome message to display when a new user starts\n",
    "    welcome_message = \"Hi there! I'm your AI learning coach. I can help answer questions about your course materials, explain difficult concepts, and guide your learning journey. What would you like to know today?\"\n",
    "\n",
    "    # Add initial welcome message to conversation history if it's empty\n",
    "    if not conversation_history:\n",
    "        conversation_history = [(\"\", welcome_message)]\n",
    "\n",
    "    with gr.Blocks() as interface:\n",
    "        gr.Markdown(\"#  AI Coaching Assistant\")\n",
    "        gr.Markdown(\n",
    "            \"Ask questions about your course materials and get real-time, personalized help.\"\n",
    "        )\n",
    "\n",
    "        # State management\n",
    "        chat_history = gr.State(conversation_history)\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(value=conversation_history, height=500)\n",
    "            with gr.Column(scale=0.5):\n",
    "                context_display = gr.Textbox(\n",
    "                    label=\"Relevant Learning Materials\", interactive=False, visible=True\n",
    "                )\n",
    "\n",
    "        user_input = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"Type your question here and press Enter...\",\n",
    "            autofocus=True,\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            undo_btn = gr.Button(\"Undo Last\")\n",
    "            clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "        def handle_submit(user_query, history):\n",
    "            \"\"\"\n",
    "            Handle user query submission with streaming response.\n",
    "\n",
    "            This generator function processes the user query and yields\n",
    "            updates to the UI as they become available.\n",
    "\n",
    "            Args:\n",
    "                user_query (str): The user's question\n",
    "                history (list): Current conversation history\n",
    "\n",
    "            Yields:\n",
    "                Updates to the UI components\n",
    "            \"\"\"\n",
    "            if not user_query.strip():\n",
    "                return gr.update(), history, \"\"\n",
    "\n",
    "            # Use the generator directly from process_user_query\n",
    "            # This will yield incremental updates as they arrive\n",
    "            response_generator = process_user_query(user_query, history)\n",
    "\n",
    "            for updated_history, context in response_generator:\n",
    "                # Directly update the chatbot with each streaming chunk\n",
    "                yield \"\", updated_history, context, updated_history\n",
    "\n",
    "        # Component interactions with streaming support\n",
    "        submit_btn.click(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        # Add submit on Enter key press\n",
    "        user_input.submit(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        undo_btn.click(\n",
    "            lambda history: history[:-1] if history else [],\n",
    "            [chat_history],\n",
    "            [chat_history],\n",
    "        ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "        clear_btn.click(lambda: [(\"\", welcome_message)], None, [chat_history]).then(\n",
    "            lambda x: x, [chat_history], [chatbot]\n",
    "        ).then(lambda: \"\", None, [context_display])\n",
    "\n",
    "    return interface\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for the application.\n",
    "\n",
    "    Initializes the conversation history with a welcome message,\n",
    "    creates the Gradio interface, and launches the web app.\n",
    "    \"\"\"\n",
    "    # Initialize conversation history with welcome message\n",
    "    welcome_message = \"Hi there! I'm your AI learning coach. I can help answer questions about your course materials, explain difficult concepts, and guide your learning journey. What would you like to know today?\"\n",
    "    initial_conversation_history = [(\"\", welcome_message)]\n",
    "\n",
    "    # Create and launch the interface\n",
    "    interface = create_gradio_interface(initial_conversation_history)\n",
    "    interface.launch(share=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
