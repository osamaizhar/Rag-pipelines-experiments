{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import glob\n",
    "# import concurrent.futures\n",
    "import gradio as gr\n",
    "\n",
    "# import numpy as np\n",
    "# import textwrap\n",
    "# import PyPDF2\n",
    "# import requests\n",
    "import os\n",
    "\n",
    "# import pinecone\n",
    "import time\n",
    "# import asyncio\n",
    "\n",
    "# ------------------------- Streaming Implementation -------------------------\n",
    "# from groq import AsyncGroq\n",
    "# from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain.schema.document import Document\n",
    "# from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "# from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# from typing import List, Tuple\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec, CloudProvider, AwsRegion, VectorType\n",
    "from groq import Groq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "# PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_CHAT_URL = os.getenv(\"GROQ_CHAT_URL\")\n",
    "\n",
    "NVIDIA_API = os.getenv(\"NVIDIA_API\")\n",
    "NVIDIA_BASE_URL = os.getenv(\"NVIDIA_BASE_URL\")\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "# LLM_MODEL = \"llama3-70b-8192\"\n",
    "LLM_MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "\n",
    "# NVidia Embedding import\n",
    "client = OpenAI(\n",
    "    api_key=NVIDIA_API,\n",
    "    base_url=NVIDIA_BASE_URL,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    - Context window: 128K\n",
    "Ouput:\n",
    "    - Output Max Tokens: 32,768\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"[Time Tracker] `{func.__name__}` took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# # EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "\n",
    "# vo = voyageai.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "# index = pc.Index(\"ai-coach\")\n",
    "# index = pc.Index(\"ahsan-400pg-pdf-doc-test\")\n",
    "index = pc.Index(\"surgical-tech-complete\")  # -- COMPLETE SURGICAL TECH BOOTCAMP\n",
    "\n",
    "\n",
    "# embedding_model = AutoModel.from_pretrained(\n",
    "#     'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# # Function to generate embeddings without tokenization\n",
    "# def get_embedding(data):\n",
    "#     embeddings = embedding_model.encode(data).tolist()\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "@track_time\n",
    "def get_embedding(text=\"None\"):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"nvidia/nv-embed-v1\",\n",
    "        encoding_format=\"float\",\n",
    "        extra_body={\"input_type\": \"query\", \"truncate\": \"NONE\"},\n",
    "    )\n",
    "\n",
    "    # print(response.data[0].embedding)\n",
    "    # print(count_tokens(response.data[0].embedding))\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "get_embedding(\"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 7\n",
      "[Time Tracker] `get_embedding` took 0.6402 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.3696 seconds\n",
      "[Time Tracker] `query_groq` took 0.5144 seconds\n"
     ]
    }
   ],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "@track_time\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=5, include_metadata=True)\n",
    "    return result[\"matches\"]\n",
    "\n",
    "\n",
    "print(query_pinecone(get_embedding(\"Pediatric surgery definition\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Groq LLM\n",
    "# def query_groq(prompt: str) -> str:\n",
    "#     response = requests.post(\n",
    "#         GROQ_CHAT_URL,\n",
    "#         headers=GROQ_HEADERS,\n",
    "#         json={\n",
    "#             \"model\": LLM_MODEL,\n",
    "#             \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#             \"temperature\": 0.5,\n",
    "#             # \"max_tokens\": 8192  # max from groq website\n",
    "#         },\n",
    "#     )\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(f\"Error querying Groq: {response.text}\")\n",
    "\n",
    "#     return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# def query_groq(prompt: str) -> str:\n",
    "#     client = Groq()\n",
    "#     completion = client.chat.completions.create(\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         # Change model to compound-beta to use agentic tooling\n",
    "#         # model: \"llama-3.3-70b-versatile\",\n",
    "#         model=LLM_MODEL,\n",
    "#     )\n",
    "#     # print(completion.choices[0].message.content)\n",
    "#     return completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# @track_time\n",
    "# def query_groq(prompt: str) -> str:\n",
    "#     client = Groq()\n",
    "#     completion = client.chat.completions.create(\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         model=LLM_MODEL,\n",
    "#         temperature=1,  # Set temperature to 0.5\n",
    "#     )\n",
    "#     return completion.choices[0]\n",
    "\n",
    "\n",
    "# query_groq(\"Hello\")\n",
    "\n",
    "\n",
    "# # Modified query_groq function with more explicit streaming handling\n",
    "@track_time\n",
    "def query_groq(prompt):\n",
    "    client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "    # Always use streaming mode\n",
    "    return client.chat.completions.create(\n",
    "        model=LLM_MODEL,  # or whichever model you're using\n",
    "        temperature=0.5,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# Print all tool calls\n",
    "# print(completion.choices[0].message.executed_tools)\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "\"\"\"\n",
    "Putting tokenizer outside of the function to avoid reinitialization and optimize performance.\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "\n",
    "@track_time\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio GUI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------------------------------- WORKING 3 Enter key submits user query -------------------------------------------\n",
    "# # Initialize empty conversation history (list of tuples)\n",
    "# conversation_history = []\n",
    "\n",
    "\n",
    "# @track_time\n",
    "# def process_user_query(user_query: str, conversation_history: list):\n",
    "#     print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "#     # Generate embedding and get relevant context\n",
    "#     embedding = get_embedding(user_query)\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     context = \"\\n\".join(chunk[\"metadata\"][\"text\"] for chunk in relevant_chunks)\n",
    "#     # print(\"CONTEXT:\", context)\n",
    "\n",
    "#     # Format conversation history for the prompt\n",
    "#     history_str = \"\\n\".join(\n",
    "#         f\"User: {user}\\nCoach: {response}\" for user, response in conversation_history\n",
    "#     )\n",
    "\n",
    "#     # Create structured prompt\n",
    "#     prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow these guidelines carefully:\n",
    "\n",
    "#     1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "#     2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "#     3. Ask guiding questions to encourage critical thinking.\n",
    "#     4. Adapt your explanation to match the student's knowledge level.\n",
    "#     5. Strictly use terminologies provided in the given context.\n",
    "#     6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "#     7. Only answer based on the provided context‚Äîdo not speculate or include external information.\n",
    "#     8. Always provide all specific relevant sources from the context in your responses urls, video names, video timestamps , links , resources , ebook names, lesson names , lesson numbers and anything else you think would be relevant to the user query.\n",
    "#     9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "#     10. Must provide all relevant video timestamp from where to start watching and where to end watching\n",
    "#     Context from learning materials:\n",
    "#     {context}\n",
    "\n",
    "#     Conversation history:\n",
    "#     {history_str}\n",
    "\n",
    "#     New student question:\n",
    "#     \"{user_query}\"\n",
    "\n",
    "#     Provide a thoughtful and contextually accurate response now:\"\"\"\n",
    "\n",
    "#     # --------------------- Reasoning Prompt --------------------------------------------------------------\n",
    "#     # prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow this structured response framework:\n",
    "\n",
    "#     # ### Response Requirements\n",
    "#     # 1. **Reasoning Process**:\n",
    "#     # - Analyze the question against provided context\n",
    "#     # - Identify relevant context snippets with source citations\n",
    "#     # - Perform sentiment analysis on conversation history\n",
    "\n",
    "#     # 2. **Response Format**:\n",
    "#     # - [Understanding] Paraphrase the question to confirm comprehension\n",
    "#     # - [Relevant Context] Cite exact source material with location references\n",
    "#     # - [Step-by-Step Explanation] Break down concepts using chain-of-thought\n",
    "#     # - [Examples] Provide 2-3 ideal examples from context\n",
    "#     # - [Guiding Questions] Pose 1-2 reflective questions to deepen learning\n",
    "#     # - [Summary] Concise answer reiterating key points\n",
    "\n",
    "#     # 3. **Style Guidelines**:\n",
    "#     # - Use terminology strictly from context\n",
    "#     # - Adapt complexity to student's history\n",
    "#     # - Maintain empathetic tone based on sentiment analysis\n",
    "\n",
    "#     # ### Context Materials:\n",
    "#     # {context}\n",
    "\n",
    "#     # ### Conversation History:\n",
    "#     # {history_str}\n",
    "\n",
    "#     # ### New Question:\n",
    "#     # \"{user_query}\"\n",
    "\n",
    "#     # ### Your Response:\n",
    "#     # [Understanding] First, let me clarify what you're asking...\n",
    "#     # [Relevant Context] According to [Source X, Section Y]...\n",
    "#     # [Step-by-Step Explanation] The process works as follows:\n",
    "#     # 1. First concept...\n",
    "#     # 2. Second concept...\n",
    "#     # 3. Practical application...\n",
    "#     # [Examples] For instance:\n",
    "#     # - Example 1...\n",
    "#     # - Example 2...\n",
    "#     # [Guiding Questions] Have you considered...? How might this apply to...?\n",
    "#     # [Summary] To recap the key points...\"\"\"\n",
    "\n",
    "#     # Get LLM response\n",
    "#     groq_response = query_groq(prompt)\n",
    "#     print(f\"Response Toke   ns: {count_tokens(groq_response.message.content)}\")\n",
    "\n",
    "#     # Return updated history with new interaction\n",
    "#     return conversation_history + [(user_query, groq_response.message.content)]\n",
    "\n",
    "\n",
    "# # Gradio Interface\n",
    "# with gr.Blocks() as interface:\n",
    "#     gr.Markdown(\"# üßë‚Äçüè´ AI Coaching Assistant\")\n",
    "#     gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "\n",
    "#     # State management\n",
    "#     chat_history = gr.State(conversation_history)\n",
    "\n",
    "#     with gr.Row():\n",
    "#         chatbot = gr.Chatbot(height=500)\n",
    "#         with gr.Column(scale=0.5):\n",
    "#             context_display = gr.Textbox(label=\"Relevant Context\", interactive=False)\n",
    "\n",
    "#     user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "\n",
    "#     with gr.Row():\n",
    "#         submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "#         undo_btn = gr.Button(\"Undo Last\")\n",
    "#         clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "#     def handle_submit(user_input, history):\n",
    "#         if not user_input.strip():\n",
    "#             return gr.update(), history, \"\"\n",
    "\n",
    "#         # Process query and update history\n",
    "#         new_history = process_user_query(user_input, history)\n",
    "\n",
    "#         # Get latest context for display\n",
    "#         latest_context = \"\\n\".join(\n",
    "#             [\n",
    "#                 chunk[\"metadata\"][\"text\"]\n",
    "#                 for chunk in query_pinecone(get_embedding(user_input))\n",
    "#             ][:]\n",
    "#         )  # Show top 3 context snippets\n",
    "\n",
    "#         return \"\", new_history, latest_context\n",
    "\n",
    "#     # Component interactions\n",
    "#     submit_btn.click(\n",
    "#         handle_submit,\n",
    "#         [user_input, chat_history],\n",
    "#         [user_input, chat_history, context_display],\n",
    "#     ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "#     # Add submit on Enter key press\n",
    "#     user_input.submit(\n",
    "#         handle_submit,\n",
    "#         [user_input, chat_history],\n",
    "#         [user_input, chat_history, context_display],\n",
    "#     ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "#     undo_btn.click(\n",
    "#         lambda history: history[:-1] if history else [], [chat_history], [chat_history]\n",
    "#     ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "#     clear_btn.click(lambda: [], None, [chat_history]).then(\n",
    "#         lambda: ([], \"\"), None, [chatbot, context_display]\n",
    "#     )\n",
    "\n",
    "# interface.launch(share=True)\n",
    "\n",
    "# ------------------------- Gradio converted into Gradio Function -------------------------------------------------------\n",
    "# Initialize empty conversation history (list of tuples)\n",
    "# conversation_history = []\n",
    "\n",
    "\n",
    "# @track_time\n",
    "# def process_user_query(user_query: str, conversation_history: list):\n",
    "#     print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "#     # Generate embedding and get relevant context\n",
    "#     embedding = get_embedding(user_query)\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     context = \"\\n\".join(chunk[\"metadata\"][\"text\"] for chunk in relevant_chunks)\n",
    "\n",
    "#     # Format conversation history for the prompt\n",
    "#     history_str = \"\\n\".join(\n",
    "#         f\"User: {user}\\nCoach: {response}\" for user, response in conversation_history\n",
    "#     )\n",
    "\n",
    "#     # Create structured prompt\n",
    "#     prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow these guidelines carefully:\n",
    "\n",
    "#     1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "#     2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "#     3. Ask guiding questions to encourage critical thinking.\n",
    "#     4. Adapt your explanation to match the student's knowledge level.\n",
    "#     5. Strictly use terminologies provided in the given context.\n",
    "#     6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "#     7. Only answer based on the provided context‚Äîdo not speculate or include external information.\n",
    "#     8. Always provide all specific relevant sources from the context in your responses urls, video names, video timestamps , links , resources , ebook names, lesson names , lesson numbers and anything else you think would be relevant to the user query.\n",
    "#     9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "#     10. Must provide all relevant video timestamp from where to start watching and where to end watching\n",
    "#     Context from learning materials:\n",
    "#     {context}\n",
    "\n",
    "#     Conversation history:\n",
    "#     {history_str}\n",
    "\n",
    "#     New student question:\n",
    "#     \"{user_query}\"\n",
    "\n",
    "#     Provide a thoughtful and contextually accurate response now:\"\"\"\n",
    "\n",
    "#     # Get LLM response\n",
    "#     groq_response = query_groq(prompt)\n",
    "#     print(f\"Response Tokens: {count_tokens(groq_response.message.content)}\")\n",
    "\n",
    "#     # Return updated history with new interaction\n",
    "#     return conversation_history + [(user_query, groq_response.message.content)]\n",
    "\n",
    "\n",
    "# @track_time\n",
    "# def create_gradio_interface(conversation_history):\n",
    "#     with gr.Blocks() as interface:\n",
    "#         gr.Markdown(\"# üßë‚Äçüè´ AI Coaching Assistant\")\n",
    "#         gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "\n",
    "#         # State management\n",
    "#         chat_history = gr.State(conversation_history)\n",
    "\n",
    "#         with gr.Row():\n",
    "#             chatbot = gr.Chatbot(height=500)\n",
    "#             with gr.Column(scale=0.5):\n",
    "#                 context_display = gr.Textbox(\n",
    "#                     label=\"Relevant Context\", interactive=False\n",
    "#                 )\n",
    "\n",
    "#         user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "\n",
    "#         with gr.Row():\n",
    "#             submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "#             undo_btn = gr.Button(\"Undo Last\")\n",
    "#             clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "#         def handle_submit(user_input, history):\n",
    "#             if not user_input.strip():\n",
    "#                 return gr.update(), history, \"\"\n",
    "\n",
    "#             # Process query and update history\n",
    "#             new_history = process_user_query(user_input, history)\n",
    "\n",
    "#             # Get latest context for display\n",
    "#             latest_context = \"\\n\".join(\n",
    "#                 [\n",
    "#                     chunk[\"metadata\"][\"text\"]\n",
    "#                     for chunk in query_pinecone(get_embedding(user_input))\n",
    "#                 ][:]\n",
    "#             )\n",
    "\n",
    "#             return \"\", new_history, latest_context\n",
    "\n",
    "#         # Component interactions\n",
    "#         submit_btn.click(\n",
    "#             handle_submit,\n",
    "#             [user_input, chat_history],\n",
    "#             [user_input, chat_history, context_display],\n",
    "#         ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "#         # Add submit on Enter key press\n",
    "#         user_input.submit(\n",
    "#             handle_submit,\n",
    "#             [user_input, chat_history],\n",
    "#             [user_input, chat_history, context_display],\n",
    "#         ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "#         undo_btn.click(\n",
    "#             lambda history: history[:-1] if history else [],\n",
    "#             [chat_history],\n",
    "#             [chat_history],\n",
    "#         ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "#         clear_btn.click(lambda: [], None, [chat_history]).then(\n",
    "#             lambda: ([], \"\"), None, [chatbot, context_display]\n",
    "#         )\n",
    "\n",
    "#     return interface\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Initialize conversation history\n",
    "#     initial_conversation_history = []\n",
    "\n",
    "#     # Create and launch the interface\n",
    "#     interface = create_gradio_interface(initial_conversation_history)\n",
    "#     interface.launch(share=True)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq and Gradio with Streaming Enabled\n",
    "\n",
    "- ### i.e. will start showing text as soon as it gets generated from groq inference\n",
    "- ### faster than optimized version\n",
    "\n",
    "## Query:\n",
    "- pediatic surgery\n",
    "## Response Time:\n",
    "User Query Tokens: 6\n",
    "[Time Tracker] `get_embedding` took 0.4752 seconds\n",
    "[Time Tracker] `query_pinecone` took 0.2222 seconds\n",
    "[Time Tracker] `query_groq` took 0.5060 seconds\n",
    "\n",
    "Total time: 1.19 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_20384\\2806160723.py:97: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(height=500)\n",
      "d:\\Disrupt Labs\\Rag Experiments\\env\\Lib\\site-packages\\gradio\\layouts\\column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `create_gradio_interface` took 0.1209 seconds\n",
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://26250d83b41b1ff906.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://26250d83b41b1ff906.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.8142 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.5735 seconds\n",
      "[Time Tracker] `query_groq` took 0.5627 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.5136 seconds\n",
      "[Time Tracker] `query_pinecone` took 0.2343 seconds\n",
      "[Time Tracker] `query_groq` took 0.5483 seconds\n"
     ]
    }
   ],
   "source": [
    "# # Modified query_groq function with more explicit streaming handling\n",
    "# @track_time\n",
    "# def query_groq(prompt):\n",
    "#     client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "#     # Always use streaming mode\n",
    "#     return client.chat.completions.create(\n",
    "#         model=\"llama3-70b-8192\",  # or whichever model you're using\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         stream=True,\n",
    "#     )\n",
    "\n",
    "# --------------------------------------------------------- ## Groq and Gradio with Streaming Enabled -----------------------------------------------------\n",
    "# Modified process_user_query to properly yield streaming updates\n",
    "@track_time\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk[\"metadata\"][\"text\"] for chunk in relevant_chunks)\n",
    "\n",
    "    # Format conversation history for the prompt\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\" for user, response in conversation_history\n",
    "    )\n",
    "\n",
    "    # Create structured prompt\n",
    "    prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow these guidelines carefully:\n",
    "\n",
    "    1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "    2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "    3. Ask guiding questions to encourage critical thinking.\n",
    "    4. Adapt your explanation to match the student's knowledge level.\n",
    "    5. Strictly use terminologies provided in the given context.\n",
    "    6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "    7. Only answer based on the provided context‚Äîdo not speculate or include external information.\n",
    "    8. Always provide all specific relevant sources from the context in your responses urls, video names, video timestamps , links , resources , ebook names, lesson names , lesson numbers and anything else you think would be relevant to the user query.\n",
    "    9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "    10. Must provide all relevant video timestamp from where to start watching and where to end watching \n",
    "    Context from learning materials:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "\n",
    "    New student question:\n",
    "    \"{user_query}\"\n",
    "    \n",
    "    Provide a thoughtful and contextually accurate response now:\"\"\"\n",
    "\n",
    "    # Get streaming LLM response\n",
    "    stream_response = query_groq(prompt)\n",
    "\n",
    "    # The function now directly yields the stream chunks for the Gradio interface to use\n",
    "    full_response = \"\"\n",
    "\n",
    "    # First, yield a response with empty text to set up the message\n",
    "    # This creates the user message immediately\n",
    "    temp_history = conversation_history.copy()\n",
    "    temp_history.append((user_query, \"\"))\n",
    "    yield temp_history, context\n",
    "\n",
    "    # Process the stream\n",
    "    for chunk in stream_response:\n",
    "        if (\n",
    "            hasattr(chunk.choices[0].delta, \"content\")\n",
    "            and chunk.choices[0].delta.content is not None\n",
    "        ):\n",
    "            content_chunk = chunk.choices[0].delta.content\n",
    "            full_response += content_chunk\n",
    "\n",
    "            # Create a temporary history with the current response\n",
    "            temp_history = conversation_history.copy()\n",
    "            temp_history.append((user_query, full_response))\n",
    "\n",
    "            # Yield the updated history for display\n",
    "            yield temp_history, context\n",
    "\n",
    "    # Return the final history with the complete response\n",
    "    final_history = conversation_history.copy()\n",
    "    final_history.append((user_query, full_response))\n",
    "    yield final_history, context\n",
    "\n",
    "\n",
    "@track_time\n",
    "def create_gradio_interface(conversation_history):\n",
    "    with gr.Blocks() as interface:\n",
    "        gr.Markdown(\"# üßë‚Äçüè´ AI Coaching Assistant\")\n",
    "        gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "\n",
    "        # State management\n",
    "        chat_history = gr.State(conversation_history)\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "            with gr.Column(scale=0.5):\n",
    "                context_display = gr.Textbox(\n",
    "                    label=\"Relevant Context\", interactive=False\n",
    "                )\n",
    "\n",
    "        user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            undo_btn = gr.Button(\"Undo Last\")\n",
    "            clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "        def handle_submit(user_query, history):\n",
    "            if not user_query.strip():\n",
    "                return gr.update(), history, \"\"\n",
    "\n",
    "            # Use the generator directly from process_user_query\n",
    "            # This will yield incremental updates as they arrive\n",
    "            response_generator = process_user_query(user_query, history)\n",
    "\n",
    "            for updated_history, context in response_generator:\n",
    "                # Directly update the chatbot with each streaming chunk\n",
    "                yield \"\", updated_history, context, updated_history\n",
    "\n",
    "        # Component interactions with streaming support\n",
    "        submit_btn.click(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        # Add submit on Enter key press\n",
    "        user_input.submit(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        undo_btn.click(\n",
    "            lambda history: history[:-1] if history else [],\n",
    "            [chat_history],\n",
    "            [chat_history],\n",
    "        ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "        clear_btn.click(lambda: [], None, [chat_history]).then(\n",
    "            lambda: ([], \"\"), None, [chatbot, context_display]\n",
    "        )\n",
    "\n",
    "    return interface\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Initialize conversation history\n",
    "#     initial_conversation_history = []\n",
    "\n",
    "#     # Create and launch the interface\n",
    "#     interface = create_gradio_interface(initial_conversation_history)\n",
    "#     interface.launch(share=True)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for the application.\n",
    "\n",
    "    Initializes the conversation history with a welcome message,\n",
    "    creates the Gradio interface, and launches the web app.\n",
    "    \"\"\"\n",
    "    # Initialize conversation history with welcome message\n",
    "    welcome_message = \"Hi there! I'm your AI coach. I can help answer questions about your course materials, explain difficult concepts, and guide your learning journey. What would you like to know today?\"\n",
    "    initial_conversation_history = [(\"\", welcome_message)]\n",
    "\n",
    "    # Create and launch the interface\n",
    "    interface = create_gradio_interface(initial_conversation_history)\n",
    "    interface.launch(share=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Version\n",
    "- ### slower than previous version\n",
    "- ### slightly better responses and source citations\n",
    "\n",
    "## Query:\n",
    "- pediatic surgery\n",
    "## Response Time:\n",
    "User Query Tokens: 7\n",
    "[Time Tracker] `get_embedding` took 0.7417 seconds\n",
    "[Time Tracker] `query_pinecone` took 1.4344 seconds\n",
    "[Time Tracker] `query_groq` took 0.4740 seconds\n",
    "\n",
    "Total time = 2.64 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama\\AppData\\Local\\Temp\\ipykernel_20384\\2842305968.py:174: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(value=conversation_history, height=500)\n",
      "d:\\Disrupt Labs\\Rag Experiments\\env\\Lib\\site-packages\\gradio\\layouts\\column.py:55: UserWarning: 'scale' value should be an integer. Using 0.5 will cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `create_gradio_interface` took 0.1014 seconds\n",
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* Running on public URL: https://8eb664d3e4cd4dc734.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8eb664d3e4cd4dc734.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.9302 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.4134 seconds\n",
      "[Time Tracker] `query_groq` took 0.5820 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0001 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.6390 seconds\n",
      "[Time Tracker] `query_pinecone` took 0.2117 seconds\n",
      "[Time Tracker] `query_groq` took 0.5676 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0001 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.6471 seconds\n",
      "[Time Tracker] `query_pinecone` took 0.2543 seconds\n",
      "[Time Tracker] `query_groq` took 0.6574 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.5009 seconds\n",
      "[Time Tracker] `query_pinecone` took 0.2117 seconds\n",
      "[Time Tracker] `query_groq` took 0.5695 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.7810 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.4718 seconds\n",
      "[Time Tracker] `query_groq` took 0.5801 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.6329 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.2848 seconds\n",
      "[Time Tracker] `query_groq` took 0.5497 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.4982 seconds\n",
      "[Time Tracker] `query_pinecone` took 0.2154 seconds\n",
      "[Time Tracker] `query_groq` took 0.5630 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.6371 seconds\n",
      "[Time Tracker] `query_pinecone` took 0.2028 seconds\n",
      "[Time Tracker] `query_groq` took 0.6182 seconds\n",
      "[Time Tracker] `process_user_query` took 0.0000 seconds\n",
      "[Time Tracker] `count_tokens` took 0.0002 seconds\n",
      "User Query Tokens: 4\n",
      "[Time Tracker] `get_embedding` took 0.8330 seconds\n",
      "[Time Tracker] `query_pinecone` took 1.3916 seconds\n",
      "[Time Tracker] `query_groq` took 0.5419 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "-- Optimized Version ---\n",
    "Streaming AI Coaching Assistant\n",
    "\n",
    "This module implements a Gradio-based chatbot that uses streaming responses from Groq API\n",
    "to provide real-time feedback to the user. It uses RAG (Retrieval-Augmented Generation)\n",
    "to provide contextually relevant answers from a knowledge base.\n",
    "\n",
    "Key components:\n",
    "- query_groq: Interfaces with Groq API for streaming LLM responses\n",
    "- process_user_query: Manages the streaming process and context retrieval \n",
    "- create_gradio_interface: Creates and configures the Gradio UI\n",
    "\n",
    "The streaming implementation uses Python generators (with yield statements) to provide\n",
    "real-time updates to the Gradio UI as tokens arrive from the LLM, rather than waiting\n",
    "for the complete response.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Modified query_groq function with optimized streaming\n",
    "@track_time\n",
    "def query_groq(prompt):\n",
    "    \"\"\"\n",
    "    Send a prompt to Groq API with streaming enabled.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The prompt to send to the LLM\n",
    "\n",
    "    Returns:\n",
    "        A stream of response chunks from the Groq API\n",
    "    \"\"\"\n",
    "    client = Groq(api_key=os.environ[\"GROQ_API_KEY\"])\n",
    "\n",
    "    # Always use streaming mode\n",
    "    return client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",  # or whichever model you're using\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "        # Add parameters to potentially improve response time\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_tokens=2048,  # Adjust based on your needs\n",
    "    )\n",
    "\n",
    "\n",
    "# Optimized process_user_query with better streaming performance\n",
    "@track_time\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    \"\"\"\n",
    "    Process a user query using streaming responses.\n",
    "\n",
    "    This generator function:\n",
    "    1. Retrieves relevant context from Pinecone\n",
    "    2. Sends the query to Groq\n",
    "    3. Yields updates as chunks arrive from the LLM\n",
    "\n",
    "    Args:\n",
    "        user_query (str): The user's question\n",
    "        conversation_history (list): List of previous (query, response) tuples\n",
    "\n",
    "    Yields:\n",
    "        Tuples of (updated_history, context) as response chunks arrive\n",
    "    \"\"\"\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context - do this once upfront\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk[\"metadata\"][\"text\"] for chunk in relevant_chunks)\n",
    "\n",
    "    # Format conversation history for the prompt - do this once\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\" for user, response in conversation_history\n",
    "    )\n",
    "\n",
    "    # Create structured prompt\n",
    "    prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow these guidelines carefully:\n",
    "\n",
    "    1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "    2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "    3. Ask guiding questions to encourage critical thinking.\n",
    "    4. Adapt your explanation to match the student's knowledge level.\n",
    "    5. Strictly use terminologies provided in the given context.\n",
    "    6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "    7. Only answer based on the provided context‚Äîdo not speculate or include external information.\n",
    "    8. Always provide all specific relevant sources from the context in your responses urls, video names, video timestamps, links, resources, ebook names, lesson names, lesson numbers and anything else you think would be relevant to the user query.\n",
    "    9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "    10. Must provide all relevant video timestamp from where to start watching and where to end watching \n",
    "    Context from learning materials:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "\n",
    "    New student question:\n",
    "    \"{user_query}\"\n",
    "    \n",
    "    Provide a thoughtful and contextually accurate response now:\"\"\"\n",
    "\n",
    "    # Get streaming LLM response\n",
    "    stream_response = query_groq(prompt)\n",
    "\n",
    "    # Initialize response\n",
    "    full_response = \"\"\n",
    "\n",
    "    # First, yield a response with empty text to set up the message\n",
    "    # This creates the user message immediately\n",
    "    temp_history = conversation_history.copy()\n",
    "    temp_history.append((user_query, \"\"))\n",
    "    yield temp_history, context\n",
    "\n",
    "    # For efficiency, create these objects once outside the loop\n",
    "    buffer = \"\"\n",
    "    buffer_size = 10  # Characters to buffer before updating UI\n",
    "    update_frequency = 0  # Counter to track updates\n",
    "\n",
    "    # Process the stream\n",
    "    for chunk in stream_response:\n",
    "        if (\n",
    "            hasattr(chunk.choices[0].delta, \"content\")\n",
    "            and chunk.choices[0].delta.content is not None\n",
    "        ):\n",
    "            content_chunk = chunk.choices[0].delta.content\n",
    "            buffer += content_chunk\n",
    "            full_response += content_chunk\n",
    "\n",
    "            # Only update UI after buffer reaches certain size or every few chunks\n",
    "            # This reduces the number of updates while still maintaining responsiveness\n",
    "            update_frequency += 1\n",
    "            if len(buffer) >= buffer_size or update_frequency >= 5:\n",
    "                temp_history = conversation_history.copy()\n",
    "                temp_history.append((user_query, full_response))\n",
    "                yield temp_history, context\n",
    "                buffer = \"\"\n",
    "                update_frequency = 0\n",
    "\n",
    "    # Ensure final state is yielded if buffer has content\n",
    "    if buffer or update_frequency > 0:\n",
    "        final_history = conversation_history.copy()\n",
    "        final_history.append((user_query, full_response))\n",
    "        yield final_history, context\n",
    "\n",
    "\n",
    "@track_time\n",
    "def create_gradio_interface(conversation_history):\n",
    "    \"\"\"\n",
    "    Create a Gradio interface for the coaching assistant.\n",
    "\n",
    "    This function sets up the UI components and defines how they interact.\n",
    "    The interface includes real-time streaming of LLM responses.\n",
    "\n",
    "    Args:\n",
    "        conversation_history (list): Initial conversation history\n",
    "\n",
    "    Returns:\n",
    "        gr.Blocks: The configured Gradio interface\n",
    "    \"\"\"\n",
    "    # Welcome message to display when a new user starts\n",
    "    welcome_message = \"Hi there! I'm your AI learning coach. I can help answer questions about your course materials, explain difficult concepts, and guide your learning journey. What would you like to know today?\"\n",
    "\n",
    "    # Add initial welcome message to conversation history if it's empty\n",
    "    if not conversation_history:\n",
    "        conversation_history = [(\"\", welcome_message)]\n",
    "\n",
    "    with gr.Blocks() as interface:\n",
    "        gr.Markdown(\"# üßë‚Äçüè´ AI Coaching Assistant\")\n",
    "        gr.Markdown(\n",
    "            \"Ask questions about your course materials and get real-time, personalized help.\"\n",
    "        )\n",
    "\n",
    "        # State management\n",
    "        chat_history = gr.State(conversation_history)\n",
    "\n",
    "        with gr.Row():\n",
    "            chatbot = gr.Chatbot(value=conversation_history, height=500)\n",
    "            with gr.Column(scale=0.5):\n",
    "                context_display = gr.Textbox(\n",
    "                    label=\"Relevant Learning Materials\", interactive=False, visible=True\n",
    "                )\n",
    "\n",
    "        user_input = gr.Textbox(\n",
    "            label=\"Your Question\",\n",
    "            placeholder=\"Type your question here and press Enter...\",\n",
    "            autofocus=True,\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "            undo_btn = gr.Button(\"Undo Last\")\n",
    "            clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "        def handle_submit(user_query, history):\n",
    "            \"\"\"\n",
    "            Handle user query submission with streaming response.\n",
    "\n",
    "            This generator function processes the user query and yields\n",
    "            updates to the UI as they become available.\n",
    "\n",
    "            Args:\n",
    "                user_query (str): The user's question\n",
    "                history (list): Current conversation history\n",
    "\n",
    "            Yields:\n",
    "                Updates to the UI components\n",
    "            \"\"\"\n",
    "            if not user_query.strip():\n",
    "                return gr.update(), history, \"\"\n",
    "\n",
    "            # Use the generator directly from process_user_query\n",
    "            # This will yield incremental updates as they arrive\n",
    "            response_generator = process_user_query(user_query, history)\n",
    "\n",
    "            for updated_history, context in response_generator:\n",
    "                # Directly update the chatbot with each streaming chunk\n",
    "                yield \"\", updated_history, context, updated_history\n",
    "\n",
    "        # Component interactions with streaming support\n",
    "        submit_btn.click(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        # Add submit on Enter key press\n",
    "        user_input.submit(\n",
    "            handle_submit,\n",
    "            [user_input, chat_history],\n",
    "            [user_input, chat_history, context_display, chatbot],\n",
    "        )\n",
    "\n",
    "        undo_btn.click(\n",
    "            lambda history: history[:-1] if history else [],\n",
    "            [chat_history],\n",
    "            [chat_history],\n",
    "        ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "        clear_btn.click(lambda: [(\"\", welcome_message)], None, [chat_history]).then(\n",
    "            lambda x: x, [chat_history], [chatbot]\n",
    "        ).then(lambda: \"\", None, [context_display])\n",
    "\n",
    "    return interface\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for the application.\n",
    "\n",
    "    Initializes the conversation history with a welcome message,\n",
    "    creates the Gradio interface, and launches the web app.\n",
    "    \"\"\"\n",
    "    # Initialize conversation history with welcome message\n",
    "    welcome_message = \"Hi there! I'm your AI learning coach. I can help answer questions about your course materials, explain difficult concepts, and guide your learning journey. What would you like to know today?\"\n",
    "    initial_conversation_history = [(\"\", welcome_message)]\n",
    "\n",
    "    # Create and launch the interface\n",
    "    interface = create_gradio_interface(initial_conversation_history)\n",
    "    interface.launch(share=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3 -- Working well but total time is incorrect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gradio as gr\n",
    "from groq import Groq\n",
    "# Make sure you have these implemented elsewhere:\n",
    "#   get_embedding(text: str) -> List[float]\n",
    "#   query_pinecone(embedding: List[float]) -> List[{\"metadata\": {\"text\": str}}]\n",
    "#   count_tokens(text: str) -> int\n",
    "\n",
    "# ‚Äî track_time decorator (unchanged, accumulates total) ‚Äî\n",
    "_total_tracked = 0.0\n",
    "\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        global _total_tracked\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        _total_tracked += elapsed\n",
    "        print(f\"[Time Tracker] {func.__name__} took {elapsed:.4f} seconds\")\n",
    "        print(f\"[Time Tracker] total accumulated: {_total_tracked:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# Groq streaming query\n",
    "@track_time\n",
    "def query_groq(prompt: str):\n",
    "    client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\", \"\"))\n",
    "    return client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_tokens=2048,\n",
    "    )\n",
    "\n",
    "\n",
    "@track_time\n",
    "def process_user_query(user_query: str, history: list):\n",
    "    # 1) Show user message immediately\n",
    "    temp_history = history + [(user_query, \"\")]\n",
    "    yield temp_history, \"\"\n",
    "\n",
    "    # 2) Retrieval: embeddings + Pinecone\n",
    "    print(f\"Token count: {count_tokens(user_query)}\")\n",
    "    emb = get_embedding(user_query)\n",
    "    chunks = query_pinecone(emb)\n",
    "    context = \"\\n\".join(c[\"metadata\"][\"text\"] for c in chunks)\n",
    "\n",
    "    # 3) Build prompt once\n",
    "    hist_str = \"\\n\".join(f\"User: {u}\\nCoach: {r}\" for u, r in history)\n",
    "    prompt = f\"\"\"You are an expert coach. Follow these rules:\n",
    "1. Step-by-step explanations\n",
    "2. Chain-of-thought reasoning\n",
    "3. Guiding questions\n",
    "4. Adapt to student's level\n",
    "5. Use only provided context\n",
    "6. Give 2‚Äì3 examples\n",
    "7. Answer ONLY from context\n",
    "8. Cite videos, timestamps, links, ebooks, lessons\n",
    "9. Adapt sentiment & tone\n",
    "10. Provide all relevant video timestamps\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "History:\n",
    "{hist_str}\n",
    "\n",
    "New question:\n",
    "\"{user_query}\"\n",
    "\n",
    "Respond now:\"\"\"\n",
    "\n",
    "    # 4) Stream and yield per token\n",
    "    full = \"\"\n",
    "    for chunk in query_groq(prompt):\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", None)\n",
    "        if not delta:\n",
    "            continue\n",
    "        full += delta\n",
    "        temp_history[-1] = (user_query, full)\n",
    "        yield temp_history, context\n",
    "\n",
    "\n",
    "@track_time\n",
    "def create_gradio_interface(initial_history):\n",
    "    welcome = \"Hi! I'm your AI coach‚Äîask me anything about your course materials.\"\n",
    "    if not initial_history:\n",
    "        initial_history = [(\"\", welcome)]\n",
    "\n",
    "    # CSS for modern dark UI\n",
    "    css = \"\"\"\n",
    "    body { background: #1e1e2e; color: #e0e0e0; }\n",
    "    .header { text-align:center; padding:1rem 0; }\n",
    "    .header h1 { margin:0; font-size:2.5rem; }\n",
    "    .header p { margin:0.25rem 0 1rem; color:#a0a0a0; }\n",
    "\n",
    "    .chat-sidebar { display:grid; grid-template-columns:3fr 1fr; gap:1rem; padding:1rem; }\n",
    "    .gr-chatbot { background:#2a2a3b !important; border-radius:12px !important; }\n",
    "    .sidebar { background:#2a2a3b; border-radius:12px; padding:1rem; }\n",
    "    .sidebar h3 { margin-top:0; color:#fff; }\n",
    "    .context-box { background:#1e1e2e; border:1px solid #3b3b4f; border-radius:8px; color:#e0e0e0; padding:0.5rem; }\n",
    "    .time-badge { margin-top:1rem; padding:0.5rem 1rem; background:#ff825c; color:#fff; font-weight:500; text-align:center; border-radius:8px; }\n",
    "\n",
    "    .input-area { display:flex; gap:0.5rem; padding:1rem; align-items:stretch; }\n",
    "    .input-area .gr-textbox { flex:1; background:#2a2a3b; border:1px solid #3b3b4f; border-radius:12px; color:#e0e0e0; padding:0.75rem 1rem; font-size:1rem; min-height:4rem; }\n",
    "    .input-area .gr-button { border-radius:12px; padding:0 1.5rem; font-weight:600; transition:background 0.2s ease; }\n",
    "    .submit { background:#ff825c !important; color:#fff !important; }\n",
    "    .submit:hover { background:#e06d45 !important; }\n",
    "    .undo, .clear { background:#44475a !important; color:#fff !important; }\n",
    "    .undo:hover, .clear:hover { background:#3b3b4f !important; }\n",
    "    .message .timestamp { display:none !important; }\n",
    "    \"\"\"\n",
    "\n",
    "    with gr.Blocks(css=css) as demo:\n",
    "        # Header\n",
    "        gr.HTML(\n",
    "            \"<div class='header'><h1>üßë‚Äçüè´ AI Coaching Assistant</h1>\"\n",
    "            \"<p>Your personal learning coach with real-time guidance</p></div>\"\n",
    "        )\n",
    "\n",
    "        history_state = gr.State(initial_history)\n",
    "\n",
    "        # Chat & Sidebar\n",
    "        with gr.Row(elem_classes=\"chat-sidebar\"):\n",
    "            chatbot = gr.Chatbot(value=initial_history, height=550)\n",
    "            with gr.Column(elem_classes=\"sidebar\"):\n",
    "                gr.HTML(\"<h3>Relevant Learning Materials</h3>\")\n",
    "                context_box = gr.Textbox(\n",
    "                    interactive=False,\n",
    "                    lines=15,\n",
    "                    elem_classes=\"context-box\",\n",
    "                    show_label=False,\n",
    "                )\n",
    "                time_display = gr.HTML(\"<div class='time-badge'>‚è± 0.0000s</div>\")\n",
    "\n",
    "        # Input row\n",
    "        with gr.Row(elem_classes=\"input-area\"):\n",
    "            user_input = gr.Textbox(\n",
    "                placeholder=\"Type your question here and press Enter‚Ä¶\",\n",
    "                show_label=False,\n",
    "                autofocus=True,\n",
    "            )\n",
    "            submit_btn = gr.Button(\"Submit\", elem_classes=\"submit\")\n",
    "            undo_btn = gr.Button(\"Undo\", elem_classes=\"undo\")\n",
    "            clear_btn = gr.Button(\"Clear\", elem_classes=\"clear\")\n",
    "\n",
    "        # Submission logic: track until first token\n",
    "        def handle_submit(q, hist):\n",
    "            if not q.strip():\n",
    "                return (\n",
    "                    gr.update(),\n",
    "                    hist,\n",
    "                    \"\",\n",
    "                    hist,\n",
    "                    \"<div class='time-badge'>‚è± 0.0000s</div>\",\n",
    "                )\n",
    "            start = time.perf_counter()\n",
    "            first_time = None\n",
    "            for new_hist, ctx in process_user_query(q, hist):\n",
    "                # on first non-empty response, record time\n",
    "                resp = new_hist[-1][1]\n",
    "                if first_time is None and resp:\n",
    "                    first_time = time.perf_counter() - start\n",
    "                badge = (\n",
    "                    f\"<div class='time-badge'>‚è± {first_time:.4f}s</div>\"\n",
    "                    if first_time is not None\n",
    "                    else \"<div class='time-badge'>‚è± ...</div>\"\n",
    "                )\n",
    "                yield \"\", new_hist, ctx, new_hist, badge\n",
    "\n",
    "        submit_btn.click(\n",
    "            handle_submit,\n",
    "            [user_input, history_state],\n",
    "            [user_input, history_state, context_box, chatbot, time_display],\n",
    "        )\n",
    "        user_input.submit(\n",
    "            handle_submit,\n",
    "            [user_input, history_state],\n",
    "            [user_input, history_state, context_box, chatbot, time_display],\n",
    "        )\n",
    "\n",
    "        undo_btn.click(\n",
    "            lambda h: h[:-1] if h else [], [history_state], [history_state]\n",
    "        ).then(lambda x: x, [history_state], [chatbot])\n",
    "\n",
    "        clear_btn.click(lambda: [(\"\", welcome)], None, [history_state]).then(\n",
    "            lambda x: x, [history_state], [chatbot]\n",
    "        )\n",
    "        clear_btn.click(lambda: \"\", None, [context_box])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "def main():\n",
    "    initial = [\n",
    "        (\"\", \"Hi! I'm your AI coach‚Äîask me anything about your course materials.\")\n",
    "    ]\n",
    "    demo = create_gradio_interface(initial)\n",
    "    demo.launch(share=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import gradio as gr\n",
    "from groq import Groq\n",
    "# Make sure these are implemented elsewhere:\n",
    "#   get_embedding(), query_pinecone(), count_tokens()\n",
    "\n",
    "# -- track_time decorator --\n",
    "_total_tracked = 0.0\n",
    "\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        global _total_tracked\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        _total_tracked += elapsed\n",
    "        print(f\"[Time Tracker] {func.__name__} took {elapsed:.4f} seconds\")\n",
    "        print(f\"[Time Tracker] total accumulated: {_total_tracked:.4f} seconds\")\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# -- Core AI Functions --\n",
    "@track_time\n",
    "def query_groq(prompt: str):\n",
    "    client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\", \"\"))\n",
    "    return client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        max_tokens=2048,\n",
    "    )\n",
    "\n",
    "\n",
    "@track_time\n",
    "def process_user_query(user_query: str, history: list):\n",
    "    # Immediate user message display\n",
    "    temp_history = history + [(user_query, \"\")]\n",
    "    yield temp_history, \"\"\n",
    "\n",
    "    # Context retrieval\n",
    "    emb = get_embedding(user_query)\n",
    "    chunks = query_pinecone(emb)\n",
    "    context = \"\\n\".join(c[\"metadata\"][\"text\"] for c in chunks)\n",
    "\n",
    "    # Prompt construction\n",
    "    hist_str = \"\\n\".join(f\"User: {u}\\nCoach: {r}\" for u, r in history)\n",
    "    prompt = f\"\"\"You are an expert coach. Follow these rules:\n",
    "1. Step-by-step explanations\n",
    "2. Chain-of-thought reasoning\n",
    "3. Guiding questions\n",
    "4. Adapt to student's level\n",
    "5. Use only provided context\n",
    "6. Give 2‚Äì3 examples\n",
    "7. Answer ONLY from context\n",
    "8. Cite videos, timestamps, links, ebooks, lessons\n",
    "9. Adapt sentiment & tone\n",
    "10. Provide all relevant video timestamps\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "History:\n",
    "{hist_str}\n",
    "\n",
    "New question:\n",
    "\"{user_query}\"\n",
    "\n",
    "Respond now:\"\"\"\n",
    "\n",
    "    # Buffered streaming response\n",
    "    full_response = \"\"\n",
    "    buffer = \"\"\n",
    "    buffer_size = 12  # Characters per chunk\n",
    "    update_counter = 0\n",
    "\n",
    "    for chunk in query_groq(prompt):\n",
    "        delta = getattr(chunk.choices[0].delta, \"content\", \"\")\n",
    "        if delta:\n",
    "            full_response += delta\n",
    "            buffer += delta\n",
    "            update_counter += 1\n",
    "\n",
    "            # Update UI when buffer fills or every 3 chunks\n",
    "            if len(buffer) >= buffer_size or update_counter >= 3:\n",
    "                temp_history[-1] = (user_query, full_response)\n",
    "                yield temp_history, context\n",
    "                buffer = \"\"\n",
    "                update_counter = 0\n",
    "\n",
    "    # Final update for remaining content\n",
    "    if buffer or update_counter > 0:\n",
    "        temp_history[-1] = (user_query, full_response)\n",
    "        yield temp_history, context\n",
    "\n",
    "\n",
    "# -- Gradio Interface --\n",
    "@track_time\n",
    "def create_gradio_interface(initial_history):\n",
    "    welcome = \"Hi! I'm your AI coach‚Äîask me anything about your course materials.\"\n",
    "    if not initial_history:\n",
    "        initial_history = [(\"\", welcome)]\n",
    "\n",
    "    # Modern dark UI CSS\n",
    "    css = \"\"\"\n",
    "    body { background: #1e1e2e; color: #e0e0e0; }\n",
    "    .header { text-align:center; padding:1rem 0; }\n",
    "    .header h1 { margin:0; font-size:2.5rem; }\n",
    "    .header p { margin:0.25rem 0 1rem; color:#a0a0a0; }\n",
    "\n",
    "    .chat-sidebar { display:grid; grid-template-columns:3fr 1fr; gap:1rem; padding:1rem; }\n",
    "    .gr-chatbot { background:#2a2a3b !important; border-radius:12px !important; }\n",
    "    .sidebar { background:#2a2a3b; border-radius:12px; padding:1rem; }\n",
    "    .sidebar h3 { margin-top:0; color:#fff; }\n",
    "    .context-box { background:#1e1e2e; border:1px solid #3b3b4f; border-radius:8px; color:#e0e0e0; padding:0.5rem; }\n",
    "    .time-badge { margin-top:1rem; padding:0.5rem 1rem; background:#ff825c; color:#fff; font-weight:500; text-align:center; border-radius:8px; }\n",
    "\n",
    "    .input-area { display:flex; gap:0.5rem; padding:1rem; align-items:stretch; }\n",
    "    .input-area .gr-textbox { flex:1; background:#2a2a3b; border:1px solid #3b3b4f; border-radius:12px; color:#e0e0e0; padding:0.75rem 1rem; font-size:1rem; min-height:4rem; }\n",
    "    .input-area .gr-button { border-radius:12px; padding:0 1.5rem; font-weight:600; transition:background 0.2s ease; }\n",
    "    .submit { background:#ff825c !important; color:#fff !important; }\n",
    "    .submit:hover { background:#e06d45 !important; }\n",
    "    .undo, .clear { background:#44475a !important; color:#fff !important; }\n",
    "    .undo:hover, .clear:hover { background:#3b3b4f !important; }\n",
    "    .message .timestamp { display:none !important; }\n",
    "    \"\"\"\n",
    "\n",
    "    with gr.Blocks(css=css) as demo:\n",
    "        # Header\n",
    "        gr.HTML(\n",
    "            \"<div class='header'><h1>üßë‚Äçüè´ AI Coaching Assistant</h1>\"\n",
    "            \"<p>Your personal learning coach with real-time guidance</p></div>\"\n",
    "        )\n",
    "\n",
    "        history_state = gr.State(initial_history)\n",
    "\n",
    "        # Chat & Sidebar\n",
    "        with gr.Row(elem_classes=\"chat-sidebar\"):\n",
    "            chatbot = gr.Chatbot(value=initial_history, height=550)\n",
    "            with gr.Column(elem_classes=\"sidebar\"):\n",
    "                gr.HTML(\"<h3>Relevant Learning Materials</h3>\")\n",
    "                context_box = gr.Textbox(\n",
    "                    interactive=False,\n",
    "                    lines=15,\n",
    "                    elem_classes=\"context-box\",\n",
    "                    show_label=False,\n",
    "                )\n",
    "                time_display = gr.HTML(\"<div class='time-badge'>‚è± 0.0000s</div>\")\n",
    "\n",
    "        # Input row\n",
    "        with gr.Row(elem_classes=\"input-area\"):\n",
    "            user_input = gr.Textbox(\n",
    "                placeholder=\"Type your question here and press Enter‚Ä¶\",\n",
    "                show_label=False,\n",
    "                autofocus=True,\n",
    "            )\n",
    "            submit_btn = gr.Button(\"Submit\", elem_classes=\"submit\")\n",
    "            undo_btn = gr.Button(\"Undo\", elem_classes=\"undo\")\n",
    "            clear_btn = gr.Button(\"Clear\", elem_classes=\"clear\")\n",
    "\n",
    "        # Submission handler with streaming\n",
    "        def handle_submit(q, hist):\n",
    "            if not q.strip():\n",
    "                return (\n",
    "                    gr.update(),\n",
    "                    hist,\n",
    "                    \"\",\n",
    "                    hist,\n",
    "                    \"<div class='time-badge'>‚è± 0.0000s</div>\",\n",
    "                )\n",
    "\n",
    "            start_time = time.perf_counter()\n",
    "            first_token_received = False\n",
    "\n",
    "            for new_hist, ctx in process_user_query(q, hist):\n",
    "                # Track time to first token\n",
    "                if not first_token_received and new_hist[-1][1]:\n",
    "                    first_token_time = time.perf_counter() - start_time\n",
    "                    first_token_received = True\n",
    "\n",
    "                # Update time badge\n",
    "                badge_time = (\n",
    "                    first_token_time\n",
    "                    if first_token_received\n",
    "                    else (time.perf_counter() - start_time)\n",
    "                )\n",
    "                badge = f\"<div class='time-badge'>‚è± {badge_time:.4f}s</div>\"\n",
    "\n",
    "                yield \"\", new_hist, ctx, new_hist, badge\n",
    "\n",
    "        # Event handlers\n",
    "        submit_btn.click(\n",
    "            handle_submit,\n",
    "            [user_input, history_state],\n",
    "            [user_input, history_state, context_box, chatbot, time_display],\n",
    "        )\n",
    "        user_input.submit(\n",
    "            handle_submit,\n",
    "            [user_input, history_state],\n",
    "            [user_input, history_state, context_box, chatbot, time_display],\n",
    "        )\n",
    "\n",
    "        undo_btn.click(\n",
    "            lambda h: h[:-1] if h else [], [history_state], [history_state]\n",
    "        ).then(lambda x: x, [history_state], [chatbot])\n",
    "\n",
    "        clear_btn.click(lambda: [(\"\", welcome)], None, [history_state]).then(\n",
    "            lambda x: x, [history_state], [chatbot]\n",
    "        )\n",
    "        clear_btn.click(lambda: \"\", None, [context_box])\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "def main():\n",
    "    initial = [\n",
    "        (\"\", \"Hi! I'm your AI coach‚Äîask me anything about your course materials.\")\n",
    "    ]\n",
    "    demo = create_gradio_interface(initial)\n",
    "    demo.launch(share=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
