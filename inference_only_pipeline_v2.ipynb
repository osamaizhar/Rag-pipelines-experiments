{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import concurrent.futures\n",
    "import gradio as gr\n",
    "import tkinter as tk\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import PyPDF2\n",
    "import requests\n",
    "import os\n",
    "import pinecone\n",
    "import time\n",
    "\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone, ServerlessSpec, CloudProvider, AwsRegion, VectorType\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "# PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "GROQ_CHAT_URL = \"https://api.groq.com/openai/v1/chat/completions\"  # for integrating groq via openai api method\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "# LLM_MODEL = \"llama3-70b-8192\"\n",
    "LLM_MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "NVIDIA_API = os.getenv(\"NVIDIA_API\")\n",
    "\n",
    "# NVidia Embedding import\n",
    "client = OpenAI(\n",
    "    api_key= NVIDIA_API,\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Input:\n",
    "    - Context window: 128K\n",
    "Ouput:\n",
    "    - Output Max Tokens: 32,768\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def track_time(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"[Time Tracker] `{func.__name__}` took {end - start:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "# # EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "\n",
    "# vo = voyageai.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "print(PINECONE_API)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(\"ai-coach\")\n",
    "# # index = pc.Index(\"ahsan-400pg-pdf-doc-test\")\n",
    "\n",
    "# embedding_model = AutoModel.from_pretrained(\n",
    "#     'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# # Function to generate embeddings without tokenization\n",
    "# def get_embedding(data):\n",
    "#     embeddings = embedding_model.encode(data).tolist()\n",
    "#     return embeddings\n",
    "\n",
    "@track_time\n",
    "def get_embedding(text=\"None\"):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"nvidia/nv-embed-v1\",\n",
    "        encoding_format=\"float\",\n",
    "        extra_body={\"input_type\": \"query\", \"truncate\": \"NONE\"},\n",
    "    )\n",
    "\n",
    "    #print(response.data[0].embedding)\n",
    "    # print(count_tokens(response.data[0].embedding))\n",
    "    return response.data[0].embedding\n",
    "get_embedding(\"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "@track_time\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=1600, include_metadata=True)\n",
    "    return result[\"matches\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "# Function to query Groq LLM\n",
    "# def query_groq(prompt: str) -> str:\n",
    "#     response = requests.post(\n",
    "#         GROQ_CHAT_URL,\n",
    "#         headers=GROQ_HEADERS,\n",
    "#         json={\n",
    "#             \"model\": LLM_MODEL,\n",
    "#             \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#             \"temperature\": 0.5,\n",
    "#             # \"max_tokens\": 8192  # max from groq website\n",
    "#         },\n",
    "#     )\n",
    "\n",
    "#     if response.status_code != 200:\n",
    "#         raise Exception(f\"Error querying Groq: {response.text}\")\n",
    "\n",
    "#     return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# def query_groq(prompt: str) -> str:\n",
    "#     client = Groq()\n",
    "#     completion = client.chat.completions.create(\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         # Change model to compound-beta to use agentic tooling\n",
    "#         # model: \"llama-3.3-70b-versatile\",\n",
    "#         model=LLM_MODEL,\n",
    "#     )\n",
    "#     # print(completion.choices[0].message.content)\n",
    "#     return completion.choices[0].message.content\n",
    "\n",
    "@track_time\n",
    "def query_groq(prompt: str) -> str:\n",
    "    client = Groq()\n",
    "    completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=LLM_MODEL,\n",
    "        temperature=0.5,  # Set temperature to 0.5\n",
    "        )\n",
    "    return completion.choices[0]\n",
    "\n",
    "\n",
    "query_groq(\"Hello\")\n",
    "# Print all tool calls\n",
    "# print(completion.choices[0].message.executed_tools)\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "\n",
    "@track_time\n",
    "def count_tokens(text: str) -> int:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio GUI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------- WORKING 3 Enter key submits user query -------------------------------------------\n",
    "# Initialize empty conversation history (list of tuples)\n",
    "conversation_history = []\n",
    "\n",
    "@track_time\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk[\"metadata\"][\"text\"] for chunk in relevant_chunks)\n",
    "    print(\"CONTEXT:\", context)\n",
    "\n",
    "    # Format conversation history for the prompt\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\" for user, response in conversation_history\n",
    "    )\n",
    "\n",
    "    # Create structured prompt\n",
    "    prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow these guidelines carefully:\n",
    "\n",
    "    1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "    2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "    3. Ask guiding questions to encourage critical thinking.\n",
    "    4. Adapt your explanation to match the student's knowledge level.\n",
    "    5. Strictly use terminologies provided in the given context.\n",
    "    6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "    7. Only answer based on the provided context—do not speculate or include external information.\n",
    "    8. Explicitly cite the sources from the context in your responses.\n",
    "    9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "\n",
    "    Context from learning materials:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "\n",
    "    New student question:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    Provide a helpful, structured response that meets the above criteria.\n",
    "\n",
    "    (Note: The following examples are only provided for your reference to demonstrate an effective response format):\n",
    "\n",
    "    Question: How long will the average externship take to complete?\n",
    "    Answer: 125 surgical cases – typically 6 months to 1 year.\n",
    "\n",
    "    Question: What should I focus on when studying anatomy, physiology, and medical terminology?\n",
    "    Answer: Focus specifically on content related to surgical procedures, emphasizing body systems, terminology, and physiological functions most relevant to surgery.\n",
    "\n",
    "    Question: What’s the best way to study and memorize surgical instrumentation?\n",
    "    Answer: First, understand the National Center for Competency Testing (NCCT) exam expectations regarding instruments. Refer to official NCCT guidelines and utilize platforms such as Quizlet and ProProfs for visual memorization and repetition.\n",
    "\n",
    "    Provide a thoughtful and contextually accurate response now:\"\"\"\n",
    "\n",
    "    # --------------------- Reasoning Prompt --------------------------------------------------------------\n",
    "    # prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow this structured response framework:\n",
    "\n",
    "    # ### Response Requirements\n",
    "    # 1. **Reasoning Process**:\n",
    "    # - Analyze the question against provided context\n",
    "    # - Identify relevant context snippets with source citations\n",
    "    # - Perform sentiment analysis on conversation history\n",
    "\n",
    "    # 2. **Response Format**:\n",
    "    # - [Understanding] Paraphrase the question to confirm comprehension\n",
    "    # - [Relevant Context] Cite exact source material with location references\n",
    "    # - [Step-by-Step Explanation] Break down concepts using chain-of-thought\n",
    "    # - [Examples] Provide 2-3 ideal examples from context\n",
    "    # - [Guiding Questions] Pose 1-2 reflective questions to deepen learning\n",
    "    # - [Summary] Concise answer reiterating key points\n",
    "\n",
    "    # 3. **Style Guidelines**:\n",
    "    # - Use terminology strictly from context\n",
    "    # - Adapt complexity to student's history\n",
    "    # - Maintain empathetic tone based on sentiment analysis\n",
    "\n",
    "    # ### Context Materials:\n",
    "    # {context}\n",
    "\n",
    "    # ### Conversation History:\n",
    "    # {history_str}\n",
    "\n",
    "    # ### New Question:\n",
    "    # \"{user_query}\"\n",
    "\n",
    "    # ### Your Response:\n",
    "    # [Understanding] First, let me clarify what you're asking...\n",
    "    # [Relevant Context] According to [Source X, Section Y]...\n",
    "    # [Step-by-Step Explanation] The process works as follows:\n",
    "    # 1. First concept...\n",
    "    # 2. Second concept...\n",
    "    # 3. Practical application...\n",
    "    # [Examples] For instance:\n",
    "    # - Example 1...\n",
    "    # - Example 2...\n",
    "    # [Guiding Questions] Have you considered...? How might this apply to...?\n",
    "    # [Summary] To recap the key points...\"\"\"\n",
    "\n",
    "    # Get LLM response\n",
    "    groq_response = query_groq(prompt)\n",
    "    print(f\"Response Toke   ns: {count_tokens(groq_response.message.content)}\")\n",
    "\n",
    "    # Return updated history with new interaction\n",
    "    return conversation_history + [(user_query, groq_response.message.content)]\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"# 🧑‍🏫 AI Coaching Assistant\")\n",
    "    gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "\n",
    "    # State management\n",
    "    chat_history = gr.State(conversation_history)\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500)\n",
    "        with gr.Column(scale=0.5):\n",
    "            context_display = gr.Textbox(label=\"Relevant Context\", interactive=False)\n",
    "\n",
    "    user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "\n",
    "    with gr.Row():\n",
    "        submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        undo_btn = gr.Button(\"Undo Last\")\n",
    "        clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "    def handle_submit(user_input, history):\n",
    "        if not user_input.strip():\n",
    "            return gr.update(), history, \"\"\n",
    "\n",
    "        # Process query and update history\n",
    "        new_history = process_user_query(user_input, history)\n",
    "\n",
    "        # Get latest context for display\n",
    "        latest_context = \"\\n\".join(\n",
    "            [\n",
    "                chunk[\"metadata\"][\"text\"]\n",
    "                for chunk in query_pinecone(get_embedding(user_input))\n",
    "            ][:]\n",
    "        )  # Show top 3 context snippets\n",
    "\n",
    "        return \"\", new_history, latest_context\n",
    "\n",
    "    # Component interactions\n",
    "    submit_btn.click(\n",
    "        handle_submit,\n",
    "        [user_input, chat_history],\n",
    "        [user_input, chat_history, context_display],\n",
    "    ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "    # Add submit on Enter key press\n",
    "    user_input.submit(\n",
    "        handle_submit,\n",
    "        [user_input, chat_history],\n",
    "        [user_input, chat_history, context_display],\n",
    "    ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "    undo_btn.click(\n",
    "        lambda history: history[:-1] if history else [], [chat_history], [chat_history]\n",
    "    ).then(lambda x: x, [chat_history], [chatbot])\n",
    "\n",
    "    clear_btn.click(lambda: [], None, [chat_history]).then(\n",
    "        lambda: ([], \"\"), None, [chatbot, context_display]\n",
    "    )\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
