{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "from pinecone import (\n",
    "    Pinecone,\n",
    "    ServerlessSpec,\n",
    "    CloudProvider,\n",
    "    AwsRegion,\n",
    "    VectorType\n",
    ")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import tkinter as tk\n",
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "import concurrent.futures\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Important: Import pinecone-client properly\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"PINECONE_API\", PINECONE_API)\n",
    "\n",
    "\n",
    "# Groq API settings\n",
    "GROQ_EMBED_URL = \"https://api.groq.com/openai/v1/embeddings\"\n",
    "GROQ_CHAT_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "#EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "EMBEDDING_MODEL = \"deepseek-r1-distill-llama-70b\"\n",
    "\n",
    "LLM_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "print(PINECONE_API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(\"ai-coach\")\n",
    "# index = pc.Index(\"ahsan-400pg-pdf-doc-test\")\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained(\n",
    "    'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# Function to generate embeddings without tokenization\n",
    "def get_embedding(data):\n",
    "    embeddings = embedding_model.encode(data).tolist()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=20, include_metadata=True)\n",
    "    return result['matches']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Groq LLM\n",
    "def query_groq(prompt: str) -> str:\n",
    "    response = requests.post(\n",
    "        GROQ_CHAT_URL,\n",
    "        headers=GROQ_HEADERS,\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.5,\n",
    "            #\"max_tokens\": 8192  # max from groq website\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error querying Groq: {response.text}\")\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio GUI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------- WORKING 3 Enter key submits user query -------------------------------------------\n",
    "# Initialize empty conversation history (list of tuples)\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk['metadata'][\"text\"] for chunk in relevant_chunks)\n",
    "    print(\"CONTEXT:\", context)\n",
    "\n",
    "    # Format conversation history for the prompt\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\"\n",
    "        for user, response in conversation_history\n",
    "    )\n",
    "\n",
    "    # Create structured prompt\n",
    "    prompt = f\"\"\"You are a knowledgeable and friendly coach. Follow these guidelines:\n",
    "    1. Provide clear, step-by-step explanations\n",
    "    2. Ask guiding questions to encourage critical thinking\n",
    "    3. Adapt to the student's knowledge level\n",
    "    4. Use examples from the provided context when relevant\n",
    "\n",
    "    Context from learning materials:\n",
    "    {context}\n",
    "\n",
    "    Conversation history:\n",
    "    {history_str}\n",
    "\n",
    "    New student question:\n",
    "    \"{user_query}\"\n",
    "\n",
    "    Provide a helpful response:\"\"\"\n",
    "\n",
    "    # Get LLM response\n",
    "    groq_response = query_groq(prompt)\n",
    "    print(f\"Response Toke   ns: {count_tokens(groq_response)}\")\n",
    "\n",
    "    # Return updated history with new interaction\n",
    "    return conversation_history + [(user_query, groq_response)]\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"# üßë‚Äçüè´ AI Coaching Assistant\")\n",
    "    gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "\n",
    "    # State management\n",
    "    chat_history = gr.State(conversation_history)\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500)\n",
    "        with gr.Column(scale=0.5):\n",
    "            context_display = gr.Textbox(\n",
    "                label=\"Relevant Context\", interactive=False)\n",
    "\n",
    "    user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "\n",
    "    with gr.Row():\n",
    "        submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        undo_btn = gr.Button(\"Undo Last\")\n",
    "        clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "    def handle_submit(user_input, history):\n",
    "        if not user_input.strip():\n",
    "            return gr.update(), history, \"\"\n",
    "\n",
    "        # Process query and update history\n",
    "        new_history = process_user_query(user_input, history)\n",
    "\n",
    "        # Get latest context for display\n",
    "        latest_context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in query_pinecone(\n",
    "            get_embedding(user_input)\n",
    "        )][:3])  # Show top 3 context snippets\n",
    "\n",
    "        return \"\", new_history, latest_context\n",
    "\n",
    "    # Component interactions\n",
    "    submit_btn.click(\n",
    "        handle_submit,\n",
    "        [user_input, chat_history],\n",
    "        [user_input, chat_history, context_display]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    # Add submit on Enter key press\n",
    "    user_input.submit(\n",
    "        handle_submit,\n",
    "        [user_input, chat_history],\n",
    "        [user_input, chat_history, context_display]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    undo_btn.click(\n",
    "        lambda history: history[:-1] if history else [],\n",
    "        [chat_history],\n",
    "        [chat_history]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        lambda: [],\n",
    "        None,\n",
    "        [chat_history]\n",
    "    ).then(\n",
    "        lambda: ([], \"\"),\n",
    "        None,\n",
    "        [chatbot, context_display]\n",
    "    )\n",
    "\n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
