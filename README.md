```markdown
# ðŸ” RAG Pipeline with Groq + LLaMA 3 70B + Pinecone

This repository is a personal lab for experimenting with **Retrieval-Augmented Generation (RAG)** pipelines using the blazing-fast `llama3-70b-8192` model via **Groq**, document embedding via **Hugging Face**, and vector storage via **Pinecone**.

> âœ… Latest notebook: [`gradio_test.ipynb`]
---

## âœ¨ What's Inside?

- ðŸ“„ Load and split documents into context chunks
- ðŸ§  Generate embeddings using Hugging Faceâ€™s `jinaai/jina-embeddings-v2-base-en`
- ðŸ§¬ Store and retrieve vectors with **Pinecone**
- âš¡ Query Groqâ€™s `llama3-70b-8192` via **LangChain**
- ðŸ’¬ Get context-aware LLM responses
- ðŸ–¼ï¸ Interactive **Gradio GUI** with:
  - ðŸ’¬ Conversation history
  - ðŸ—‘ï¸ Delete last message
  - â™»ï¸ Clear full chat

---

## ðŸ› ï¸ Tech Stack

| Component        | Tool/Service                             |
|------------------|-------------------------------------------|
| Embedding Model  | `sentence-transformers/all-MiniLM-L6-v2`  |
| Vector Store     | [Pinecone](https://www.pinecone.io/)      |
| LLM Inference    | [Groq](https://console.groq.com/)         |
| LLM Model        | `llama3-70b-8192`                         |
| Orchestration    | LangChain                                 |
| UI               | [Gradio](https://www.gradio.app/)         |

---

## ðŸš€ Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/osamaizhar/Rag-pipelines-experiments.git
cd Rag-pipelines-experiments

```

### 2. Install Requirements

```bash
pip install -r requirements.txt
```

### 3. Set Your API Keys

You will need API keys for both Groq and Pinecone. Create a `.env` file or set the following environment variables:

```bash
export GROQ_API_KEY="your_groq_api_key"
export PINECONE_API_KEY="your_pinecone_api_key"
export PINECONE_ENVIRONMENT="your_pinecone_environment"  # e.g., "gcp-starter"
```

---

## â–¶ï¸ How to Use

1. Add your custom data to `data.txt`
2. To run the full pipeline (including data processing and embedding):
   - Run all cells in `groq_test.ipynb`
3. To run only the inference (using pre-existing embeddings):
   - Comment out or skip the cells for upsert, chunking, and chunks embedding in `groq_test.ipynb`
   - Run the remaining cells
4. Use the **Gradio chat interface** to:
   - Ask questions about your data
   - View **conversation history**
   - ðŸ—‘ï¸ **Delete last message**
   - â™»ï¸ **Clear entire chat**
5. Get fast, context-aware responses using LLaMA 3 on Groq
---

> âš¡ Built for experimentation and speed. Powered by Groqâ€™s LPU inference, Pinecone vector DB, and LangChain orchestration.

