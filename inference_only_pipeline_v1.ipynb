{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "from pinecone import (\n",
    "    Pinecone,\n",
    "    ServerlessSpec,\n",
    "    CloudProvider,\n",
    "    AwsRegion,\n",
    "    VectorType\n",
    ")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import tkinter as tk\n",
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "import concurrent.futures\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Important: Import pinecone-client properly\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "# Groq API settings\n",
    "GROQ_EMBED_URL = \"https://api.groq.com/openai/v1/embeddings\"\n",
    "GROQ_CHAT_URL = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "LLM_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "\n",
    "# Configure headers for Groq API requests\n",
    "GROQ_HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "print(PINECONE_API)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(\"ai-coach\")\n",
    "# index = pc.Index(\"ahsan-400pg-pdf-doc-test\")\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained(\n",
    "    'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# Function to generate embeddings without tokenization\n",
    "def get_embedding(data):\n",
    "    embeddings = embedding_model.encode(data).tolist()\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Pinecone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Pinecone index using embeddings\n",
    "def query_pinecone(embedding):\n",
    "    # Use keyword arguments to pass the embedding and other parameters\n",
    "    result = index.query(vector=embedding, top_k=20, include_metadata=True)\n",
    "    return result['matches']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Groq Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query Groq LLM\n",
    "def query_groq(prompt: str) -> str:\n",
    "    response = requests.post(\n",
    "        GROQ_CHAT_URL,\n",
    "        headers=GROQ_HEADERS,\n",
    "        json={\n",
    "            \"model\": LLM_MODEL,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"temperature\": 0.5,\n",
    "            \"max_tokens\": 8192  # max from groq website\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error querying Groq: {response.text}\")\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# Tokenizer to count number of tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process User Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio GUI TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_message = f\"\"\"\n",
    "#     You are a knowledgeable and friendly coach. Your goal is to help students understand concepts in a detailed and easy-to-understand manner.\n",
    "#     Be patient, ask guiding questions, and provide step-by-step explanations where needed. Adapt your responses to the student's knowledge level\n",
    "#     and help them build confidence in their learning. Refer relevant material to the student and encourage them to explore further.\n",
    "\n",
    "#     Based on the context and the student's question, provide a thoughtful and detailed explanation. Encourage them to think about the topic and\n",
    "#     offer further guidance if needed.\n",
    "#     \"\"\"\n",
    "\n",
    "# def gradio_interface(prompt,history =[]):\n",
    "#     output = process_user_query(prompt,history)\n",
    "#     history.append((prompt,output))\n",
    "#     return history\n",
    "\n",
    "# gr.Interface(fn=gradio_interface, inputs= ['text',\"state\"], outputs=[\"chatbot\",\"state\"]).launch(debug=True,share=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------- WORKING 1 -------------------------------------------\n",
    "\n",
    "# # Function to be used by Gradio for handling the query\n",
    "# def gradio_process(user_query):\n",
    "#     response = process_user_query(user_query, conversation_history)\n",
    "#     return response\n",
    "\n",
    "# # Create Gradio interface\n",
    "# interface = gr.Interface(fn=gradio_process, inputs=\"text\", outputs=\"text\", title=\"RAG-based Coaching System\")\n",
    "\n",
    "# # Launch Gradio app\n",
    "# interface.launch()\n",
    "# ------------------------------------------- WORKING 2 -------------------------------------------\n",
    "\n",
    "# Initialize empty conversation history (list of tuples)\n",
    "# conversation_history = []\n",
    "\n",
    "# def process_user_query(user_query: str, conversation_history: list):\n",
    "#     print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "#     # Generate embedding and get relevant context\n",
    "#     embedding = get_embedding(user_query)\n",
    "#     relevant_chunks = query_pinecone(embedding)\n",
    "#     context = \"\\n\".join(chunk['metadata'][\"text\"] for chunk in relevant_chunks)\n",
    "#     print(\"CONTEXT:\", context)\n",
    "\n",
    "#     # Format conversation history for the prompt\n",
    "#     history_str = \"\\n\".join(\n",
    "#         f\"User: {user}\\nCoach: {response}\"\n",
    "#         for user, response in conversation_history\n",
    "#     )\n",
    "\n",
    "#     # Create structured prompt\n",
    "#     prompt = f\"\"\"You are a knowledgeable and friendly coach. Follow these guidelines:\n",
    "#     1. Provide clear, step-by-step explanations\n",
    "#     2. Ask guiding questions to encourage critical thinking\n",
    "#     3. Adapt to the student's knowledge level\n",
    "#     4. Use examples from the provided context when relevant\n",
    "\n",
    "#     Context from learning materials:\n",
    "#     {context}\n",
    "\n",
    "#     Conversation history:\n",
    "#     {history_str}\n",
    "\n",
    "#     New student question:\n",
    "#     \"{user_query}\"\n",
    "\n",
    "#     Provide a helpful response:\"\"\"\n",
    "\n",
    "#     # Get LLM response\n",
    "#     groq_response = query_groq(prompt)\n",
    "#     print(f\"Response Tokens: {count_tokens(groq_response)}\")\n",
    "\n",
    "#     # Return updated history with new interaction\n",
    "#     return conversation_history + [(user_query, groq_response)]\n",
    "\n",
    "# # Gradio Interface\n",
    "# with gr.Blocks() as interface:\n",
    "#     gr.Markdown(\"# üßë‚Äçüè´ AI Coaching Assistant\")\n",
    "#     gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "\n",
    "#     # State management\n",
    "#     chat_history = gr.State(conversation_history)\n",
    "\n",
    "#     with gr.Row():\n",
    "#         chatbot = gr.Chatbot(height=500)\n",
    "#         with gr.Column(scale=0.5):\n",
    "#             context_display = gr.Textbox(label=\"Relevant Context\", interactive=False)\n",
    "\n",
    "#     user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "\n",
    "#     with gr.Row():\n",
    "#         submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "#         undo_btn = gr.Button(\"Undo Last\")\n",
    "#         clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "#     def handle_submit(user_input, history):\n",
    "#         if not user_input.strip():\n",
    "#             return gr.update(), history, \"\"\n",
    "\n",
    "#         # Process query and update history\n",
    "#         new_history = process_user_query(user_input, history)\n",
    "\n",
    "#         # Get latest context for display\n",
    "#         latest_context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in query_pinecone(\n",
    "#             get_embedding(user_input)\n",
    "#         )][:3])  # Show top 3 context snippets\n",
    "\n",
    "#         return \"\", new_history, latest_context\n",
    "\n",
    "#     # Component interactions\n",
    "#     submit_btn.click(\n",
    "#         handle_submit,\n",
    "#         [user_input, chat_history],\n",
    "#         [user_input, chat_history, context_display]\n",
    "#     ).then(\n",
    "#         lambda x: x,\n",
    "#         [chat_history],\n",
    "#         [chatbot]\n",
    "#     )\n",
    "\n",
    "#     undo_btn.click(\n",
    "#         lambda history: history[:-1] if history else [],\n",
    "#         [chat_history],\n",
    "#         [chat_history]\n",
    "#     ).then(\n",
    "#         lambda x: x,\n",
    "#         [chat_history],\n",
    "#         [chatbot]\n",
    "#     )\n",
    "\n",
    "#     clear_btn.click(\n",
    "#         lambda: [],\n",
    "#         None,\n",
    "#         [chat_history]\n",
    "#     ).then(\n",
    "#         lambda: ([], \"\"),\n",
    "#         None,\n",
    "#         [chatbot, context_display]\n",
    "#     )\n",
    "\n",
    "# interface.launch(share=True)\n",
    "# Just change the launch command to:\n",
    "# interface.launch(share=True, auth=(\"username\", \"password\"))  # Add basic auth\n",
    "\n",
    "\n",
    "# self hosting\n",
    "\n",
    "# # Run with:\n",
    "# interface.launch(\n",
    "#     server_name=\"0.0.0.0\",\n",
    "#     server_port=7860,\n",
    "#     show_error=True\n",
    "# )\n",
    "\n",
    "\n",
    "# ------------------------------------------- WORKING 3 Enter key submits user query -------------------------------------------\n",
    "# Initialize empty conversation history (list of tuples)\n",
    "conversation_history = []\n",
    "\n",
    "\n",
    "def process_user_query(user_query: str, conversation_history: list):\n",
    "    print(f\"User Query Tokens: {count_tokens(user_query)}\")\n",
    "\n",
    "    # Generate embedding and get relevant context\n",
    "    embedding = get_embedding(user_query)\n",
    "    relevant_chunks = query_pinecone(embedding)\n",
    "    context = \"\\n\".join(chunk['metadata'][\"text\"] for chunk in relevant_chunks)\n",
    "    print(\"CONTEXT:\", context)\n",
    "\n",
    "    # Format conversation history for the prompt\n",
    "    history_str = \"\\n\".join(\n",
    "        f\"User: {user}\\nCoach: {response}\"\n",
    "        for user, response in conversation_history\n",
    "    )\n",
    "\n",
    "    # Create structured prompt\n",
    "    prompt = f\"\"\"You are an expert, knowledgeable, and friendly coach. Follow these guidelines carefully:\n",
    "\n",
    "1. Provide clear, step-by-step explanations to ensure deep understanding.\n",
    "2. Use chain-of-thought reasoning to thoroughly evaluate the provided context before responding.\n",
    "3. Ask guiding questions to encourage critical thinking.\n",
    "4. Adapt your explanation to match the student's knowledge level.\n",
    "5. Strictly use terminologies provided in the given context.\n",
    "6. Provide short, ideal examples (2-3) to illustrate your points clearly.\n",
    "7. Only answer based on the provided context‚Äîdo not speculate or include external information.\n",
    "8. Explicitly cite the sources from the context in your responses.\n",
    "9. Perform sentiment analysis based on conversation history and user queries to adapt your responses empathetically and effectively.\n",
    "\n",
    "Context from learning materials:\n",
    "{context}\n",
    "\n",
    "Conversation history:\n",
    "{history_str}\n",
    "\n",
    "New student question:\n",
    "\"{user_query}\"\n",
    "\n",
    "Provide a helpful, structured response that meets the above criteria.\n",
    "\n",
    "(Note: The following examples are only provided for your reference to demonstrate an effective response format):\n",
    "\n",
    "Question: How long will the average externship take to complete?\n",
    "Answer: 125 surgical cases ‚Äì typically 6 months to 1 year.\n",
    "\n",
    "Question: What should I focus on when studying anatomy, physiology, and medical terminology?\n",
    "Answer: Focus specifically on content related to surgical procedures, emphasizing body systems, terminology, and physiological functions most relevant to surgery.\n",
    "\n",
    "Question: What‚Äôs the best way to study and memorize surgical instrumentation?\n",
    "Answer: First, understand the National Center for Competency Testing (NCCT) exam expectations regarding instruments. Refer to official NCCT guidelines and utilize platforms such as Quizlet and ProProfs for visual memorization and repetition.\n",
    "\n",
    "Provide a thoughtful and contextually accurate response now:\"\"\"\n",
    "    # Get LLM response\n",
    "    groq_response = query_groq(prompt)\n",
    "    print(f\"Response Toke   ns: {count_tokens(groq_response)}\")\n",
    "\n",
    "    # Return updated history with new interaction\n",
    "    return conversation_history + [(user_query, groq_response)]\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"# üßë‚Äçüè´ AI Coaching Assistant\")\n",
    "    gr.Markdown(\"Welcome! I'm here to help you learn. Type your question below.\")\n",
    "\n",
    "    # State management\n",
    "    chat_history = gr.State(conversation_history)\n",
    "\n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(height=500)\n",
    "        with gr.Column(scale=0.5):\n",
    "            context_display = gr.Textbox(\n",
    "                label=\"Relevant Context\", interactive=False)\n",
    "\n",
    "    user_input = gr.Textbox(label=\"Your Question\", placeholder=\"Type here...\")\n",
    "\n",
    "    with gr.Row():\n",
    "        submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        undo_btn = gr.Button(\"Undo Last\")\n",
    "        clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "    def handle_submit(user_input, history):\n",
    "        if not user_input.strip():\n",
    "            return gr.update(), history, \"\"\n",
    "\n",
    "        # Process query and update history\n",
    "        new_history = process_user_query(user_input, history)\n",
    "\n",
    "        # Get latest context for display\n",
    "        latest_context = \"\\n\".join([chunk['metadata'][\"text\"] for chunk in query_pinecone(\n",
    "            get_embedding(user_input)\n",
    "        )][:3])  # Show top 3 context snippets\n",
    "\n",
    "        return \"\", new_history, latest_context\n",
    "\n",
    "    # Component interactions\n",
    "    submit_btn.click(\n",
    "        handle_submit,\n",
    "        [user_input, chat_history],\n",
    "        [user_input, chat_history, context_display]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    # Add submit on Enter key press\n",
    "    user_input.submit(\n",
    "        handle_submit,\n",
    "        [user_input, chat_history],\n",
    "        [user_input, chat_history, context_display]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    undo_btn.click(\n",
    "        lambda history: history[:-1] if history else [],\n",
    "        [chat_history],\n",
    "        [chat_history]\n",
    "    ).then(\n",
    "        lambda x: x,\n",
    "        [chat_history],\n",
    "        [chatbot]\n",
    "    )\n",
    "\n",
    "    clear_btn.click(\n",
    "        lambda: [],\n",
    "        None,\n",
    "        [chat_history]\n",
    "    ).then(\n",
    "        lambda: ([], \"\"),\n",
    "        None,\n",
    "        [chatbot, context_display]\n",
    "    )\n",
    "\n",
    "interface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
