{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file now only contains the upsert and updating Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import concurrent.futures\n",
    "import gradio as gr\n",
    "import tkinter as tk\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import PyPDF2\n",
    "import requests\n",
    "import os\n",
    "import pinecone\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "from tkinter import scrolledtext, messagebox\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "import time\n",
    "\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "from pinecone import Pinecone, ServerlessSpec, CloudProvider, AwsRegion, VectorType\n",
    "\n",
    "\n",
    "# import voyageai\n",
    "\n",
    "\n",
    "# Important: Import pinecone-client properly\n",
    "\n",
    "# Load environment variables from .env file\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "\n",
    "# PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "NVIDIA_API = os.getenv(\"NVIDIA_API\")\n",
    "\n",
    "# NVidia Embedding import\n",
    "client = OpenAI(\n",
    "    api_key=NVIDIA_API,\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    ")\n",
    "\n",
    "# # EMBEDDING_MODEL = \"llama3-405b-8192-embed\"\n",
    "\n",
    "# vo = voyageai.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "\n",
    "# documents = pdf_load_documents()\n",
    "# documents\n",
    "\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "#     \"\"\"Extract text from a PDF file.\"\"\"\n",
    "#     with open(pdf_path, 'r') as file:\n",
    "#         pdf_reader = PyPDF2.PdfReader(file)\n",
    "#         text = \"\"\n",
    "#         for page_num in range(len(pdf_reader.pages)):\n",
    "#             page = pdf_reader.pages[page_num]\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "#     return text\n",
    "# extract_text_from_pdf(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Directory Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load PDF and Excel files from DATA_PATH.\n",
    "    Returns a list of documents with content and metadata and a list of filenames.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    file_names = []\n",
    "\n",
    "    # Load PDFs\n",
    "    pdf_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    pdf_docs = pdf_loader.load()\n",
    "    for doc in pdf_docs:\n",
    "        documents.append(\n",
    "            {\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": {\n",
    "                    \"source\": doc.metadata.get(\"source\", \"unknown\"),\n",
    "                    \"file_type\": \"pdf\",\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "        file_names.append(doc.metadata.get(\"source\", \"unknown\"))\n",
    "\n",
    "    # Load Excel files\n",
    "    excel_files = glob.glob(os.path.join(DATA_PATH, \"*.xlsx\"))\n",
    "    for file in excel_files:\n",
    "        df = pd.read_excel(file)\n",
    "        headers = df.columns.tolist()\n",
    "        for _, row in df.iterrows():\n",
    "            content = \" \".join([f\"{col}: {str(row[col])}\" for col in headers])\n",
    "            documents.append(\n",
    "                {\"content\": content, \"metadata\": {\"source\": file, \"file_type\": \"excel\"}}\n",
    "            )\n",
    "        file_names.append(file)\n",
    "\n",
    "    if not documents:\n",
    "        print(f\"No PDF or Excel files found in {DATA_PATH}\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "    return documents, file_names\n",
    "\n",
    "\n",
    "# Load documents\n",
    "documents, file_names = load_documents()\n",
    "\n",
    "# # Print file names\n",
    "# print(\"Files parsed:\")\n",
    "# for name in file_names:\n",
    "#     print(f\"- {name}\")\n",
    "\n",
    "# print(f\"\\nTotal documents loaded: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Directory Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF DOCS len: \n",
      "\n",
      " 0\n",
      "No PDF or Excel files found in D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\n",
      "Files parsed:\n",
      "\n",
      "Total documents loaded: 0\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load PDF and Excel files from DATA_PATH and its subdirectories.\n",
    "    Returns a list of documents with content and metadata, and a list of filenames.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    file_names = []\n",
    "    processed_files = []\n",
    "\n",
    "    # Load PDFs from all subdirectories\n",
    "    pdf_loader = PyPDFDirectoryLoader(DATA_PATH, recursive=True)\n",
    "    pdf_docs = pdf_loader.load()\n",
    "    print(\"PDF DOCS len: \\n\\n\", len(pdf_docs))\n",
    "    # print(\"PDF DOCS: \\n\\n\",pdf_docs)\n",
    "    count = 0\n",
    "    for doc in pdf_docs:\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        if source not in processed_files:\n",
    "            documents.append(\n",
    "                {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": {\"source\": source, \"file_type\": \"pdf\"},\n",
    "                }\n",
    "            )\n",
    "            # print(f\"PARSED DATA {source}: \\n\",documents[-1])\n",
    "\n",
    "            processed_files.append(source)\n",
    "            file_names.append(source)\n",
    "            # print(f\"Parsed PDF file : {source}\")\n",
    "        # print(\"COUNT: \", count)\n",
    "        count += 1\n",
    "    # return\n",
    "\n",
    "    # Load Excel files from all subdirectories\n",
    "    for root, _, files in os.walk(DATA_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xlsx\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file_path not in processed_files:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                    headers = df.columns.tolist()\n",
    "                    for _, row in df.iterrows():\n",
    "                        content = \" \".join(\n",
    "                            [f\"{col}: {str(row[col])}\" for col in headers]\n",
    "                        )\n",
    "                        documents.append(\n",
    "                            {\n",
    "                                \"content\": content,\n",
    "                                \"metadata\": {\"source\": file_path, \"file_type\": \"excel\"},\n",
    "                            }\n",
    "                        )\n",
    "                        # print(\"PARSED DATA: \\n\\n\\n\",documents[-1])\n",
    "                    processed_files.append(file_path)\n",
    "                    file_names.append(file_path)\n",
    "                    print(f\"Parsed Excel file: {file_path}\")\n",
    "\n",
    "    if not documents:\n",
    "        print(f\"No PDF or Excel files found in {DATA_PATH}\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "    # Print file names\n",
    "    print(\"Files parsed:\")\n",
    "    for name in processed_files:\n",
    "        print(f\"- {name}\")\n",
    "    return documents, file_names\n",
    "\n",
    "\n",
    "# Load documents\n",
    "documents, file_names = load_documents()\n",
    "\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitting \\ Chunking using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_documents(documents):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1000,\n",
    "#         chunk_overlap=500,\n",
    "#         length_function=len,\n",
    "#         is_separator_regex=True  # considers separators like '\\n\\n' if true\n",
    "#     )\n",
    "#     # Assuming each document is a dictionary with 'content' and 'metadata'\n",
    "#     docs = []\n",
    "#     for doc in documents:\n",
    "#         chunks = text_splitter.split_text(doc['content'])\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             docs.append({\n",
    "#                 \"content\": chunk,\n",
    "#                 \"metadata\": {\n",
    "#                     **doc['metadata'],\n",
    "#                     \"chunk_id\": i\n",
    "#                 }\n",
    "#             })\n",
    "#     return docs\n",
    "\n",
    "\n",
    "# # Split documents into chunks\n",
    "# chunks = split_documents(documents)\n",
    "# # print(len(chunks))\n",
    "\n",
    "# ------------------------- For Nvidia EMbed v2 ------------------------------------------------------------------------------\n",
    "\n",
    "# # Initialize NV Embed's tokenizer for precise length calculation\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"nvidia/NV-Embed-v2\")\n",
    "\n",
    "\n",
    "# def split_documents(documents):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "#         tokenizer=tokenizer,\n",
    "#         chunk_size=30000,          # 30k tokens (93.75% of 32k context)\n",
    "#         chunk_overlap=2000,        # Balanced overlap for continuity\n",
    "#         separators=[\n",
    "#             '\\n\\n---\\n\\n',         # Markdown section breaks\n",
    "#             '\\n\\n',                # Paragraphs\n",
    "#             '.\\n',                 # Sentence + newline\n",
    "#             '. ', '! ', '? ',      # Sentence boundaries\n",
    "#             '\\n', '。', '．',      # Fallback separators\n",
    "#             ' ',                   # Last resort\n",
    "#         ],\n",
    "#         keep_separator=True        # Retain separators in chunks\n",
    "#     )\n",
    "\n",
    "#     docs = []\n",
    "#     for doc in documents:\n",
    "#         # Pre-process to remove excessive whitespace\n",
    "#         clean_content = \" \".join(doc[\"content\"].split())\n",
    "\n",
    "#         chunks = text_splitter.split_text(clean_content)\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             token_count = len(tokenizer.tokenize(chunk))\n",
    "#             docs.append({\n",
    "#                 \"content\": chunk,\n",
    "#                 \"metadata\": {\n",
    "#                     **doc[\"metadata\"],\n",
    "#                     \"chunk_id\": i,\n",
    "#                     \"token_count\": token_count,\n",
    "#                     # Flag if doc fits in one chunk\n",
    "#                     \"is_full_document\": len(chunks) == 1\n",
    "#                 }\n",
    "#             })\n",
    "#     return docs\n",
    "\n",
    "\n",
    "# ----------- Testing voyager 3 large embedding model ---------------------------\n",
    "\n",
    "# # version 1 maq\n",
    "# def get_embedding(data):\n",
    "#     embeddings = vo.embed(data, model=\"voyage-3\",\n",
    "#                           input_type=\"document\").embeddings[0]\n",
    "#     return embeddings\n",
    "\n",
    "# # version 2 maq\n",
    "# def get_embedding(data):\n",
    "#     try:\n",
    "#         embeddings = vo.embed(data, model=\"voyage-3\",\n",
    "#                               input_type=\"document\").embeddings[0]\n",
    "#         # Wait 0.5 seconds between requests to stay under 3 RPM limit\n",
    "#         time.sleep(0.5)\n",
    "#         return embeddings\n",
    "#     except voyageai.error.RateLimitError:\n",
    "#         print(\"Rate limit hit, waiting 1 seconds...\")\n",
    "#         time.sleep(1)  # Wait longer if we hit the rate limit\n",
    "#         return get_embedding(data)  # Retry the request\n",
    "# # def get_embedding(data):\n",
    "# #     embeddings = vo.embed(data, model=\"voyage-3\",\n",
    "# #                           input_type=\"document\").tolist()\n",
    "# #     return embeddings\n",
    "\n",
    "\n",
    "# # embedding_model = vo.get_tokenizer(\"voyage-3\")\n",
    "\n",
    "# # Function to generate embeddings without tokenization\n",
    "\n",
    "\n",
    "# # def get_embedding(data):\n",
    "# #     embeddings = embedding_model.encode(data).tolist()\n",
    "# #     return embeddings\n",
    "\n",
    "\n",
    "# # Tokenizer to count number of tokens\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "\n",
    "\n",
    "# def split_documents(documents):\n",
    "#     # Use ~90% of the 32 k window for chunk_size, leave room for overlap\n",
    "#     chunk_size = 28800   # 90% of 32 000\n",
    "#     chunk_overlap = 3200   # 10% overlap\n",
    "\n",
    "#     text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap,\n",
    "#         tokenizer=tokenizer,\n",
    "#         separators=[\n",
    "#             \"\\n\\n---\\n\\n\",  # markdown breaks\n",
    "#             \"\\n\\n\",         # paragraphs\n",
    "#             \".\\n\",          # sentence + newline\n",
    "#             \". \", \"! \", \"? \",\n",
    "#             \"\\n\", \"。\", \"．\",\n",
    "#             \" \",\n",
    "#         ],\n",
    "#         keep_separator=True\n",
    "#     )\n",
    "\n",
    "#     docs = []\n",
    "#     for doc in documents:\n",
    "#         # collapse whitespace\n",
    "#         clean = \" \".join(doc[\"content\"].split())\n",
    "#         chunks = text_splitter.split_text(clean)\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             docs.append({\n",
    "#                 \"content\": chunk,\n",
    "#                 \"metadata\": {\n",
    "#                     **doc.get(\"metadata\", {}),\n",
    "#                     \"chunk_id\": i,\n",
    "#                     \"token_count\": len(tokenizer.encode(chunk)),\n",
    "#                     \"is_full_document\": (len(chunks) == 1)\n",
    "#                 }\n",
    "#             })\n",
    "#     return docs\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# chunks = split_documents(documents)\n",
    "# print(f\"Total chunks: {len(chunks)}\")\n",
    "\n",
    "# # Split documents into chunks\n",
    "# chunks = split_documents(documents)\n",
    "# print(len(chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Splitting \\ Chunking for llama text embed v2 via pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
    "    # Encode the text into tokens\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "def split_documents(documents):\n",
    "    # Each chunk is ~800-1000 tokens to leave room for metadata tokens if needed\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=32000,  # fit comfortably within 2048 token limit\n",
    "        chunk_overlap=16000,  # helps retain context between chunks\n",
    "        length_function=len,  # use token length if tokenizer available\n",
    "        is_separator_regex=True,  # respect newline and semantic breaks\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    for doc in documents:\n",
    "        split_texts = text_splitter.split_text(doc[\"content\"])\n",
    "\n",
    "        for i, chunk in enumerate(split_texts):\n",
    "            chunks.append(\n",
    "                {\"content\": chunk, \"metadata\": {**doc[\"metadata\"], \"chunk_id\": i}}\n",
    "            )\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "chunks = split_documents(documents)\n",
    "\n",
    "# print(len(chunks))\n",
    "print(chunks[0])\n",
    "print(\n",
    "    \"Tokens: \", count_tokens(chunks[0][\"content\"])\n",
    ")  # Check if splitting looks reasonable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "# pc = Pinecone(api_key=PINECONE_API, headers={\n",
    "#     \"X-Pinecone-API-Version\": \"2025-04\"\n",
    "# })\n",
    "\n",
    "print(PINECONE_API)\n",
    "\n",
    "\n",
    "# -------- Integrated Emebeddings ---------------------------\n",
    "# # Create a dense index with integrated inference\n",
    "# index_name = \"llama-text-embed-v2\"\n",
    "\n",
    "# pc.create_index_for_model(\n",
    "#     name=index_name,\n",
    "#     cloud=\"aws\",\n",
    "#     region=\"us-east-1\",\n",
    "#     embed={\n",
    "#         \"model\": \"llama-text-embed-v2\",\n",
    "#         \"dimensions\": 2046,\n",
    "#         \"field_map\": {\n",
    "#             \"text\": \"text\"  # Map the record field to be embedded\n",
    "#         }\n",
    "#     }\n",
    "# )\n",
    "# index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What:\n",
    "**Use Upsert:**\n",
    "\n",
    "When you're adding new vectors or want to replace existing vectors with new data (including changing the vector values).\n",
    "When you need to add a completely new document or vector.\n",
    "When you want to update both the vector values and metadata.\n",
    "\n",
    "**Use Update:**\n",
    "\n",
    "When you're only modifying the metadata of an existing vector.\n",
    "When the vector values (embeddings) themselves are correct and only extra information like text, author, or document-related metadata needs to be updated.\n",
    "Summary:\n",
    "Upsert: Adds or replaces both the vector values and metadata. Use when inserting or completely replacing data.\n",
    "Update: Modifies the metadata without changing the vector values. Use when the vectors are correct, but metadata needs an update.\n",
    "For your case, if you just want to add or update the page_content or any other metadata for existing vectors, use update. If you want to re-upload vectors with new embeddings or metadata, use upsert.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings Via AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en'  and Upsert each to Pinecone one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connect to the index\n",
    "# index = pc.Index(\"ai-coach-1024\")\n",
    "\n",
    "# # index = pc.Index(\"ai-coach-nvidia-v2\")\n",
    "\n",
    "# # embedding_model = AutoModel.from_pretrained(\n",
    "# #     'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "\n",
    "# # ----------- Testing Nvidia nv embed v2 embedding model (req of memory and ram too high)---------------------------\n",
    "\n",
    "# # embedding_model = AutoModel.from_pretrained(\n",
    "# #     'nvidia/NV-Embed-v2',  # Updated to NV Embed v2\n",
    "# #     trust_remote_code=True\n",
    "# # )\n",
    "\n",
    "\n",
    "# def upsert_chunks_to_pinecone(index, chunks):\n",
    "#     count = 1\n",
    "#     for chunk in chunks:\n",
    "#         # Ensure the chunk has the correct structure\n",
    "#         content = chunk.get('content')\n",
    "#         metadata = chunk.get('metadata', {})\n",
    "\n",
    "#         # Get the embedding for the chunk\n",
    "#         embedding = get_embedding(content)\n",
    "\n",
    "#         # Add the text as part of the metadata\n",
    "#         metadata['text'] = content  # Store text in metadata\n",
    "\n",
    "#         # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "#         vector_id = f\"vec_{count}\"\n",
    "\n",
    "#         # Upsert the embedding along with its metadata\n",
    "#         index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "\n",
    "#         print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "#         count += 1\n",
    "\n",
    "#     print(f\"All {count-1} embeddings have been upserted to Pinecone\")\n",
    "\n",
    "# # maqs version 2\n",
    "# # def upsert_chunks_to_pinecone(index, chunks):\n",
    "# #     count = 1\n",
    "# #     for chunk in chunks:\n",
    "# #         try:\n",
    "# #             content = chunk.get('content')\n",
    "# #             metadata = chunk.get('metadata', {})\n",
    "\n",
    "# #             embedding = get_embedding(content)\n",
    "# #             metadata['text'] = content\n",
    "\n",
    "# #             vector_id = f\"vec_{count}\"\n",
    "# #             index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "\n",
    "# #             print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "# #             count += 1\n",
    "\n",
    "# #             if count % 3 == 0:  # Process in batches of 3\n",
    "# #                 print(\"Waiting 60 seconds before next batch...\")\n",
    "# #                 time.sleep(60)\n",
    "\n",
    "# #         except Exception as e:\n",
    "# #             print(f\"Error processing chunk {count}: {str(e)}\")\n",
    "# #             time.sleep(20)  # Wait before retrying\n",
    "# #             continue\n",
    "\n",
    "# #     print(f\"All {count-1} embeddings have been upserted to Pinecone\")\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# # upsert_chunks_to_pinecone(index, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Embeddings Via **nvidia/nv-embed-v1** and Upserting to pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the index\n",
    "# index = pc.Index(\"ai-coach\") -- COURSE 1 ONLY\n",
    "index = pc.Index(\"surgical-tech-complete\")  # -- COMPLETE SURGICAL TECH BOOTCAMP\n",
    "\n",
    "\n",
    "# embedding_model = AutoModel.from_pretrained(\n",
    "#     'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "# # Function to generate embeddings without tokenization\n",
    "\n",
    "\n",
    "# def get_embedding(data):\n",
    "#     embeddings = pc.inference.embed(\n",
    "#     model=\"llama-text-embed-v2\",\n",
    "#     inputs=[data],\n",
    "#     parameters={\n",
    "#         \"input_type\": \"passage\"\n",
    "#     }\n",
    "# )\n",
    "#     return embeddings\n",
    "# print(get_embedding(\"hello\"))\n",
    "\n",
    "# def upsert_chunks_to_pinecone(index, chunks):\n",
    "#     count = 1\n",
    "#     for chunk in chunks:\n",
    "#         # Ensure the chunk has the correct structure\n",
    "#         content = chunk.get('content')\n",
    "#         metadata = chunk.get('metadata', {})\n",
    "\n",
    "#         # Get the embedding for the chunk\n",
    "#         embedding = get_embedding(content)\n",
    "\n",
    "#         # Add the text as part of the metadata\n",
    "#         metadata['text'] = content  # Store text in metadata\n",
    "\n",
    "#         # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "#         vector_id = f\"vec_{count}\"\n",
    "\n",
    "#         # Upsert the embedding along with its metadata\n",
    "#         index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "\n",
    "#         print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "#         count += 1\n",
    "\n",
    "#     print(f\"All {count-1} embeddings have been upserted to Pinecone\")\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# # Assuming `index` is your Pinecone index and `chunks` is the list of chunked documents\n",
    "# upsert_chunks_to_pinecone(index, chunks)\n",
    "\n",
    "# query_embeddings = embedding_model.encode(user_query).tolist()\n",
    "# query_embeddings\n",
    "\n",
    "\n",
    "# ----------------------------- Updated Code ---------------------------------\n",
    "\n",
    "# def get_embedding(texts):\n",
    "#     \"\"\"\n",
    "#     Get embeddings for a list of texts\n",
    "#     \"\"\"\n",
    "#     embeddings = pc.inference.embed(\n",
    "#         model=\"llama-text-embed-v2\",\n",
    "#         inputs=texts,\n",
    "#         parameters={\n",
    "#             \"input_type\": \"passage\",\n",
    "#             \"dimension\": 2048\n",
    "#         }\n",
    "#     )\n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "def get_embedding(text=\"None\"):\n",
    "    response = client.embeddings.create(\n",
    "        input=text,\n",
    "        model=\"nvidia/nv-embed-v1\",\n",
    "        encoding_format=\"float\",\n",
    "        extra_body={\"input_type\": \"query\", \"truncate\": \"NONE\"},\n",
    "    )\n",
    "\n",
    "    # print(response.data[0].embedding)\n",
    "    # print(count_tokens(response.data[0].embedding))\n",
    "    return response.data[0].embedding\n",
    "\n",
    "\n",
    "# print(get_embedding(\"Surgical Conscience\"))\n",
    "\n",
    "\n",
    "def upsert_chunks_to_pinecone(index, chunks):\n",
    "    count = 0\n",
    "    for chunk in chunks:\n",
    "        # Ensure the chunk has the correct structure\n",
    "        content = chunk.get(\"content\")\n",
    "        metadata = chunk.get(\"metadata\", {})\n",
    "\n",
    "        # Get the embedding for the chunk\n",
    "        # embedding = get_embedding(content).data[0]['values']\n",
    "        embedding = get_embedding(content)\n",
    "\n",
    "        # Add the text as part of the metadata\n",
    "        metadata[\"text\"] = content  # Store text in metadata\n",
    "        # metadata[\"token_count\"] = count_tokens(content)\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Upsert the embedding along with its metadata\n",
    "        index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "\n",
    "        count += 1\n",
    "        print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "\n",
    "    print(f\"All {count} embeddings have been upserted to Pinecone\")\n",
    "\n",
    "\n",
    "# upsert_chunks_to_pinecone(index, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Vectors Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pinecone_chunks(index, chunks):\n",
    "    count = 0\n",
    "    for chunk in chunks:\n",
    "        # Get updated embedding\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "\n",
    "        # Extract metadata and page content\n",
    "        metadata = chunk.metadata\n",
    "        text = chunk.page_content\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Update the embedding and metadata\n",
    "        index.update(id=vector_id, values=embedding, set_metadata=metadata)\n",
    "\n",
    "        count += 1\n",
    "        print(f\"Embedding {count} updated in Pinecone with new metadata\")\n",
    "\n",
    "    print(f\"All {count} embeddings have been updated in Pinecone\")\n",
    "\n",
    "\n",
    "# update_pinecone_chunks(index, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your application is designed to answer a wide range of student queries and suggest relevant material, you want to retrieve enough content to cover different facets of a topic without overwhelming the LLM with too much information.\n",
    "\n",
    "# Starting Point:\n",
    "- A common starting point is to set top_k between **5 and 10.**\n",
    "- **top_k=5:** This can work well if your curated content is highly relevant and precise, ensuring that the top 5 matches are very close to the query.\n",
    "-  **top_k=10:** If you want the coach to consider a broader range of content—perhaps to provide diverse perspectives or cover a topic more comprehensively—increasing top_k to around 10 might be beneficial.\n",
    "\n",
    "# Experiment and Adjust:\n",
    "- The “best” value depends on factors such as the diversity of your content, how densely your data covers the topics, and the quality of the embedding matches. It’s a good idea to experiment with different top_k values and evaluate the quality and relevance of the responses in your specific\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
