{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This file now only contains the upsert and updating Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All imports and inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINECONE_API pcsk_4bLR9o_3crxHE9zjHW76VdRnBPi2Xo794pQnKSifnRfQ9iQc6U3iqeqeyVEZ3RjBPYtoD4\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from tkinter import scrolledtext, messagebox\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# from pinecone import Pinecone, ServerlessSpec\n",
    "import pinecone\n",
    "from pinecone import (\n",
    "    Pinecone,\n",
    "    ServerlessSpec,\n",
    "    CloudProvider,\n",
    "    AwsRegion,\n",
    "    VectorType\n",
    ")\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import PyPDF2\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import tkinter as tk\n",
    "import gradio as gr\n",
    "from typing import List, Tuple\n",
    "import concurrent.futures\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Important: Import pinecone-client properly\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")\n",
    "PINECONE_API = os.getenv(\"PINECONE_API\")\n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "print(\"PINECONE_API\", PINECONE_API)\n",
    "\n",
    "\n",
    "EMBEDDING_MODEL = \"llama3-405b-8192-embed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    return document_loader.load()\n",
    "\n",
    "\n",
    "# documents = pdf_load_documents()\n",
    "# documents\n",
    "\n",
    "\n",
    "# def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "#     \"\"\"Extract text from a PDF file.\"\"\"\n",
    "#     with open(pdf_path, 'r') as file:\n",
    "#         pdf_reader = PyPDF2.PdfReader(file)\n",
    "#         text = \"\"\n",
    "#         for page_num in range(len(pdf_reader.pages)):\n",
    "#             page = pdf_reader.pages[page_num]\n",
    "#             text += page.extract_text() + \"\\n\"\n",
    "#     return text\n",
    "# extract_text_from_pdf(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Directory Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 369 documents\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load PDF and Excel files from DATA_PATH.\n",
    "    Returns a list of documents with content and metadata and a list of filenames.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    file_names = []\n",
    "\n",
    "    # Load PDFs\n",
    "    pdf_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    pdf_docs = pdf_loader.load()\n",
    "    for doc in pdf_docs:\n",
    "        documents.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": {\"source\": doc.metadata.get(\"source\", \"unknown\"), \"file_type\": \"pdf\"}\n",
    "        })\n",
    "        file_names.append(doc.metadata.get(\"source\", \"unknown\"))\n",
    "\n",
    "    # Load Excel files\n",
    "    excel_files = glob.glob(os.path.join(DATA_PATH, \"*.xlsx\"))\n",
    "    for file in excel_files:\n",
    "        df = pd.read_excel(file)\n",
    "        headers = df.columns.tolist()\n",
    "        for _, row in df.iterrows():\n",
    "            content = \" \".join([f\"{col}: {str(row[col])}\" for col in headers])\n",
    "            documents.append({\n",
    "                \"content\": content,\n",
    "                \"metadata\": {\"source\": file, \"file_type\": \"excel\"}\n",
    "            })\n",
    "        file_names.append(file)\n",
    "\n",
    "    if not documents:\n",
    "        print(f\"No PDF or Excel files found in {DATA_PATH}\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "    return documents, file_names\n",
    "\n",
    "\n",
    "# Load documents\n",
    "documents, file_names = load_documents()\n",
    "\n",
    "# # Print file names\n",
    "# print(\"Files parsed:\")\n",
    "# for name in file_names:\n",
    "#     print(f\"- {name}\")\n",
    "\n",
    "# print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Directory Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF DOCS len: \n",
      "\n",
      " 369\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Exam 01.xlsx\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 01 - Quiz.xlsx\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 02 - Quiz.xlsx\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 03 - Quiz.xlsx\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 04 - Quiz.xlsx\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 05 - Quiz.xlsx\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Meta Data - Assessments - Course 1.xlsx\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Ebooks\\Meta data - Ebooks Course 1.xlsx\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Lessons (PPTs)\\Meta Data Lessons - Course 1.xlsx\n",
      "Parsed Excel file: D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Videos - Transcripts\\Meta Data - Videos C1.xlsx\n",
      "Loaded 130 documents\n",
      "Files parsed:\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Ebooks\\Chapter 1 (Introduction to Surgical Technology).pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Ebooks\\Chapter 2 (Communication and Teamwork).pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Ebooks\\Chapter 3 (Medicolegal Aspects of Surgical Technology).pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Ebooks\\Chapter 4 (Health Care Facility Structure and Environment).pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Ebooks\\Chapter 5  (Supporting the Psychosocial Needs of the Patient).pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Lessons (PPTs)\\Lesson 01 - Surgical Technology The Profession and The Professional.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Lessons (PPTs)\\Lesson 02 - Communication and Teamwork.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Lessons (PPTs)\\Lesson 03 - Medicolegal Aspects of Surgical Technology.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Lessons (PPTs)\\Lesson 04 - Health Care Facility Structure and Environment.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Lessons (PPTs)\\Lesson 05 - Supporting the Psychosocial Needs of the Patient   .pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Videos - Transcripts\\C1L1.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Videos - Transcripts\\C1L1_Youtube.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Videos - Transcripts\\C1L2.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Videos - Transcripts\\C1L2_YouTube.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Videos - Transcripts\\C1L3.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Videos - Transcripts\\C1L4.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Videos - Transcripts\\C1L5.pdf\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Exam 01.xlsx\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 01 - Quiz.xlsx\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 02 - Quiz.xlsx\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 03 - Quiz.xlsx\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 04 - Quiz.xlsx\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Lesson 05 - Quiz.xlsx\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Assessments\\Meta Data - Assessments - Course 1.xlsx\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Ebooks\\Meta data - Ebooks Course 1.xlsx\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Lessons (PPTs)\\Meta Data Lessons - Course 1.xlsx\n",
      "- D:\\Disrupt Labs\\AI Coach Project\\Rag-pipelines-experiments\\Course 1 - Introduction to Surgical Technology\\Videos - Transcripts\\Meta Data - Videos C1.xlsx\n",
      "\n",
      "Total documents loaded: 130\n"
     ]
    }
   ],
   "source": [
    "def load_documents():\n",
    "    \"\"\"\n",
    "    Load PDF and Excel files from DATA_PATH and its subdirectories.\n",
    "    Returns a list of documents with content and metadata, and a list of filenames.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    file_names = []\n",
    "    processed_files = []\n",
    "\n",
    "    # Load PDFs from all subdirectories\n",
    "    pdf_loader = PyPDFDirectoryLoader(DATA_PATH, recursive=True)\n",
    "    pdf_docs = pdf_loader.load()\n",
    "    print(\"PDF DOCS len: \\n\\n\", len(pdf_docs))\n",
    "    # print(\"PDF DOCS: \\n\\n\",pdf_docs)\n",
    "    count = 0\n",
    "    for doc in pdf_docs:\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        if source not in processed_files:\n",
    "            documents.append({\n",
    "                \"content\": doc.page_content,\n",
    "                \"metadata\": {\"source\": source, \"file_type\": \"pdf\"}\n",
    "            })\n",
    "            # print(f\"PARSED DATA {source}: \\n\",documents[-1])\n",
    "\n",
    "            processed_files.append(source)\n",
    "            file_names.append(source)\n",
    "            # print(f\"Parsed PDF file : {source}\")\n",
    "        # print(\"COUNT: \", count)\n",
    "        count += 1\n",
    "   # return\n",
    "\n",
    "    # Load Excel files from all subdirectories\n",
    "    for root, _, files in os.walk(DATA_PATH):\n",
    "        for file in files:\n",
    "            if file.endswith(\".xlsx\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                if file_path not in processed_files:\n",
    "                    df = pd.read_excel(file_path)\n",
    "                    headers = df.columns.tolist()\n",
    "                    for _, row in df.iterrows():\n",
    "                        content = \" \".join(\n",
    "                            [f\"{col}: {str(row[col])}\" for col in headers])\n",
    "                        documents.append({\n",
    "                            \"content\": content,\n",
    "                            \"metadata\": {\"source\": file_path, \"file_type\": \"excel\"}\n",
    "                        })\n",
    "                        # print(\"PARSED DATA: \\n\\n\\n\",documents[-1])\n",
    "                    processed_files.append(file_path)\n",
    "                    file_names.append(file_path)\n",
    "                    print(f\"Parsed Excel file: {file_path}\")\n",
    "\n",
    "    if not documents:\n",
    "        print(f\"No PDF or Excel files found in {DATA_PATH}\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(documents)} documents\")\n",
    "    # Print file names\n",
    "    print(\"Files parsed:\")\n",
    "    for name in processed_files:\n",
    "        print(f\"- {name}\")\n",
    "    return documents, file_names\n",
    "\n",
    "\n",
    "# Load documents\n",
    "documents, file_names = load_documents()\n",
    "\n",
    "\n",
    "print(f\"\\nTotal documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitting \\ Chunking using Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len,\n",
    "        is_separator_regex=True  # considers separators like '\\n\\n' if true\n",
    "    )\n",
    "    # Assuming each document is a dictionary with 'content' and 'metadata'\n",
    "    docs = []\n",
    "    for doc in documents:\n",
    "        chunks = text_splitter.split_text(doc['content'])\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            docs.append({\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    **doc['metadata'],\n",
    "                    \"chunk_id\": i\n",
    "                }\n",
    "            })\n",
    "    return docs\n",
    "\n",
    "\n",
    "# Split documents into chunks\n",
    "chunks = split_documents(documents)\n",
    "# print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pcsk_4bLR9o_3crxHE9zjHW76VdRnBPi2Xo794pQnKSifnRfQ9iQc6U3iqeqeyVEZ3RjBPYtoD4\n"
     ]
    }
   ],
   "source": [
    "pc = Pinecone(api_key=PINECONE_API)\n",
    "print(PINECONE_API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use What:\n",
    "**Use Upsert:**\n",
    "\n",
    "When you're adding new vectors or want to replace existing vectors with new data (including changing the vector values).\n",
    "When you need to add a completely new document or vector.\n",
    "When you want to update both the vector values and metadata.\n",
    "\n",
    "**Use Update:**\n",
    "\n",
    "When you're only modifying the metadata of an existing vector.\n",
    "When the vector values (embeddings) themselves are correct and only extra information like text, author, or document-related metadata needs to be updated.\n",
    "Summary:\n",
    "Upsert: Adds or replaces both the vector values and metadata. Use when inserting or completely replacing data.\n",
    "Update: Modifies the metadata without changing the vector values. Use when the vectors are correct, but metadata needs an update.\n",
    "For your case, if you just want to add or update the page_content or any other metadata for existing vectors, use update. If you want to re-upload vectors with new embeddings or metadata, use upsert.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings Via AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en'  and Upsert each to Pinecone one by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1 upserted to Pinecone with metadata\n",
      "Embedding 2 upserted to Pinecone with metadata\n",
      "Embedding 3 upserted to Pinecone with metadata\n",
      "Embedding 4 upserted to Pinecone with metadata\n",
      "Embedding 5 upserted to Pinecone with metadata\n",
      "Embedding 6 upserted to Pinecone with metadata\n",
      "Embedding 7 upserted to Pinecone with metadata\n",
      "Embedding 8 upserted to Pinecone with metadata\n",
      "Embedding 9 upserted to Pinecone with metadata\n",
      "Embedding 10 upserted to Pinecone with metadata\n",
      "Embedding 11 upserted to Pinecone with metadata\n",
      "Embedding 12 upserted to Pinecone with metadata\n",
      "Embedding 13 upserted to Pinecone with metadata\n",
      "Embedding 14 upserted to Pinecone with metadata\n",
      "Embedding 15 upserted to Pinecone with metadata\n",
      "Embedding 16 upserted to Pinecone with metadata\n",
      "Embedding 17 upserted to Pinecone with metadata\n",
      "Embedding 18 upserted to Pinecone with metadata\n",
      "Embedding 19 upserted to Pinecone with metadata\n",
      "Embedding 20 upserted to Pinecone with metadata\n",
      "Embedding 21 upserted to Pinecone with metadata\n",
      "Embedding 22 upserted to Pinecone with metadata\n",
      "Embedding 23 upserted to Pinecone with metadata\n",
      "Embedding 24 upserted to Pinecone with metadata\n",
      "Embedding 25 upserted to Pinecone with metadata\n",
      "Embedding 26 upserted to Pinecone with metadata\n",
      "Embedding 27 upserted to Pinecone with metadata\n",
      "Embedding 28 upserted to Pinecone with metadata\n",
      "Embedding 29 upserted to Pinecone with metadata\n",
      "Embedding 30 upserted to Pinecone with metadata\n",
      "Embedding 31 upserted to Pinecone with metadata\n",
      "Embedding 32 upserted to Pinecone with metadata\n",
      "Embedding 33 upserted to Pinecone with metadata\n",
      "Embedding 34 upserted to Pinecone with metadata\n",
      "Embedding 35 upserted to Pinecone with metadata\n",
      "Embedding 36 upserted to Pinecone with metadata\n",
      "Embedding 37 upserted to Pinecone with metadata\n",
      "Embedding 38 upserted to Pinecone with metadata\n",
      "Embedding 39 upserted to Pinecone with metadata\n",
      "Embedding 40 upserted to Pinecone with metadata\n",
      "Embedding 41 upserted to Pinecone with metadata\n",
      "Embedding 42 upserted to Pinecone with metadata\n",
      "Embedding 43 upserted to Pinecone with metadata\n",
      "Embedding 44 upserted to Pinecone with metadata\n",
      "Embedding 45 upserted to Pinecone with metadata\n",
      "Embedding 46 upserted to Pinecone with metadata\n",
      "Embedding 47 upserted to Pinecone with metadata\n",
      "Embedding 48 upserted to Pinecone with metadata\n",
      "Embedding 49 upserted to Pinecone with metadata\n",
      "Embedding 50 upserted to Pinecone with metadata\n",
      "Embedding 51 upserted to Pinecone with metadata\n",
      "Embedding 52 upserted to Pinecone with metadata\n",
      "Embedding 53 upserted to Pinecone with metadata\n",
      "Embedding 54 upserted to Pinecone with metadata\n",
      "Embedding 55 upserted to Pinecone with metadata\n",
      "Embedding 56 upserted to Pinecone with metadata\n",
      "Embedding 57 upserted to Pinecone with metadata\n",
      "Embedding 58 upserted to Pinecone with metadata\n",
      "Embedding 59 upserted to Pinecone with metadata\n",
      "Embedding 60 upserted to Pinecone with metadata\n",
      "Embedding 61 upserted to Pinecone with metadata\n",
      "Embedding 62 upserted to Pinecone with metadata\n",
      "Embedding 63 upserted to Pinecone with metadata\n",
      "Embedding 64 upserted to Pinecone with metadata\n",
      "Embedding 65 upserted to Pinecone with metadata\n",
      "Embedding 66 upserted to Pinecone with metadata\n",
      "Embedding 67 upserted to Pinecone with metadata\n",
      "Embedding 68 upserted to Pinecone with metadata\n",
      "Embedding 69 upserted to Pinecone with metadata\n",
      "Embedding 70 upserted to Pinecone with metadata\n",
      "Embedding 71 upserted to Pinecone with metadata\n",
      "Embedding 72 upserted to Pinecone with metadata\n",
      "Embedding 73 upserted to Pinecone with metadata\n",
      "Embedding 74 upserted to Pinecone with metadata\n",
      "Embedding 75 upserted to Pinecone with metadata\n",
      "Embedding 76 upserted to Pinecone with metadata\n",
      "Embedding 77 upserted to Pinecone with metadata\n",
      "Embedding 78 upserted to Pinecone with metadata\n",
      "Embedding 79 upserted to Pinecone with metadata\n",
      "Embedding 80 upserted to Pinecone with metadata\n",
      "Embedding 81 upserted to Pinecone with metadata\n",
      "Embedding 82 upserted to Pinecone with metadata\n",
      "Embedding 83 upserted to Pinecone with metadata\n",
      "Embedding 84 upserted to Pinecone with metadata\n",
      "Embedding 85 upserted to Pinecone with metadata\n",
      "Embedding 86 upserted to Pinecone with metadata\n",
      "Embedding 87 upserted to Pinecone with metadata\n",
      "Embedding 88 upserted to Pinecone with metadata\n",
      "Embedding 89 upserted to Pinecone with metadata\n",
      "Embedding 90 upserted to Pinecone with metadata\n",
      "Embedding 91 upserted to Pinecone with metadata\n",
      "Embedding 92 upserted to Pinecone with metadata\n",
      "Embedding 93 upserted to Pinecone with metadata\n",
      "Embedding 94 upserted to Pinecone with metadata\n",
      "Embedding 95 upserted to Pinecone with metadata\n",
      "Embedding 96 upserted to Pinecone with metadata\n",
      "Embedding 97 upserted to Pinecone with metadata\n",
      "Embedding 98 upserted to Pinecone with metadata\n",
      "Embedding 99 upserted to Pinecone with metadata\n",
      "Embedding 100 upserted to Pinecone with metadata\n",
      "Embedding 101 upserted to Pinecone with metadata\n",
      "Embedding 102 upserted to Pinecone with metadata\n",
      "Embedding 103 upserted to Pinecone with metadata\n",
      "Embedding 104 upserted to Pinecone with metadata\n",
      "Embedding 105 upserted to Pinecone with metadata\n",
      "Embedding 106 upserted to Pinecone with metadata\n",
      "Embedding 107 upserted to Pinecone with metadata\n",
      "Embedding 108 upserted to Pinecone with metadata\n",
      "Embedding 109 upserted to Pinecone with metadata\n",
      "Embedding 110 upserted to Pinecone with metadata\n",
      "Embedding 111 upserted to Pinecone with metadata\n",
      "Embedding 112 upserted to Pinecone with metadata\n",
      "Embedding 113 upserted to Pinecone with metadata\n",
      "Embedding 114 upserted to Pinecone with metadata\n",
      "Embedding 115 upserted to Pinecone with metadata\n",
      "Embedding 116 upserted to Pinecone with metadata\n",
      "Embedding 117 upserted to Pinecone with metadata\n",
      "Embedding 118 upserted to Pinecone with metadata\n",
      "Embedding 119 upserted to Pinecone with metadata\n",
      "Embedding 120 upserted to Pinecone with metadata\n",
      "Embedding 121 upserted to Pinecone with metadata\n",
      "Embedding 122 upserted to Pinecone with metadata\n",
      "Embedding 123 upserted to Pinecone with metadata\n",
      "Embedding 124 upserted to Pinecone with metadata\n",
      "Embedding 125 upserted to Pinecone with metadata\n",
      "Embedding 126 upserted to Pinecone with metadata\n",
      "Embedding 127 upserted to Pinecone with metadata\n",
      "Embedding 128 upserted to Pinecone with metadata\n",
      "Embedding 129 upserted to Pinecone with metadata\n",
      "Embedding 130 upserted to Pinecone with metadata\n",
      "Embedding 131 upserted to Pinecone with metadata\n",
      "Embedding 132 upserted to Pinecone with metadata\n",
      "Embedding 133 upserted to Pinecone with metadata\n",
      "Embedding 134 upserted to Pinecone with metadata\n",
      "Embedding 135 upserted to Pinecone with metadata\n",
      "Embedding 136 upserted to Pinecone with metadata\n",
      "Embedding 137 upserted to Pinecone with metadata\n",
      "Embedding 138 upserted to Pinecone with metadata\n",
      "Embedding 139 upserted to Pinecone with metadata\n",
      "Embedding 140 upserted to Pinecone with metadata\n",
      "Embedding 141 upserted to Pinecone with metadata\n",
      "Embedding 142 upserted to Pinecone with metadata\n",
      "Embedding 143 upserted to Pinecone with metadata\n",
      "Embedding 144 upserted to Pinecone with metadata\n",
      "Embedding 145 upserted to Pinecone with metadata\n",
      "Embedding 146 upserted to Pinecone with metadata\n",
      "Embedding 147 upserted to Pinecone with metadata\n",
      "Embedding 148 upserted to Pinecone with metadata\n",
      "Embedding 149 upserted to Pinecone with metadata\n",
      "Embedding 150 upserted to Pinecone with metadata\n",
      "Embedding 151 upserted to Pinecone with metadata\n",
      "Embedding 152 upserted to Pinecone with metadata\n",
      "Embedding 153 upserted to Pinecone with metadata\n",
      "Embedding 154 upserted to Pinecone with metadata\n",
      "All 154 embeddings have been upserted to Pinecone\n"
     ]
    }
   ],
   "source": [
    "# Connect to the index\n",
    "index = pc.Index(\"ai-coach\")\n",
    "\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained(\n",
    "    'jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
    "\n",
    "# Function to generate embeddings without tokenization\n",
    "\n",
    "\n",
    "def get_embedding(data):\n",
    "    embeddings = embedding_model.encode(data).tolist()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def upsert_chunks_to_pinecone(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Ensure the chunk has the correct structure\n",
    "        content = chunk.get('content')\n",
    "        metadata = chunk.get('metadata', {})\n",
    "\n",
    "        # Get the embedding for the chunk\n",
    "        embedding = get_embedding(content)\n",
    "\n",
    "        # Add the text as part of the metadata\n",
    "        metadata['text'] = content  # Store text in metadata\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Upsert the embedding along with its metadata\n",
    "        index.upsert(vectors=[(vector_id, embedding, metadata)])\n",
    "\n",
    "        print(f\"Embedding {count} upserted to Pinecone with metadata\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"All {count-1} embeddings have been upserted to Pinecone\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming `index` is your Pinecone index and `chunks` is the list of chunked documents\n",
    "upsert_chunks_to_pinecone(index, chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Vectors Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pinecone_chunks(index, chunks):\n",
    "    count = 1\n",
    "    for chunk in chunks:\n",
    "        # Get updated embedding\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "\n",
    "        # Extract metadata and page content\n",
    "        metadata = chunk.metadata\n",
    "        text = chunk.page_content\n",
    "\n",
    "        # Create a unique vector ID for each chunk (e.g., based on count or some unique identifier)\n",
    "        vector_id = f\"vec_{count}\"\n",
    "\n",
    "        # Update the embedding and metadata\n",
    "        index.update(id=vector_id, values=embedding, set_metadata=metadata)\n",
    "\n",
    "        print(f\"Embedding {count} updated in Pinecone with new metadata\")\n",
    "        count += 1\n",
    "\n",
    "    print(f\"All {count-1} embeddings have been updated in Pinecone\")\n",
    "\n",
    "# update_pinecone_chunks(index, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since your application is designed to answer a wide range of student queries and suggest relevant material, you want to retrieve enough content to cover different facets of a topic without overwhelming the LLM with too much information.\n",
    "\n",
    "# Starting Point:\n",
    "- A common starting point is to set top_k between **5 and 10.**\n",
    "- **top_k=5:** This can work well if your curated content is highly relevant and precise, ensuring that the top 5 matches are very close to the query.\n",
    "-  **top_k=10:** If you want the coach to consider a broader range of content—perhaps to provide diverse perspectives or cover a topic more comprehensively—increasing top_k to around 10 might be beneficial.\n",
    "\n",
    "# Experiment and Adjust:\n",
    "- The “best” value depends on factors such as the diversity of your content, how densely your data covers the topics, and the quality of the embedding matches. It’s a good idea to experiment with different top_k values and evaluate the quality and relevance of the responses in your specific\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
